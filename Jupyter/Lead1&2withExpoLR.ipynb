{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "514aeb8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchsummary in c:\\users\\btbm\\anaconda3\\lib\\site-packages (1.5.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchsummary\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from tqdm import notebook\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2f8dae",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "326a8f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1']\n"
     ]
    }
   ],
   "source": [
    "data_dir='G:\\DCDA-Net\\Data'\n",
    "print(os.listdir(data_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "898f19aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d32cbea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Resize(size=(224,224)),\n",
    "                transforms.Normalize((0.3433, 0.7419, 0.5770), (0.3453, 0.0661, 0.2652))\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59d78715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4867, 912, 304)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset=ImageFolder(data_dir,transform=transform)\n",
    "torch.manual_seed(42)\n",
    "train_ds, val_ds, test_ds = random_split(dataset, [0.8,0.15,0.05])\n",
    "len(train_ds),len(val_ds),len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c6fa2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader =DataLoader(train_ds, batch_size=16, shuffle=True,num_workers=4)\n",
    "valloader = DataLoader(val_ds, batch_size=32, shuffle=True,num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d789754d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(dataloader, model):\n",
    "    total, correct = 0, 0\n",
    "    model.eval()\n",
    "    for data in dataloader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, pred = torch.max(outputs.data, 1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (pred == labels).sum().item()\n",
    "    return 100 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "980d5a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func(model, loss_fn, opt, trainloader, valloader, scheduler , epochs):\n",
    "    loss_epoch_arr = []\n",
    "    train_accuracy_lis = []\n",
    "    val_accuracy_lis = []\n",
    "    val_loss_epoch_arr = []\n",
    "\n",
    "    max_epochs = epochs\n",
    "    min_loss = 1000\n",
    "    model.train()\n",
    "\n",
    "    for epoch in notebook.tqdm(range(max_epochs), total=max_epochs, unit=\"epochs\"):\n",
    "        model.train()\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "    \n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            opt.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            if min_loss > loss.item():\n",
    "                min_loss = loss.item()\n",
    "                best_model = copy.deepcopy(model.state_dict())\n",
    "                print(\"Min loss %0.2f\" %min_loss)\n",
    "            del inputs, labels, outputs\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            if i % 2000 == 0:\n",
    "                print('Iteration: %d, Loss: %0.2f' % (i, loss.item()))\n",
    "            \n",
    "       \n",
    "        loss_epoch_arr.append(loss.item())\n",
    "        train_accuracy = evaluation(trainloader, model)\n",
    "        val_accuracy = evaluation(valloader, model)\n",
    "        print('Epoch: %d/%d, Test acc: %0.2f, Train acc: %0.2f' % (\n",
    "              epoch, max_epochs, val_accuracy, train_accuracy))\n",
    "        train_accuracy_lis.append(train_accuracy)\n",
    "        val_accuracy_lis.append(val_accuracy)\n",
    "    \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(valloader, 0):\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "\n",
    "        del inputs, labels, outputs\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        val_loss_epoch_arr.append(loss.item())\n",
    "    return best_model, loss_epoch_arr, train_accuracy_lis, val_accuracy_lis, val_loss_epoch_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ce456c",
   "metadata": {},
   "source": [
    "# Resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "572807cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BTBM\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "resnet = models.resnet50(weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ab59738",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in resnet.parameters():\n",
    "    param.requires_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b89f93b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = resnet.fc.in_features\n",
    "new_classifier = nn.Sequential(\n",
    "    nn.Linear(num_features, 256), nn.ReLU(), nn.Dropout(0.2),\n",
    "    nn.Linear(256, 2))\n",
    "resnet.fc = new_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd3c204a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]           4,096\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-12          [-1, 256, 56, 56]             512\n",
      "           Conv2d-13          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-14          [-1, 256, 56, 56]             512\n",
      "             ReLU-15          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-16          [-1, 256, 56, 56]               0\n",
      "           Conv2d-17           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-18           [-1, 64, 56, 56]             128\n",
      "             ReLU-19           [-1, 64, 56, 56]               0\n",
      "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
      "             ReLU-22           [-1, 64, 56, 56]               0\n",
      "           Conv2d-23          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-24          [-1, 256, 56, 56]             512\n",
      "             ReLU-25          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-26          [-1, 256, 56, 56]               0\n",
      "           Conv2d-27           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-28           [-1, 64, 56, 56]             128\n",
      "             ReLU-29           [-1, 64, 56, 56]               0\n",
      "           Conv2d-30           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-31           [-1, 64, 56, 56]             128\n",
      "             ReLU-32           [-1, 64, 56, 56]               0\n",
      "           Conv2d-33          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-34          [-1, 256, 56, 56]             512\n",
      "             ReLU-35          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-36          [-1, 256, 56, 56]               0\n",
      "           Conv2d-37          [-1, 128, 56, 56]          32,768\n",
      "      BatchNorm2d-38          [-1, 128, 56, 56]             256\n",
      "             ReLU-39          [-1, 128, 56, 56]               0\n",
      "           Conv2d-40          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-41          [-1, 128, 28, 28]             256\n",
      "             ReLU-42          [-1, 128, 28, 28]               0\n",
      "           Conv2d-43          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-44          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-45          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-46          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-47          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-48          [-1, 512, 28, 28]               0\n",
      "           Conv2d-49          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
      "             ReLU-51          [-1, 128, 28, 28]               0\n",
      "           Conv2d-52          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-53          [-1, 128, 28, 28]             256\n",
      "             ReLU-54          [-1, 128, 28, 28]               0\n",
      "           Conv2d-55          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-56          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-57          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-58          [-1, 512, 28, 28]               0\n",
      "           Conv2d-59          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-60          [-1, 128, 28, 28]             256\n",
      "             ReLU-61          [-1, 128, 28, 28]               0\n",
      "           Conv2d-62          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
      "             ReLU-64          [-1, 128, 28, 28]               0\n",
      "           Conv2d-65          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-66          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-67          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-68          [-1, 512, 28, 28]               0\n",
      "           Conv2d-69          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-70          [-1, 128, 28, 28]             256\n",
      "             ReLU-71          [-1, 128, 28, 28]               0\n",
      "           Conv2d-72          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-73          [-1, 128, 28, 28]             256\n",
      "             ReLU-74          [-1, 128, 28, 28]               0\n",
      "           Conv2d-75          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-76          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-77          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-78          [-1, 512, 28, 28]               0\n",
      "           Conv2d-79          [-1, 256, 28, 28]         131,072\n",
      "      BatchNorm2d-80          [-1, 256, 28, 28]             512\n",
      "             ReLU-81          [-1, 256, 28, 28]               0\n",
      "           Conv2d-82          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-83          [-1, 256, 14, 14]             512\n",
      "             ReLU-84          [-1, 256, 14, 14]               0\n",
      "           Conv2d-85         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-86         [-1, 1024, 14, 14]           2,048\n",
      "           Conv2d-87         [-1, 1024, 14, 14]         524,288\n",
      "      BatchNorm2d-88         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-89         [-1, 1024, 14, 14]               0\n",
      "       Bottleneck-90         [-1, 1024, 14, 14]               0\n",
      "           Conv2d-91          [-1, 256, 14, 14]         262,144\n",
      "      BatchNorm2d-92          [-1, 256, 14, 14]             512\n",
      "             ReLU-93          [-1, 256, 14, 14]               0\n",
      "           Conv2d-94          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-95          [-1, 256, 14, 14]             512\n",
      "             ReLU-96          [-1, 256, 14, 14]               0\n",
      "           Conv2d-97         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-98         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-99         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-100         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-101          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-102          [-1, 256, 14, 14]             512\n",
      "            ReLU-103          [-1, 256, 14, 14]               0\n",
      "          Conv2d-104          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-105          [-1, 256, 14, 14]             512\n",
      "            ReLU-106          [-1, 256, 14, 14]               0\n",
      "          Conv2d-107         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-108         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-109         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-110         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-111          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-112          [-1, 256, 14, 14]             512\n",
      "            ReLU-113          [-1, 256, 14, 14]               0\n",
      "          Conv2d-114          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-115          [-1, 256, 14, 14]             512\n",
      "            ReLU-116          [-1, 256, 14, 14]               0\n",
      "          Conv2d-117         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-118         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-119         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-120         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-121          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-122          [-1, 256, 14, 14]             512\n",
      "            ReLU-123          [-1, 256, 14, 14]               0\n",
      "          Conv2d-124          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-125          [-1, 256, 14, 14]             512\n",
      "            ReLU-126          [-1, 256, 14, 14]               0\n",
      "          Conv2d-127         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-128         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-129         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-130         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-131          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-132          [-1, 256, 14, 14]             512\n",
      "            ReLU-133          [-1, 256, 14, 14]               0\n",
      "          Conv2d-134          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-135          [-1, 256, 14, 14]             512\n",
      "            ReLU-136          [-1, 256, 14, 14]               0\n",
      "          Conv2d-137         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-138         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-139         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-140         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-141          [-1, 512, 14, 14]         524,288\n",
      "     BatchNorm2d-142          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-143          [-1, 512, 14, 14]               0\n",
      "          Conv2d-144            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-145            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-146            [-1, 512, 7, 7]               0\n",
      "          Conv2d-147           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-148           [-1, 2048, 7, 7]           4,096\n",
      "          Conv2d-149           [-1, 2048, 7, 7]       2,097,152\n",
      "     BatchNorm2d-150           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-151           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-152           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-153            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-154            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-155            [-1, 512, 7, 7]               0\n",
      "          Conv2d-156            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-157            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-158            [-1, 512, 7, 7]               0\n",
      "          Conv2d-159           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-160           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-161           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-162           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-163            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-164            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-165            [-1, 512, 7, 7]               0\n",
      "          Conv2d-166            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-167            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-168            [-1, 512, 7, 7]               0\n",
      "          Conv2d-169           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-170           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-171           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-172           [-1, 2048, 7, 7]               0\n",
      "AdaptiveAvgPool2d-173           [-1, 2048, 1, 1]               0\n",
      "          Linear-174                  [-1, 256]         524,544\n",
      "            ReLU-175                  [-1, 256]               0\n",
      "         Dropout-176                  [-1, 256]               0\n",
      "          Linear-177                    [-1, 2]             514\n",
      "================================================================\n",
      "Total params: 24,033,090\n",
      "Trainable params: 525,058\n",
      "Non-trainable params: 23,508,032\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 286.56\n",
      "Params size (MB): 91.68\n",
      "Estimated Total Size (MB): 378.81\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "resnet =resnet.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(resnet.parameters(), lr=0.001)\n",
    "summary(resnet, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89634e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9  # The decay factor for each epoch\n",
    "scheduler = lr_scheduler.ExponentialLR(opt, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6479fba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72c62c111cf54400915f0981fbe9251e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?epochs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min loss 0.68\n",
      "Iteration: 0, Loss: 0.68\n",
      "Min loss 0.64\n",
      "Min loss 0.61\n",
      "Min loss 0.59\n",
      "Epoch: 0/50, Test acc: 49.12, Train acc: 52.89\n",
      "Iteration: 0, Loss: 0.70\n",
      "Min loss 0.57\n",
      "Epoch: 1/50, Test acc: 48.25, Train acc: 52.97\n",
      "Iteration: 0, Loss: 0.62\n",
      "Epoch: 2/50, Test acc: 49.56, Train acc: 53.03\n",
      "Iteration: 0, Loss: 0.78\n",
      "Epoch: 3/50, Test acc: 48.90, Train acc: 53.46\n",
      "Iteration: 0, Loss: 0.70\n",
      "Epoch: 4/50, Test acc: 50.66, Train acc: 53.19\n",
      "Iteration: 0, Loss: 0.70\n",
      "Min loss 0.55\n",
      "Epoch: 5/50, Test acc: 49.56, Train acc: 53.36\n",
      "Iteration: 0, Loss: 0.71\n",
      "Epoch: 6/50, Test acc: 49.67, Train acc: 52.76\n",
      "Iteration: 0, Loss: 0.70\n",
      "Epoch: 7/50, Test acc: 50.33, Train acc: 52.66\n",
      "Iteration: 0, Loss: 0.72\n",
      "Epoch: 8/50, Test acc: 49.23, Train acc: 53.40\n",
      "Iteration: 0, Loss: 0.67\n",
      "Epoch: 9/50, Test acc: 49.12, Train acc: 53.26\n",
      "Iteration: 0, Loss: 0.64\n",
      "Epoch: 10/50, Test acc: 49.56, Train acc: 53.24\n",
      "Iteration: 0, Loss: 0.72\n",
      "Epoch: 11/50, Test acc: 49.89, Train acc: 52.64\n",
      "Iteration: 0, Loss: 0.69\n",
      "Epoch: 12/50, Test acc: 50.22, Train acc: 53.01\n",
      "Iteration: 0, Loss: 0.65\n",
      "Epoch: 13/50, Test acc: 50.55, Train acc: 52.91\n",
      "Iteration: 0, Loss: 0.70\n",
      "Epoch: 14/50, Test acc: 49.34, Train acc: 52.89\n",
      "Iteration: 0, Loss: 0.74\n",
      "Epoch: 15/50, Test acc: 48.46, Train acc: 53.11\n",
      "Iteration: 0, Loss: 0.72\n",
      "Epoch: 16/50, Test acc: 50.66, Train acc: 53.17\n",
      "Iteration: 0, Loss: 0.71\n",
      "Epoch: 17/50, Test acc: 50.88, Train acc: 52.97\n",
      "Iteration: 0, Loss: 0.74\n",
      "Epoch: 18/50, Test acc: 49.56, Train acc: 52.68\n",
      "Iteration: 0, Loss: 0.64\n",
      "Epoch: 19/50, Test acc: 50.88, Train acc: 52.76\n",
      "Iteration: 0, Loss: 0.73\n",
      "Epoch: 20/50, Test acc: 49.67, Train acc: 53.26\n",
      "Iteration: 0, Loss: 0.66\n",
      "Epoch: 21/50, Test acc: 50.44, Train acc: 53.13\n",
      "Iteration: 0, Loss: 0.76\n",
      "Epoch: 22/50, Test acc: 49.45, Train acc: 52.95\n",
      "Iteration: 0, Loss: 0.66\n",
      "Epoch: 23/50, Test acc: 48.46, Train acc: 52.64\n",
      "Iteration: 0, Loss: 0.74\n",
      "Epoch: 24/50, Test acc: 49.67, Train acc: 53.17\n",
      "Iteration: 0, Loss: 0.68\n",
      "Epoch: 25/50, Test acc: 49.23, Train acc: 53.07\n",
      "Iteration: 0, Loss: 0.71\n",
      "Epoch: 26/50, Test acc: 49.34, Train acc: 53.44\n",
      "Iteration: 0, Loss: 0.68\n",
      "Epoch: 27/50, Test acc: 48.90, Train acc: 53.11\n",
      "Iteration: 0, Loss: 0.67\n",
      "Epoch: 28/50, Test acc: 49.56, Train acc: 52.48\n",
      "Iteration: 0, Loss: 0.70\n",
      "Epoch: 29/50, Test acc: 49.45, Train acc: 52.43\n",
      "Iteration: 0, Loss: 0.74\n",
      "Epoch: 30/50, Test acc: 50.00, Train acc: 53.48\n",
      "Iteration: 0, Loss: 0.71\n",
      "Epoch: 31/50, Test acc: 50.44, Train acc: 52.85\n",
      "Iteration: 0, Loss: 0.71\n",
      "Epoch: 32/50, Test acc: 48.57, Train acc: 53.19\n",
      "Iteration: 0, Loss: 0.68\n",
      "Epoch: 33/50, Test acc: 49.78, Train acc: 53.36\n",
      "Iteration: 0, Loss: 0.69\n",
      "Epoch: 34/50, Test acc: 51.21, Train acc: 52.72\n",
      "Iteration: 0, Loss: 0.74\n",
      "Epoch: 35/50, Test acc: 48.68, Train acc: 53.05\n",
      "Iteration: 0, Loss: 0.70\n",
      "Epoch: 36/50, Test acc: 49.56, Train acc: 53.32\n",
      "Iteration: 0, Loss: 0.64\n",
      "Epoch: 37/50, Test acc: 50.11, Train acc: 52.62\n",
      "Iteration: 0, Loss: 0.68\n",
      "Epoch: 38/50, Test acc: 51.75, Train acc: 53.03\n",
      "Iteration: 0, Loss: 0.70\n",
      "Epoch: 39/50, Test acc: 48.46, Train acc: 53.07\n",
      "Iteration: 0, Loss: 0.65\n",
      "Epoch: 40/50, Test acc: 50.44, Train acc: 53.22\n",
      "Iteration: 0, Loss: 0.71\n",
      "Epoch: 41/50, Test acc: 48.90, Train acc: 53.15\n",
      "Iteration: 0, Loss: 0.72\n",
      "Epoch: 42/50, Test acc: 50.22, Train acc: 53.15\n",
      "Iteration: 0, Loss: 0.77\n",
      "Min loss 0.55\n",
      "Epoch: 43/50, Test acc: 50.11, Train acc: 53.13\n",
      "Iteration: 0, Loss: 0.64\n",
      "Epoch: 44/50, Test acc: 51.32, Train acc: 53.17\n",
      "Iteration: 0, Loss: 0.67\n",
      "Epoch: 45/50, Test acc: 50.77, Train acc: 53.22\n",
      "Iteration: 0, Loss: 0.68\n",
      "Epoch: 46/50, Test acc: 50.33, Train acc: 52.83\n",
      "Iteration: 0, Loss: 0.71\n",
      "Epoch: 47/50, Test acc: 49.56, Train acc: 52.80\n",
      "Iteration: 0, Loss: 0.72\n",
      "Epoch: 48/50, Test acc: 50.22, Train acc: 52.78\n",
      "Iteration: 0, Loss: 0.68\n",
      "Epoch: 49/50, Test acc: 50.66, Train acc: 53.11\n"
     ]
    }
   ],
   "source": [
    "best_model, loss_epoch_arr, train_accuracy_lis, val_accuracy_lis, val_loss_epoch_arr = train_func(resnet, loss_fn, opt, trainloader, valloader, scheduler , epochs=50\n",
    "                                                                            )      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fdec74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = DataLoader(test_ds, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e15acc0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55.5921052631579"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation(testloader,resnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87b2862",
   "metadata": {},
   "source": [
    "# Resnet-152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "abdeaa54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BTBM\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "resnet = models.resnet152(weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0eefad49",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in resnet.parameters():\n",
    "    param.requires_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "756d947b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = resnet.fc.in_features\n",
    "new_classifier = nn.Sequential(\n",
    "    nn.Linear(num_features, 256), nn.ReLU(), nn.Dropout(0.2),\n",
    "    nn.Linear(256, 2))\n",
    "resnet.fc = new_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4cb30f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]           4,096\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-12          [-1, 256, 56, 56]             512\n",
      "           Conv2d-13          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-14          [-1, 256, 56, 56]             512\n",
      "             ReLU-15          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-16          [-1, 256, 56, 56]               0\n",
      "           Conv2d-17           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-18           [-1, 64, 56, 56]             128\n",
      "             ReLU-19           [-1, 64, 56, 56]               0\n",
      "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
      "             ReLU-22           [-1, 64, 56, 56]               0\n",
      "           Conv2d-23          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-24          [-1, 256, 56, 56]             512\n",
      "             ReLU-25          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-26          [-1, 256, 56, 56]               0\n",
      "           Conv2d-27           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-28           [-1, 64, 56, 56]             128\n",
      "             ReLU-29           [-1, 64, 56, 56]               0\n",
      "           Conv2d-30           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-31           [-1, 64, 56, 56]             128\n",
      "             ReLU-32           [-1, 64, 56, 56]               0\n",
      "           Conv2d-33          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-34          [-1, 256, 56, 56]             512\n",
      "             ReLU-35          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-36          [-1, 256, 56, 56]               0\n",
      "           Conv2d-37          [-1, 128, 56, 56]          32,768\n",
      "      BatchNorm2d-38          [-1, 128, 56, 56]             256\n",
      "             ReLU-39          [-1, 128, 56, 56]               0\n",
      "           Conv2d-40          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-41          [-1, 128, 28, 28]             256\n",
      "             ReLU-42          [-1, 128, 28, 28]               0\n",
      "           Conv2d-43          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-44          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-45          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-46          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-47          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-48          [-1, 512, 28, 28]               0\n",
      "           Conv2d-49          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
      "             ReLU-51          [-1, 128, 28, 28]               0\n",
      "           Conv2d-52          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-53          [-1, 128, 28, 28]             256\n",
      "             ReLU-54          [-1, 128, 28, 28]               0\n",
      "           Conv2d-55          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-56          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-57          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-58          [-1, 512, 28, 28]               0\n",
      "           Conv2d-59          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-60          [-1, 128, 28, 28]             256\n",
      "             ReLU-61          [-1, 128, 28, 28]               0\n",
      "           Conv2d-62          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
      "             ReLU-64          [-1, 128, 28, 28]               0\n",
      "           Conv2d-65          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-66          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-67          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-68          [-1, 512, 28, 28]               0\n",
      "           Conv2d-69          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-70          [-1, 128, 28, 28]             256\n",
      "             ReLU-71          [-1, 128, 28, 28]               0\n",
      "           Conv2d-72          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-73          [-1, 128, 28, 28]             256\n",
      "             ReLU-74          [-1, 128, 28, 28]               0\n",
      "           Conv2d-75          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-76          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-77          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-78          [-1, 512, 28, 28]               0\n",
      "           Conv2d-79          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-80          [-1, 128, 28, 28]             256\n",
      "             ReLU-81          [-1, 128, 28, 28]               0\n",
      "           Conv2d-82          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-83          [-1, 128, 28, 28]             256\n",
      "             ReLU-84          [-1, 128, 28, 28]               0\n",
      "           Conv2d-85          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-86          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-87          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-88          [-1, 512, 28, 28]               0\n",
      "           Conv2d-89          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-90          [-1, 128, 28, 28]             256\n",
      "             ReLU-91          [-1, 128, 28, 28]               0\n",
      "           Conv2d-92          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-93          [-1, 128, 28, 28]             256\n",
      "             ReLU-94          [-1, 128, 28, 28]               0\n",
      "           Conv2d-95          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-96          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-97          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-98          [-1, 512, 28, 28]               0\n",
      "           Conv2d-99          [-1, 128, 28, 28]          65,536\n",
      "     BatchNorm2d-100          [-1, 128, 28, 28]             256\n",
      "            ReLU-101          [-1, 128, 28, 28]               0\n",
      "          Conv2d-102          [-1, 128, 28, 28]         147,456\n",
      "     BatchNorm2d-103          [-1, 128, 28, 28]             256\n",
      "            ReLU-104          [-1, 128, 28, 28]               0\n",
      "          Conv2d-105          [-1, 512, 28, 28]          65,536\n",
      "     BatchNorm2d-106          [-1, 512, 28, 28]           1,024\n",
      "            ReLU-107          [-1, 512, 28, 28]               0\n",
      "      Bottleneck-108          [-1, 512, 28, 28]               0\n",
      "          Conv2d-109          [-1, 128, 28, 28]          65,536\n",
      "     BatchNorm2d-110          [-1, 128, 28, 28]             256\n",
      "            ReLU-111          [-1, 128, 28, 28]               0\n",
      "          Conv2d-112          [-1, 128, 28, 28]         147,456\n",
      "     BatchNorm2d-113          [-1, 128, 28, 28]             256\n",
      "            ReLU-114          [-1, 128, 28, 28]               0\n",
      "          Conv2d-115          [-1, 512, 28, 28]          65,536\n",
      "     BatchNorm2d-116          [-1, 512, 28, 28]           1,024\n",
      "            ReLU-117          [-1, 512, 28, 28]               0\n",
      "      Bottleneck-118          [-1, 512, 28, 28]               0\n",
      "          Conv2d-119          [-1, 256, 28, 28]         131,072\n",
      "     BatchNorm2d-120          [-1, 256, 28, 28]             512\n",
      "            ReLU-121          [-1, 256, 28, 28]               0\n",
      "          Conv2d-122          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-123          [-1, 256, 14, 14]             512\n",
      "            ReLU-124          [-1, 256, 14, 14]               0\n",
      "          Conv2d-125         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-126         [-1, 1024, 14, 14]           2,048\n",
      "          Conv2d-127         [-1, 1024, 14, 14]         524,288\n",
      "     BatchNorm2d-128         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-129         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-130         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-131          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-132          [-1, 256, 14, 14]             512\n",
      "            ReLU-133          [-1, 256, 14, 14]               0\n",
      "          Conv2d-134          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-135          [-1, 256, 14, 14]             512\n",
      "            ReLU-136          [-1, 256, 14, 14]               0\n",
      "          Conv2d-137         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-138         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-139         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-140         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-141          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-142          [-1, 256, 14, 14]             512\n",
      "            ReLU-143          [-1, 256, 14, 14]               0\n",
      "          Conv2d-144          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-145          [-1, 256, 14, 14]             512\n",
      "            ReLU-146          [-1, 256, 14, 14]               0\n",
      "          Conv2d-147         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-148         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-149         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-150         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-151          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-152          [-1, 256, 14, 14]             512\n",
      "            ReLU-153          [-1, 256, 14, 14]               0\n",
      "          Conv2d-154          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-155          [-1, 256, 14, 14]             512\n",
      "            ReLU-156          [-1, 256, 14, 14]               0\n",
      "          Conv2d-157         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-158         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-159         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-160         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-161          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-162          [-1, 256, 14, 14]             512\n",
      "            ReLU-163          [-1, 256, 14, 14]               0\n",
      "          Conv2d-164          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-165          [-1, 256, 14, 14]             512\n",
      "            ReLU-166          [-1, 256, 14, 14]               0\n",
      "          Conv2d-167         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-168         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-169         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-170         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-171          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-172          [-1, 256, 14, 14]             512\n",
      "            ReLU-173          [-1, 256, 14, 14]               0\n",
      "          Conv2d-174          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-175          [-1, 256, 14, 14]             512\n",
      "            ReLU-176          [-1, 256, 14, 14]               0\n",
      "          Conv2d-177         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-178         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-179         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-180         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-181          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-182          [-1, 256, 14, 14]             512\n",
      "            ReLU-183          [-1, 256, 14, 14]               0\n",
      "          Conv2d-184          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-185          [-1, 256, 14, 14]             512\n",
      "            ReLU-186          [-1, 256, 14, 14]               0\n",
      "          Conv2d-187         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-188         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-189         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-190         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-191          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-192          [-1, 256, 14, 14]             512\n",
      "            ReLU-193          [-1, 256, 14, 14]               0\n",
      "          Conv2d-194          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-195          [-1, 256, 14, 14]             512\n",
      "            ReLU-196          [-1, 256, 14, 14]               0\n",
      "          Conv2d-197         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-198         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-199         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-200         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-201          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-202          [-1, 256, 14, 14]             512\n",
      "            ReLU-203          [-1, 256, 14, 14]               0\n",
      "          Conv2d-204          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-205          [-1, 256, 14, 14]             512\n",
      "            ReLU-206          [-1, 256, 14, 14]               0\n",
      "          Conv2d-207         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-208         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-209         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-210         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-211          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-212          [-1, 256, 14, 14]             512\n",
      "            ReLU-213          [-1, 256, 14, 14]               0\n",
      "          Conv2d-214          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-215          [-1, 256, 14, 14]             512\n",
      "            ReLU-216          [-1, 256, 14, 14]               0\n",
      "          Conv2d-217         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-218         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-219         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-220         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-221          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-222          [-1, 256, 14, 14]             512\n",
      "            ReLU-223          [-1, 256, 14, 14]               0\n",
      "          Conv2d-224          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-225          [-1, 256, 14, 14]             512\n",
      "            ReLU-226          [-1, 256, 14, 14]               0\n",
      "          Conv2d-227         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-228         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-229         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-230         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-231          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-232          [-1, 256, 14, 14]             512\n",
      "            ReLU-233          [-1, 256, 14, 14]               0\n",
      "          Conv2d-234          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-235          [-1, 256, 14, 14]             512\n",
      "            ReLU-236          [-1, 256, 14, 14]               0\n",
      "          Conv2d-237         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-238         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-239         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-240         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-241          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-242          [-1, 256, 14, 14]             512\n",
      "            ReLU-243          [-1, 256, 14, 14]               0\n",
      "          Conv2d-244          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-245          [-1, 256, 14, 14]             512\n",
      "            ReLU-246          [-1, 256, 14, 14]               0\n",
      "          Conv2d-247         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-248         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-249         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-250         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-251          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-252          [-1, 256, 14, 14]             512\n",
      "            ReLU-253          [-1, 256, 14, 14]               0\n",
      "          Conv2d-254          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-255          [-1, 256, 14, 14]             512\n",
      "            ReLU-256          [-1, 256, 14, 14]               0\n",
      "          Conv2d-257         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-258         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-259         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-260         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-261          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-262          [-1, 256, 14, 14]             512\n",
      "            ReLU-263          [-1, 256, 14, 14]               0\n",
      "          Conv2d-264          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-265          [-1, 256, 14, 14]             512\n",
      "            ReLU-266          [-1, 256, 14, 14]               0\n",
      "          Conv2d-267         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-268         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-269         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-270         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-271          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-272          [-1, 256, 14, 14]             512\n",
      "            ReLU-273          [-1, 256, 14, 14]               0\n",
      "          Conv2d-274          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-275          [-1, 256, 14, 14]             512\n",
      "            ReLU-276          [-1, 256, 14, 14]               0\n",
      "          Conv2d-277         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-278         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-279         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-280         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-281          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-282          [-1, 256, 14, 14]             512\n",
      "            ReLU-283          [-1, 256, 14, 14]               0\n",
      "          Conv2d-284          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-285          [-1, 256, 14, 14]             512\n",
      "            ReLU-286          [-1, 256, 14, 14]               0\n",
      "          Conv2d-287         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-288         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-289         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-290         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-291          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-292          [-1, 256, 14, 14]             512\n",
      "            ReLU-293          [-1, 256, 14, 14]               0\n",
      "          Conv2d-294          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-295          [-1, 256, 14, 14]             512\n",
      "            ReLU-296          [-1, 256, 14, 14]               0\n",
      "          Conv2d-297         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-298         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-299         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-300         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-301          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-302          [-1, 256, 14, 14]             512\n",
      "            ReLU-303          [-1, 256, 14, 14]               0\n",
      "          Conv2d-304          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-305          [-1, 256, 14, 14]             512\n",
      "            ReLU-306          [-1, 256, 14, 14]               0\n",
      "          Conv2d-307         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-308         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-309         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-310         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-311          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-312          [-1, 256, 14, 14]             512\n",
      "            ReLU-313          [-1, 256, 14, 14]               0\n",
      "          Conv2d-314          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-315          [-1, 256, 14, 14]             512\n",
      "            ReLU-316          [-1, 256, 14, 14]               0\n",
      "          Conv2d-317         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-318         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-319         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-320         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-321          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-322          [-1, 256, 14, 14]             512\n",
      "            ReLU-323          [-1, 256, 14, 14]               0\n",
      "          Conv2d-324          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-325          [-1, 256, 14, 14]             512\n",
      "            ReLU-326          [-1, 256, 14, 14]               0\n",
      "          Conv2d-327         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-328         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-329         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-330         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-331          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-332          [-1, 256, 14, 14]             512\n",
      "            ReLU-333          [-1, 256, 14, 14]               0\n",
      "          Conv2d-334          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-335          [-1, 256, 14, 14]             512\n",
      "            ReLU-336          [-1, 256, 14, 14]               0\n",
      "          Conv2d-337         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-338         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-339         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-340         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-341          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-342          [-1, 256, 14, 14]             512\n",
      "            ReLU-343          [-1, 256, 14, 14]               0\n",
      "          Conv2d-344          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-345          [-1, 256, 14, 14]             512\n",
      "            ReLU-346          [-1, 256, 14, 14]               0\n",
      "          Conv2d-347         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-348         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-349         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-350         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-351          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-352          [-1, 256, 14, 14]             512\n",
      "            ReLU-353          [-1, 256, 14, 14]               0\n",
      "          Conv2d-354          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-355          [-1, 256, 14, 14]             512\n",
      "            ReLU-356          [-1, 256, 14, 14]               0\n",
      "          Conv2d-357         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-358         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-359         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-360         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-361          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-362          [-1, 256, 14, 14]             512\n",
      "            ReLU-363          [-1, 256, 14, 14]               0\n",
      "          Conv2d-364          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-365          [-1, 256, 14, 14]             512\n",
      "            ReLU-366          [-1, 256, 14, 14]               0\n",
      "          Conv2d-367         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-368         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-369         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-370         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-371          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-372          [-1, 256, 14, 14]             512\n",
      "            ReLU-373          [-1, 256, 14, 14]               0\n",
      "          Conv2d-374          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-375          [-1, 256, 14, 14]             512\n",
      "            ReLU-376          [-1, 256, 14, 14]               0\n",
      "          Conv2d-377         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-378         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-379         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-380         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-381          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-382          [-1, 256, 14, 14]             512\n",
      "            ReLU-383          [-1, 256, 14, 14]               0\n",
      "          Conv2d-384          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-385          [-1, 256, 14, 14]             512\n",
      "            ReLU-386          [-1, 256, 14, 14]               0\n",
      "          Conv2d-387         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-388         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-389         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-390         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-391          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-392          [-1, 256, 14, 14]             512\n",
      "            ReLU-393          [-1, 256, 14, 14]               0\n",
      "          Conv2d-394          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-395          [-1, 256, 14, 14]             512\n",
      "            ReLU-396          [-1, 256, 14, 14]               0\n",
      "          Conv2d-397         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-398         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-399         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-400         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-401          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-402          [-1, 256, 14, 14]             512\n",
      "            ReLU-403          [-1, 256, 14, 14]               0\n",
      "          Conv2d-404          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-405          [-1, 256, 14, 14]             512\n",
      "            ReLU-406          [-1, 256, 14, 14]               0\n",
      "          Conv2d-407         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-408         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-409         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-410         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-411          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-412          [-1, 256, 14, 14]             512\n",
      "            ReLU-413          [-1, 256, 14, 14]               0\n",
      "          Conv2d-414          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-415          [-1, 256, 14, 14]             512\n",
      "            ReLU-416          [-1, 256, 14, 14]               0\n",
      "          Conv2d-417         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-418         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-419         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-420         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-421          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-422          [-1, 256, 14, 14]             512\n",
      "            ReLU-423          [-1, 256, 14, 14]               0\n",
      "          Conv2d-424          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-425          [-1, 256, 14, 14]             512\n",
      "            ReLU-426          [-1, 256, 14, 14]               0\n",
      "          Conv2d-427         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-428         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-429         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-430         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-431          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-432          [-1, 256, 14, 14]             512\n",
      "            ReLU-433          [-1, 256, 14, 14]               0\n",
      "          Conv2d-434          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-435          [-1, 256, 14, 14]             512\n",
      "            ReLU-436          [-1, 256, 14, 14]               0\n",
      "          Conv2d-437         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-438         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-439         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-440         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-441          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-442          [-1, 256, 14, 14]             512\n",
      "            ReLU-443          [-1, 256, 14, 14]               0\n",
      "          Conv2d-444          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-445          [-1, 256, 14, 14]             512\n",
      "            ReLU-446          [-1, 256, 14, 14]               0\n",
      "          Conv2d-447         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-448         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-449         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-450         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-451          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-452          [-1, 256, 14, 14]             512\n",
      "            ReLU-453          [-1, 256, 14, 14]               0\n",
      "          Conv2d-454          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-455          [-1, 256, 14, 14]             512\n",
      "            ReLU-456          [-1, 256, 14, 14]               0\n",
      "          Conv2d-457         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-458         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-459         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-460         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-461          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-462          [-1, 256, 14, 14]             512\n",
      "            ReLU-463          [-1, 256, 14, 14]               0\n",
      "          Conv2d-464          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-465          [-1, 256, 14, 14]             512\n",
      "            ReLU-466          [-1, 256, 14, 14]               0\n",
      "          Conv2d-467         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-468         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-469         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-470         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-471          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-472          [-1, 256, 14, 14]             512\n",
      "            ReLU-473          [-1, 256, 14, 14]               0\n",
      "          Conv2d-474          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-475          [-1, 256, 14, 14]             512\n",
      "            ReLU-476          [-1, 256, 14, 14]               0\n",
      "          Conv2d-477         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-478         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-479         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-480         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-481          [-1, 512, 14, 14]         524,288\n",
      "     BatchNorm2d-482          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-483          [-1, 512, 14, 14]               0\n",
      "          Conv2d-484            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-485            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-486            [-1, 512, 7, 7]               0\n",
      "          Conv2d-487           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-488           [-1, 2048, 7, 7]           4,096\n",
      "          Conv2d-489           [-1, 2048, 7, 7]       2,097,152\n",
      "     BatchNorm2d-490           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-491           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-492           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-493            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-494            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-495            [-1, 512, 7, 7]               0\n",
      "          Conv2d-496            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-497            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-498            [-1, 512, 7, 7]               0\n",
      "          Conv2d-499           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-500           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-501           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-502           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-503            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-504            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-505            [-1, 512, 7, 7]               0\n",
      "          Conv2d-506            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-507            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-508            [-1, 512, 7, 7]               0\n",
      "          Conv2d-509           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-510           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-511           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-512           [-1, 2048, 7, 7]               0\n",
      "AdaptiveAvgPool2d-513           [-1, 2048, 1, 1]               0\n",
      "          Linear-514                  [-1, 256]         524,544\n",
      "            ReLU-515                  [-1, 256]               0\n",
      "         Dropout-516                  [-1, 256]               0\n",
      "          Linear-517                    [-1, 2]             514\n",
      "================================================================\n",
      "Total params: 58,668,866\n",
      "Trainable params: 525,058\n",
      "Non-trainable params: 58,143,808\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 606.59\n",
      "Params size (MB): 223.80\n",
      "Estimated Total Size (MB): 830.97\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "resnet =resnet.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(resnet.parameters(), lr=0.001)\n",
    "summary(resnet, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01973add",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9  # The decay factor for each epoch\n",
    "scheduler = lr_scheduler.ExponentialLR(opt, gamma=gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c2d6fb0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e8b7c7590b2488596b0d57f2fda38f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?epochs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min loss 0.69\n",
      "Iteration: 0, Loss: 0.69\n",
      "Min loss 0.66\n",
      "Min loss 0.65\n",
      "Min loss 0.62\n",
      "Min loss 0.60\n",
      "Min loss 0.57\n",
      "Min loss 0.55\n",
      "Epoch: 0/50, Test acc: 59.32, Train acc: 58.35\n",
      "Iteration: 0, Loss: 0.70\n",
      "Epoch: 1/50, Test acc: 58.88, Train acc: 58.39\n",
      "Iteration: 0, Loss: 0.71\n",
      "Epoch: 2/50, Test acc: 59.32, Train acc: 58.80\n",
      "Iteration: 0, Loss: 0.67\n",
      "Epoch: 3/50, Test acc: 59.54, Train acc: 57.61\n",
      "Iteration: 0, Loss: 0.59\n",
      "Epoch: 4/50, Test acc: 58.55, Train acc: 58.68\n",
      "Iteration: 0, Loss: 0.82\n",
      "Epoch: 5/50, Test acc: 59.10, Train acc: 58.66\n",
      "Iteration: 0, Loss: 0.64\n",
      "Epoch: 6/50, Test acc: 58.99, Train acc: 58.02\n",
      "Iteration: 0, Loss: 0.72\n",
      "Epoch: 7/50, Test acc: 58.77, Train acc: 58.17\n",
      "Iteration: 0, Loss: 0.64\n",
      "Epoch: 8/50, Test acc: 59.65, Train acc: 58.15\n",
      "Iteration: 0, Loss: 0.66\n",
      "Epoch: 9/50, Test acc: 59.87, Train acc: 58.17\n",
      "Iteration: 0, Loss: 0.63\n",
      "Epoch: 10/50, Test acc: 58.66, Train acc: 58.35\n",
      "Iteration: 0, Loss: 0.69\n",
      "Epoch: 11/50, Test acc: 59.32, Train acc: 57.94\n",
      "Iteration: 0, Loss: 0.75\n",
      "Min loss 0.53\n",
      "Epoch: 12/50, Test acc: 58.55, Train acc: 57.88\n",
      "Iteration: 0, Loss: 0.63\n",
      "Min loss 0.50\n",
      "Epoch: 13/50, Test acc: 58.66, Train acc: 58.00\n",
      "Iteration: 0, Loss: 0.70\n",
      "Epoch: 14/50, Test acc: 59.10, Train acc: 58.33\n",
      "Iteration: 0, Loss: 0.78\n",
      "Epoch: 15/50, Test acc: 58.55, Train acc: 57.59\n",
      "Iteration: 0, Loss: 0.77\n",
      "Epoch: 16/50, Test acc: 58.11, Train acc: 58.00\n",
      "Iteration: 0, Loss: 0.67\n",
      "Epoch: 17/50, Test acc: 59.54, Train acc: 58.85\n",
      "Iteration: 0, Loss: 0.69\n",
      "Epoch: 18/50, Test acc: 59.21, Train acc: 58.27\n",
      "Iteration: 0, Loss: 0.80\n",
      "Epoch: 19/50, Test acc: 59.43, Train acc: 58.56\n",
      "Iteration: 0, Loss: 0.70\n",
      "Epoch: 20/50, Test acc: 58.99, Train acc: 58.54\n",
      "Iteration: 0, Loss: 0.71\n",
      "Epoch: 21/50, Test acc: 57.89, Train acc: 58.06\n",
      "Iteration: 0, Loss: 0.72\n",
      "Epoch: 22/50, Test acc: 59.43, Train acc: 58.25\n",
      "Iteration: 0, Loss: 0.69\n",
      "Epoch: 23/50, Test acc: 58.88, Train acc: 58.29\n",
      "Iteration: 0, Loss: 0.67\n",
      "Epoch: 24/50, Test acc: 58.44, Train acc: 58.09\n",
      "Iteration: 0, Loss: 0.76\n",
      "Epoch: 25/50, Test acc: 59.54, Train acc: 58.35\n",
      "Iteration: 0, Loss: 0.67\n",
      "Epoch: 26/50, Test acc: 59.10, Train acc: 58.06\n",
      "Iteration: 0, Loss: 0.71\n",
      "Min loss 0.50\n",
      "Epoch: 27/50, Test acc: 58.33, Train acc: 57.92\n",
      "Iteration: 0, Loss: 0.65\n",
      "Epoch: 28/50, Test acc: 59.32, Train acc: 58.62\n",
      "Iteration: 0, Loss: 0.65\n",
      "Epoch: 29/50, Test acc: 58.44, Train acc: 58.50\n",
      "Iteration: 0, Loss: 0.73\n",
      "Epoch: 30/50, Test acc: 59.54, Train acc: 57.76\n",
      "Iteration: 0, Loss: 0.73\n",
      "Epoch: 31/50, Test acc: 58.99, Train acc: 58.21\n",
      "Iteration: 0, Loss: 0.72\n",
      "Epoch: 32/50, Test acc: 59.10, Train acc: 58.43\n",
      "Iteration: 0, Loss: 0.77\n",
      "Epoch: 33/50, Test acc: 58.33, Train acc: 58.31\n",
      "Iteration: 0, Loss: 0.64\n",
      "Epoch: 34/50, Test acc: 58.55, Train acc: 58.50\n",
      "Iteration: 0, Loss: 0.71\n",
      "Epoch: 35/50, Test acc: 59.21, Train acc: 58.91\n",
      "Iteration: 0, Loss: 0.70\n",
      "Epoch: 36/50, Test acc: 59.21, Train acc: 58.17\n",
      "Iteration: 0, Loss: 0.71\n",
      "Epoch: 37/50, Test acc: 59.21, Train acc: 59.07\n",
      "Iteration: 0, Loss: 0.68\n",
      "Epoch: 38/50, Test acc: 59.32, Train acc: 58.15\n",
      "Iteration: 0, Loss: 0.74\n",
      "Epoch: 39/50, Test acc: 58.44, Train acc: 57.49\n",
      "Iteration: 0, Loss: 0.77\n",
      "Epoch: 40/50, Test acc: 59.10, Train acc: 58.35\n",
      "Iteration: 0, Loss: 0.69\n",
      "Epoch: 41/50, Test acc: 59.10, Train acc: 57.86\n",
      "Iteration: 0, Loss: 0.73\n",
      "Epoch: 42/50, Test acc: 58.77, Train acc: 58.13\n",
      "Iteration: 0, Loss: 0.71\n",
      "Epoch: 43/50, Test acc: 59.32, Train acc: 58.33\n",
      "Iteration: 0, Loss: 0.68\n",
      "Epoch: 44/50, Test acc: 58.66, Train acc: 58.00\n",
      "Iteration: 0, Loss: 0.64\n",
      "Epoch: 45/50, Test acc: 59.21, Train acc: 58.11\n",
      "Iteration: 0, Loss: 0.70\n",
      "Epoch: 46/50, Test acc: 59.43, Train acc: 58.17\n",
      "Iteration: 0, Loss: 0.65\n",
      "Epoch: 47/50, Test acc: 58.88, Train acc: 58.09\n",
      "Iteration: 0, Loss: 0.64\n",
      "Epoch: 48/50, Test acc: 58.11, Train acc: 58.70\n",
      "Iteration: 0, Loss: 0.79\n",
      "Epoch: 49/50, Test acc: 59.76, Train acc: 58.35\n"
     ]
    }
   ],
   "source": [
    "best_model, loss_epoch_arr, train_accuracy_lis, val_accuracy_lis, val_loss_epoch_arr = train_func(resnet, loss_fn, opt, trainloader, valloader, scheduler , epochs=50\n",
    "                                                                            )      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ba3c234",
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = DataLoader(test_ds, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb643838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57.23684210526316"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation(testloader,resnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08018180",
   "metadata": {},
   "source": [
    "# Resnet 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a6f8f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BTBM\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "resnet = models.resnet18(weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c8a7210d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in resnet.parameters():\n",
    "    param.requires_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ed597b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = resnet.fc.in_features\n",
    "new_classifier = nn.Sequential(\n",
    "    nn.Linear(num_features, 256), nn.ReLU(), nn.Dropout(0.2),\n",
    "    nn.Linear(256, 2))\n",
    "resnet.fc = new_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9e07130f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-11           [-1, 64, 56, 56]               0\n",
      "           Conv2d-12           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-13           [-1, 64, 56, 56]             128\n",
      "             ReLU-14           [-1, 64, 56, 56]               0\n",
      "           Conv2d-15           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-16           [-1, 64, 56, 56]             128\n",
      "             ReLU-17           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-18           [-1, 64, 56, 56]               0\n",
      "           Conv2d-19          [-1, 128, 28, 28]          73,728\n",
      "      BatchNorm2d-20          [-1, 128, 28, 28]             256\n",
      "             ReLU-21          [-1, 128, 28, 28]               0\n",
      "           Conv2d-22          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-23          [-1, 128, 28, 28]             256\n",
      "           Conv2d-24          [-1, 128, 28, 28]           8,192\n",
      "      BatchNorm2d-25          [-1, 128, 28, 28]             256\n",
      "             ReLU-26          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-27          [-1, 128, 28, 28]               0\n",
      "           Conv2d-28          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-29          [-1, 128, 28, 28]             256\n",
      "             ReLU-30          [-1, 128, 28, 28]               0\n",
      "           Conv2d-31          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-32          [-1, 128, 28, 28]             256\n",
      "             ReLU-33          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-34          [-1, 128, 28, 28]               0\n",
      "           Conv2d-35          [-1, 256, 14, 14]         294,912\n",
      "      BatchNorm2d-36          [-1, 256, 14, 14]             512\n",
      "             ReLU-37          [-1, 256, 14, 14]               0\n",
      "           Conv2d-38          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-39          [-1, 256, 14, 14]             512\n",
      "           Conv2d-40          [-1, 256, 14, 14]          32,768\n",
      "      BatchNorm2d-41          [-1, 256, 14, 14]             512\n",
      "             ReLU-42          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-43          [-1, 256, 14, 14]               0\n",
      "           Conv2d-44          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-45          [-1, 256, 14, 14]             512\n",
      "             ReLU-46          [-1, 256, 14, 14]               0\n",
      "           Conv2d-47          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-48          [-1, 256, 14, 14]             512\n",
      "             ReLU-49          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-50          [-1, 256, 14, 14]               0\n",
      "           Conv2d-51            [-1, 512, 7, 7]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-53            [-1, 512, 7, 7]               0\n",
      "           Conv2d-54            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 7, 7]           1,024\n",
      "           Conv2d-56            [-1, 512, 7, 7]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-58            [-1, 512, 7, 7]               0\n",
      "       BasicBlock-59            [-1, 512, 7, 7]               0\n",
      "           Conv2d-60            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-62            [-1, 512, 7, 7]               0\n",
      "           Conv2d-63            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-65            [-1, 512, 7, 7]               0\n",
      "       BasicBlock-66            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                  [-1, 256]         131,328\n",
      "             ReLU-69                  [-1, 256]               0\n",
      "          Dropout-70                  [-1, 256]               0\n",
      "           Linear-71                    [-1, 2]             514\n",
      "================================================================\n",
      "Total params: 11,308,354\n",
      "Trainable params: 131,842\n",
      "Non-trainable params: 11,176,512\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 62.79\n",
      "Params size (MB): 43.14\n",
      "Estimated Total Size (MB): 106.50\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "resnet =resnet.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(resnet.parameters(), lr=0.001)\n",
    "summary(resnet, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b0b07000",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9  # The decay factor for each epoch\n",
    "scheduler = lr_scheduler.ExponentialLR(opt, gamma=gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "56dc6b49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90da975b401f4481ae19ef7231571b17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?epochs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min loss 0.73\n",
      "Iteration: 0, Loss: 0.73\n",
      "Min loss 0.71\n",
      "Min loss 0.64\n",
      "Min loss 0.62\n",
      "Min loss 0.61\n",
      "Min loss 0.58\n",
      "Min loss 0.58\n",
      "Epoch: 0/50, Test acc: 59.10, Train acc: 59.32\n",
      "Iteration: 0, Loss: 0.66\n",
      "Min loss 0.56\n",
      "Min loss 0.56\n",
      "Epoch: 1/50, Test acc: 57.46, Train acc: 57.76\n",
      "Iteration: 0, Loss: 0.68\n",
      "Min loss 0.54\n",
      "Epoch: 2/50, Test acc: 58.88, Train acc: 59.50\n",
      "Iteration: 0, Loss: 0.72\n",
      "Min loss 0.54\n",
      "Epoch: 3/50, Test acc: 59.10, Train acc: 58.93\n",
      "Iteration: 0, Loss: 0.67\n",
      "Epoch: 4/50, Test acc: 59.98, Train acc: 59.46\n",
      "Iteration: 0, Loss: 0.73\n",
      "Epoch: 5/50, Test acc: 59.10, Train acc: 59.15\n",
      "Iteration: 0, Loss: 0.70\n",
      "Min loss 0.53\n",
      "Epoch: 6/50, Test acc: 59.10, Train acc: 59.24\n",
      "Iteration: 0, Loss: 0.67\n",
      "Epoch: 7/50, Test acc: 60.20, Train acc: 59.38\n",
      "Iteration: 0, Loss: 0.68\n",
      "Epoch: 8/50, Test acc: 58.88, Train acc: 59.28\n",
      "Iteration: 0, Loss: 0.71\n",
      "Epoch: 9/50, Test acc: 58.44, Train acc: 58.56\n",
      "Iteration: 0, Loss: 0.67\n",
      "Epoch: 10/50, Test acc: 57.79, Train acc: 58.70\n",
      "Iteration: 0, Loss: 0.64\n",
      "Epoch: 11/50, Test acc: 57.79, Train acc: 58.62\n",
      "Iteration: 0, Loss: 0.70\n",
      "Epoch: 12/50, Test acc: 57.35, Train acc: 58.31\n",
      "Iteration: 0, Loss: 0.72\n",
      "Epoch: 13/50, Test acc: 58.55, Train acc: 59.15\n",
      "Iteration: 0, Loss: 0.70\n",
      "Epoch: 14/50, Test acc: 59.10, Train acc: 59.01\n",
      "Iteration: 0, Loss: 0.67\n",
      "Epoch: 15/50, Test acc: 59.43, Train acc: 59.44\n",
      "Iteration: 0, Loss: 0.71\n",
      "Epoch: 16/50, Test acc: 57.89, Train acc: 58.37\n",
      "Iteration: 0, Loss: 0.64\n",
      "Epoch: 17/50, Test acc: 59.32, Train acc: 59.48\n",
      "Iteration: 0, Loss: 0.76\n",
      "Epoch: 18/50, Test acc: 58.55, Train acc: 58.66\n",
      "Iteration: 0, Loss: 0.70\n",
      "Epoch: 19/50, Test acc: 58.99, Train acc: 59.22\n",
      "Iteration: 0, Loss: 0.66\n",
      "Epoch: 20/50, Test acc: 59.65, Train acc: 59.40\n",
      "Iteration: 0, Loss: 0.66\n",
      "Epoch: 21/50, Test acc: 58.88, Train acc: 58.66\n",
      "Iteration: 0, Loss: 0.68\n",
      "Epoch: 22/50, Test acc: 59.54, Train acc: 59.38\n",
      "Iteration: 0, Loss: 0.72\n",
      "Epoch: 23/50, Test acc: 58.99, Train acc: 58.91\n",
      "Iteration: 0, Loss: 0.69\n",
      "Epoch: 24/50, Test acc: 59.32, Train acc: 59.40\n",
      "Iteration: 0, Loss: 0.64\n",
      "Epoch: 25/50, Test acc: 59.10, Train acc: 59.40\n",
      "Iteration: 0, Loss: 0.69\n",
      "Epoch: 26/50, Test acc: 58.66, Train acc: 58.97\n",
      "Iteration: 0, Loss: 0.68\n",
      "Epoch: 27/50, Test acc: 58.44, Train acc: 58.89\n",
      "Iteration: 0, Loss: 0.66\n",
      "Epoch: 28/50, Test acc: 58.77, Train acc: 59.28\n",
      "Iteration: 0, Loss: 0.69\n",
      "Epoch: 29/50, Test acc: 58.99, Train acc: 58.97\n",
      "Iteration: 0, Loss: 0.63\n",
      "Epoch: 30/50, Test acc: 59.54, Train acc: 59.46\n",
      "Iteration: 0, Loss: 0.65\n",
      "Epoch: 31/50, Test acc: 59.43, Train acc: 58.95\n",
      "Iteration: 0, Loss: 0.71\n",
      "Epoch: 32/50, Test acc: 59.32, Train acc: 59.40\n",
      "Iteration: 0, Loss: 0.65\n",
      "Epoch: 33/50, Test acc: 59.21, Train acc: 58.82\n",
      "Iteration: 0, Loss: 0.64\n",
      "Epoch: 34/50, Test acc: 58.99, Train acc: 58.99\n",
      "Iteration: 0, Loss: 0.77\n",
      "Min loss 0.53\n",
      "Epoch: 35/50, Test acc: 58.44, Train acc: 58.66\n",
      "Iteration: 0, Loss: 0.64\n",
      "Epoch: 36/50, Test acc: 58.99, Train acc: 59.32\n",
      "Iteration: 0, Loss: 0.77\n",
      "Epoch: 37/50, Test acc: 59.32, Train acc: 59.56\n",
      "Iteration: 0, Loss: 0.74\n",
      "Epoch: 38/50, Test acc: 58.99, Train acc: 58.99\n",
      "Iteration: 0, Loss: 0.73\n",
      "Epoch: 39/50, Test acc: 58.66, Train acc: 58.78\n",
      "Iteration: 0, Loss: 0.76\n",
      "Min loss 0.53\n",
      "Epoch: 40/50, Test acc: 60.09, Train acc: 59.67\n",
      "Iteration: 0, Loss: 0.70\n",
      "Epoch: 41/50, Test acc: 59.10, Train acc: 59.05\n",
      "Iteration: 0, Loss: 0.71\n",
      "Epoch: 42/50, Test acc: 58.99, Train acc: 59.48\n",
      "Iteration: 0, Loss: 0.69\n",
      "Epoch: 43/50, Test acc: 58.33, Train acc: 58.97\n",
      "Iteration: 0, Loss: 0.69\n",
      "Epoch: 44/50, Test acc: 59.54, Train acc: 59.58\n",
      "Iteration: 0, Loss: 0.69\n",
      "Epoch: 45/50, Test acc: 59.21, Train acc: 59.30\n",
      "Iteration: 0, Loss: 0.73\n",
      "Epoch: 46/50, Test acc: 59.10, Train acc: 59.30\n",
      "Iteration: 0, Loss: 0.64\n",
      "Epoch: 47/50, Test acc: 59.10, Train acc: 59.19\n",
      "Iteration: 0, Loss: 0.68\n",
      "Epoch: 48/50, Test acc: 59.32, Train acc: 59.28\n",
      "Iteration: 0, Loss: 0.69\n",
      "Epoch: 49/50, Test acc: 59.76, Train acc: 59.07\n"
     ]
    }
   ],
   "source": [
    "best_model, loss_epoch_arr, train_accuracy_lis, val_accuracy_lis, val_loss_epoch_arr = train_func(resnet, loss_fn, opt, trainloader, valloader, scheduler , epochs=50\n",
    "                                                                            )      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9ca9604c",
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = DataLoader(test_ds, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e53b74e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63.48684210526316"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation(testloader,resnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c56f5c4",
   "metadata": {},
   "source": [
    "# resnet-34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "baf068b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BTBM\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "resnet = models.resnet34(weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9ee0f4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in resnet.parameters():\n",
    "    param.requires_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "926d67af",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = resnet.fc.in_features\n",
    "new_classifier = nn.Sequential(\n",
    "    nn.Linear(num_features, 256), nn.ReLU(), nn.Dropout(0.2),\n",
    "    nn.Linear(256, 2))\n",
    "resnet.fc = new_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c7b9967a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-11           [-1, 64, 56, 56]               0\n",
      "           Conv2d-12           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-13           [-1, 64, 56, 56]             128\n",
      "             ReLU-14           [-1, 64, 56, 56]               0\n",
      "           Conv2d-15           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-16           [-1, 64, 56, 56]             128\n",
      "             ReLU-17           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-18           [-1, 64, 56, 56]               0\n",
      "           Conv2d-19           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-20           [-1, 64, 56, 56]             128\n",
      "             ReLU-21           [-1, 64, 56, 56]               0\n",
      "           Conv2d-22           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-23           [-1, 64, 56, 56]             128\n",
      "             ReLU-24           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-25           [-1, 64, 56, 56]               0\n",
      "           Conv2d-26          [-1, 128, 28, 28]          73,728\n",
      "      BatchNorm2d-27          [-1, 128, 28, 28]             256\n",
      "             ReLU-28          [-1, 128, 28, 28]               0\n",
      "           Conv2d-29          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-30          [-1, 128, 28, 28]             256\n",
      "           Conv2d-31          [-1, 128, 28, 28]           8,192\n",
      "      BatchNorm2d-32          [-1, 128, 28, 28]             256\n",
      "             ReLU-33          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-34          [-1, 128, 28, 28]               0\n",
      "           Conv2d-35          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-36          [-1, 128, 28, 28]             256\n",
      "             ReLU-37          [-1, 128, 28, 28]               0\n",
      "           Conv2d-38          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-39          [-1, 128, 28, 28]             256\n",
      "             ReLU-40          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-41          [-1, 128, 28, 28]               0\n",
      "           Conv2d-42          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-43          [-1, 128, 28, 28]             256\n",
      "             ReLU-44          [-1, 128, 28, 28]               0\n",
      "           Conv2d-45          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-46          [-1, 128, 28, 28]             256\n",
      "             ReLU-47          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-48          [-1, 128, 28, 28]               0\n",
      "           Conv2d-49          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
      "             ReLU-51          [-1, 128, 28, 28]               0\n",
      "           Conv2d-52          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-53          [-1, 128, 28, 28]             256\n",
      "             ReLU-54          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-55          [-1, 128, 28, 28]               0\n",
      "           Conv2d-56          [-1, 256, 14, 14]         294,912\n",
      "      BatchNorm2d-57          [-1, 256, 14, 14]             512\n",
      "             ReLU-58          [-1, 256, 14, 14]               0\n",
      "           Conv2d-59          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-60          [-1, 256, 14, 14]             512\n",
      "           Conv2d-61          [-1, 256, 14, 14]          32,768\n",
      "      BatchNorm2d-62          [-1, 256, 14, 14]             512\n",
      "             ReLU-63          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-64          [-1, 256, 14, 14]               0\n",
      "           Conv2d-65          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-66          [-1, 256, 14, 14]             512\n",
      "             ReLU-67          [-1, 256, 14, 14]               0\n",
      "           Conv2d-68          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-69          [-1, 256, 14, 14]             512\n",
      "             ReLU-70          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-71          [-1, 256, 14, 14]               0\n",
      "           Conv2d-72          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-73          [-1, 256, 14, 14]             512\n",
      "             ReLU-74          [-1, 256, 14, 14]               0\n",
      "           Conv2d-75          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-76          [-1, 256, 14, 14]             512\n",
      "             ReLU-77          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-78          [-1, 256, 14, 14]               0\n",
      "           Conv2d-79          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-80          [-1, 256, 14, 14]             512\n",
      "             ReLU-81          [-1, 256, 14, 14]               0\n",
      "           Conv2d-82          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-83          [-1, 256, 14, 14]             512\n",
      "             ReLU-84          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-85          [-1, 256, 14, 14]               0\n",
      "           Conv2d-86          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-87          [-1, 256, 14, 14]             512\n",
      "             ReLU-88          [-1, 256, 14, 14]               0\n",
      "           Conv2d-89          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-90          [-1, 256, 14, 14]             512\n",
      "             ReLU-91          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-92          [-1, 256, 14, 14]               0\n",
      "           Conv2d-93          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-94          [-1, 256, 14, 14]             512\n",
      "             ReLU-95          [-1, 256, 14, 14]               0\n",
      "           Conv2d-96          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-97          [-1, 256, 14, 14]             512\n",
      "             ReLU-98          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-99          [-1, 256, 14, 14]               0\n",
      "          Conv2d-100            [-1, 512, 7, 7]       1,179,648\n",
      "     BatchNorm2d-101            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-102            [-1, 512, 7, 7]               0\n",
      "          Conv2d-103            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-104            [-1, 512, 7, 7]           1,024\n",
      "          Conv2d-105            [-1, 512, 7, 7]         131,072\n",
      "     BatchNorm2d-106            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-107            [-1, 512, 7, 7]               0\n",
      "      BasicBlock-108            [-1, 512, 7, 7]               0\n",
      "          Conv2d-109            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-110            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-111            [-1, 512, 7, 7]               0\n",
      "          Conv2d-112            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-113            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-114            [-1, 512, 7, 7]               0\n",
      "      BasicBlock-115            [-1, 512, 7, 7]               0\n",
      "          Conv2d-116            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-117            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-118            [-1, 512, 7, 7]               0\n",
      "          Conv2d-119            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-120            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-121            [-1, 512, 7, 7]               0\n",
      "      BasicBlock-122            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-123            [-1, 512, 1, 1]               0\n",
      "          Linear-124                  [-1, 256]         131,328\n",
      "            ReLU-125                  [-1, 256]               0\n",
      "         Dropout-126                  [-1, 256]               0\n",
      "          Linear-127                    [-1, 2]             514\n",
      "================================================================\n",
      "Total params: 21,416,514\n",
      "Trainable params: 131,842\n",
      "Non-trainable params: 21,284,672\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 96.29\n",
      "Params size (MB): 81.70\n",
      "Estimated Total Size (MB): 178.56\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "resnet =resnet.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(resnet.parameters(), lr=0.001)\n",
    "summary(resnet, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1bea8a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9  # The decay factor for each epoch\n",
    "scheduler = lr_scheduler.ExponentialLR(opt, gamma=gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f7e094f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0363afa3a5e64fcdb06727662e2cbebf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?epochs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min loss 0.65\n",
      "Iteration: 0, Loss: 0.65\n",
      "Min loss 0.59\n",
      "Min loss 0.47\n",
      "Epoch: 0/50, Test acc: 54.06, Train acc: 53.77\n",
      "Iteration: 0, Loss: 0.67\n",
      "Epoch: 1/50, Test acc: 52.85, Train acc: 52.93\n",
      "Iteration: 0, Loss: 0.74\n",
      "Epoch: 2/50, Test acc: 52.96, Train acc: 53.69\n",
      "Iteration: 0, Loss: 0.70\n",
      "Epoch: 3/50, Test acc: 53.29, Train acc: 53.54\n",
      "Iteration: 0, Loss: 0.72\n",
      "Epoch: 4/50, Test acc: 53.73, Train acc: 53.54\n",
      "Iteration: 0, Loss: 0.74\n",
      "Epoch: 5/50, Test acc: 53.07, Train acc: 53.52\n",
      "Iteration: 0, Loss: 0.70\n",
      "Epoch: 6/50, Test acc: 53.29, Train acc: 53.59\n",
      "Iteration: 0, Loss: 0.70\n",
      "Epoch: 7/50, Test acc: 53.40, Train acc: 53.87\n",
      "Iteration: 0, Loss: 0.71\n",
      "Epoch: 8/50, Test acc: 54.39, Train acc: 53.81\n",
      "Iteration: 0, Loss: 0.76\n",
      "Epoch: 9/50, Test acc: 53.62, Train acc: 53.54\n",
      "Iteration: 0, Loss: 0.69\n",
      "Epoch: 10/50, Test acc: 53.73, Train acc: 53.15\n",
      "Iteration: 0, Loss: 0.68\n",
      "Epoch: 11/50, Test acc: 53.40, Train acc: 53.11\n",
      "Iteration: 0, Loss: 0.66\n",
      "Epoch: 12/50, Test acc: 53.40, Train acc: 53.36\n",
      "Iteration: 0, Loss: 0.69\n",
      "Epoch: 13/50, Test acc: 52.96, Train acc: 53.05\n",
      "Iteration: 0, Loss: 0.73\n",
      "Epoch: 14/50, Test acc: 53.40, Train acc: 53.40\n",
      "Iteration: 0, Loss: 0.66\n",
      "Epoch: 15/50, Test acc: 53.18, Train acc: 53.07\n",
      "Iteration: 0, Loss: 0.71\n",
      "Epoch: 16/50, Test acc: 53.40, Train acc: 53.24\n",
      "Iteration: 0, Loss: 0.62\n",
      "Epoch: 17/50, Test acc: 53.84, Train acc: 53.63\n",
      "Iteration: 0, Loss: 0.66\n",
      "Epoch: 18/50, Test acc: 54.50, Train acc: 54.30\n",
      "Iteration: 0, Loss: 0.73\n",
      "Epoch: 19/50, Test acc: 53.29, Train acc: 52.68\n",
      "Iteration: 0, Loss: 0.66\n",
      "Epoch: 20/50, Test acc: 53.62, Train acc: 53.15\n",
      "Iteration: 0, Loss: 0.78\n",
      "Epoch: 21/50, Test acc: 53.73, Train acc: 53.22\n",
      "Iteration: 0, Loss: 0.75\n",
      "Epoch: 22/50, Test acc: 53.29, Train acc: 53.28\n",
      "Iteration: 0, Loss: 0.70\n",
      "Epoch: 23/50, Test acc: 53.51, Train acc: 53.54\n",
      "Iteration: 0, Loss: 0.66\n",
      "Epoch: 24/50, Test acc: 54.06, Train acc: 53.22\n",
      "Iteration: 0, Loss: 0.72\n",
      "Epoch: 25/50, Test acc: 54.17, Train acc: 53.83\n",
      "Iteration: 0, Loss: 0.67\n",
      "Epoch: 26/50, Test acc: 54.28, Train acc: 53.65\n",
      "Iteration: 0, Loss: 0.60\n",
      "Epoch: 27/50, Test acc: 53.62, Train acc: 53.46\n",
      "Iteration: 0, Loss: 0.71\n",
      "Epoch: 28/50, Test acc: 53.29, Train acc: 54.00\n",
      "Iteration: 0, Loss: 0.67\n",
      "Epoch: 29/50, Test acc: 54.17, Train acc: 53.42\n",
      "Iteration: 0, Loss: 0.77\n",
      "Epoch: 30/50, Test acc: 53.29, Train acc: 53.63\n",
      "Iteration: 0, Loss: 0.71\n",
      "Epoch: 31/50, Test acc: 53.62, Train acc: 53.48\n",
      "Iteration: 0, Loss: 0.70\n",
      "Epoch: 32/50, Test acc: 53.84, Train acc: 53.89\n",
      "Iteration: 0, Loss: 0.62\n",
      "Epoch: 33/50, Test acc: 53.62, Train acc: 52.72\n",
      "Iteration: 0, Loss: 0.63\n",
      "Epoch: 34/50, Test acc: 54.50, Train acc: 53.77\n",
      "Iteration: 0, Loss: 0.63\n",
      "Epoch: 35/50, Test acc: 54.06, Train acc: 53.42\n",
      "Iteration: 0, Loss: 0.78\n",
      "Epoch: 36/50, Test acc: 53.62, Train acc: 53.77\n",
      "Iteration: 0, Loss: 0.71\n",
      "Epoch: 37/50, Test acc: 53.84, Train acc: 54.18\n",
      "Iteration: 0, Loss: 0.70\n",
      "Epoch: 38/50, Test acc: 53.51, Train acc: 53.65\n",
      "Iteration: 0, Loss: 0.70\n",
      "Epoch: 39/50, Test acc: 53.40, Train acc: 54.22\n",
      "Iteration: 0, Loss: 0.73\n",
      "Epoch: 40/50, Test acc: 53.84, Train acc: 53.19\n",
      "Iteration: 0, Loss: 0.67\n",
      "Epoch: 41/50, Test acc: 54.71, Train acc: 53.89\n",
      "Iteration: 0, Loss: 0.69\n",
      "Epoch: 42/50, Test acc: 53.29, Train acc: 53.50\n",
      "Iteration: 0, Loss: 0.69\n",
      "Epoch: 43/50, Test acc: 53.62, Train acc: 53.93\n",
      "Iteration: 0, Loss: 0.74\n",
      "Epoch: 44/50, Test acc: 53.51, Train acc: 53.22\n",
      "Iteration: 0, Loss: 0.73\n",
      "Epoch: 45/50, Test acc: 53.84, Train acc: 53.67\n",
      "Iteration: 0, Loss: 0.72\n",
      "Epoch: 46/50, Test acc: 53.51, Train acc: 54.06\n",
      "Iteration: 0, Loss: 0.70\n",
      "Epoch: 47/50, Test acc: 53.29, Train acc: 53.56\n",
      "Iteration: 0, Loss: 0.65\n",
      "Epoch: 48/50, Test acc: 54.28, Train acc: 53.26\n",
      "Iteration: 0, Loss: 0.63\n",
      "Epoch: 49/50, Test acc: 53.95, Train acc: 53.42\n"
     ]
    }
   ],
   "source": [
    "best_model, loss_epoch_arr, train_accuracy_lis, val_accuracy_lis, val_loss_epoch_arr = train_func(resnet, loss_fn, opt, trainloader, valloader, scheduler , epochs=50\n",
    "                                                                            )      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1d70649b",
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = DataLoader(test_ds, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8e03926a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53.28947368421053"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation(testloader,resnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8899fa37",
   "metadata": {},
   "source": [
    "# resnet 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cc00dd71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BTBM\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "resnet = models.resnet101(weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e9d38b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in resnet.parameters():\n",
    "    param.requires_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0738c147",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = resnet.fc.in_features\n",
    "new_classifier = nn.Sequential(\n",
    "    nn.Linear(num_features, 256), nn.ReLU(), nn.Dropout(0.2),\n",
    "    nn.Linear(256, 2))\n",
    "resnet.fc = new_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "faa0d516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]           4,096\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-12          [-1, 256, 56, 56]             512\n",
      "           Conv2d-13          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-14          [-1, 256, 56, 56]             512\n",
      "             ReLU-15          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-16          [-1, 256, 56, 56]               0\n",
      "           Conv2d-17           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-18           [-1, 64, 56, 56]             128\n",
      "             ReLU-19           [-1, 64, 56, 56]               0\n",
      "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
      "             ReLU-22           [-1, 64, 56, 56]               0\n",
      "           Conv2d-23          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-24          [-1, 256, 56, 56]             512\n",
      "             ReLU-25          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-26          [-1, 256, 56, 56]               0\n",
      "           Conv2d-27           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-28           [-1, 64, 56, 56]             128\n",
      "             ReLU-29           [-1, 64, 56, 56]               0\n",
      "           Conv2d-30           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-31           [-1, 64, 56, 56]             128\n",
      "             ReLU-32           [-1, 64, 56, 56]               0\n",
      "           Conv2d-33          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-34          [-1, 256, 56, 56]             512\n",
      "             ReLU-35          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-36          [-1, 256, 56, 56]               0\n",
      "           Conv2d-37          [-1, 128, 56, 56]          32,768\n",
      "      BatchNorm2d-38          [-1, 128, 56, 56]             256\n",
      "             ReLU-39          [-1, 128, 56, 56]               0\n",
      "           Conv2d-40          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-41          [-1, 128, 28, 28]             256\n",
      "             ReLU-42          [-1, 128, 28, 28]               0\n",
      "           Conv2d-43          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-44          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-45          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-46          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-47          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-48          [-1, 512, 28, 28]               0\n",
      "           Conv2d-49          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
      "             ReLU-51          [-1, 128, 28, 28]               0\n",
      "           Conv2d-52          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-53          [-1, 128, 28, 28]             256\n",
      "             ReLU-54          [-1, 128, 28, 28]               0\n",
      "           Conv2d-55          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-56          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-57          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-58          [-1, 512, 28, 28]               0\n",
      "           Conv2d-59          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-60          [-1, 128, 28, 28]             256\n",
      "             ReLU-61          [-1, 128, 28, 28]               0\n",
      "           Conv2d-62          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
      "             ReLU-64          [-1, 128, 28, 28]               0\n",
      "           Conv2d-65          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-66          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-67          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-68          [-1, 512, 28, 28]               0\n",
      "           Conv2d-69          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-70          [-1, 128, 28, 28]             256\n",
      "             ReLU-71          [-1, 128, 28, 28]               0\n",
      "           Conv2d-72          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-73          [-1, 128, 28, 28]             256\n",
      "             ReLU-74          [-1, 128, 28, 28]               0\n",
      "           Conv2d-75          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-76          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-77          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-78          [-1, 512, 28, 28]               0\n",
      "           Conv2d-79          [-1, 256, 28, 28]         131,072\n",
      "      BatchNorm2d-80          [-1, 256, 28, 28]             512\n",
      "             ReLU-81          [-1, 256, 28, 28]               0\n",
      "           Conv2d-82          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-83          [-1, 256, 14, 14]             512\n",
      "             ReLU-84          [-1, 256, 14, 14]               0\n",
      "           Conv2d-85         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-86         [-1, 1024, 14, 14]           2,048\n",
      "           Conv2d-87         [-1, 1024, 14, 14]         524,288\n",
      "      BatchNorm2d-88         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-89         [-1, 1024, 14, 14]               0\n",
      "       Bottleneck-90         [-1, 1024, 14, 14]               0\n",
      "           Conv2d-91          [-1, 256, 14, 14]         262,144\n",
      "      BatchNorm2d-92          [-1, 256, 14, 14]             512\n",
      "             ReLU-93          [-1, 256, 14, 14]               0\n",
      "           Conv2d-94          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-95          [-1, 256, 14, 14]             512\n",
      "             ReLU-96          [-1, 256, 14, 14]               0\n",
      "           Conv2d-97         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-98         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-99         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-100         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-101          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-102          [-1, 256, 14, 14]             512\n",
      "            ReLU-103          [-1, 256, 14, 14]               0\n",
      "          Conv2d-104          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-105          [-1, 256, 14, 14]             512\n",
      "            ReLU-106          [-1, 256, 14, 14]               0\n",
      "          Conv2d-107         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-108         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-109         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-110         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-111          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-112          [-1, 256, 14, 14]             512\n",
      "            ReLU-113          [-1, 256, 14, 14]               0\n",
      "          Conv2d-114          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-115          [-1, 256, 14, 14]             512\n",
      "            ReLU-116          [-1, 256, 14, 14]               0\n",
      "          Conv2d-117         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-118         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-119         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-120         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-121          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-122          [-1, 256, 14, 14]             512\n",
      "            ReLU-123          [-1, 256, 14, 14]               0\n",
      "          Conv2d-124          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-125          [-1, 256, 14, 14]             512\n",
      "            ReLU-126          [-1, 256, 14, 14]               0\n",
      "          Conv2d-127         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-128         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-129         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-130         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-131          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-132          [-1, 256, 14, 14]             512\n",
      "            ReLU-133          [-1, 256, 14, 14]               0\n",
      "          Conv2d-134          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-135          [-1, 256, 14, 14]             512\n",
      "            ReLU-136          [-1, 256, 14, 14]               0\n",
      "          Conv2d-137         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-138         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-139         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-140         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-141          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-142          [-1, 256, 14, 14]             512\n",
      "            ReLU-143          [-1, 256, 14, 14]               0\n",
      "          Conv2d-144          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-145          [-1, 256, 14, 14]             512\n",
      "            ReLU-146          [-1, 256, 14, 14]               0\n",
      "          Conv2d-147         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-148         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-149         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-150         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-151          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-152          [-1, 256, 14, 14]             512\n",
      "            ReLU-153          [-1, 256, 14, 14]               0\n",
      "          Conv2d-154          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-155          [-1, 256, 14, 14]             512\n",
      "            ReLU-156          [-1, 256, 14, 14]               0\n",
      "          Conv2d-157         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-158         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-159         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-160         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-161          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-162          [-1, 256, 14, 14]             512\n",
      "            ReLU-163          [-1, 256, 14, 14]               0\n",
      "          Conv2d-164          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-165          [-1, 256, 14, 14]             512\n",
      "            ReLU-166          [-1, 256, 14, 14]               0\n",
      "          Conv2d-167         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-168         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-169         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-170         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-171          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-172          [-1, 256, 14, 14]             512\n",
      "            ReLU-173          [-1, 256, 14, 14]               0\n",
      "          Conv2d-174          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-175          [-1, 256, 14, 14]             512\n",
      "            ReLU-176          [-1, 256, 14, 14]               0\n",
      "          Conv2d-177         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-178         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-179         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-180         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-181          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-182          [-1, 256, 14, 14]             512\n",
      "            ReLU-183          [-1, 256, 14, 14]               0\n",
      "          Conv2d-184          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-185          [-1, 256, 14, 14]             512\n",
      "            ReLU-186          [-1, 256, 14, 14]               0\n",
      "          Conv2d-187         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-188         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-189         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-190         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-191          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-192          [-1, 256, 14, 14]             512\n",
      "            ReLU-193          [-1, 256, 14, 14]               0\n",
      "          Conv2d-194          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-195          [-1, 256, 14, 14]             512\n",
      "            ReLU-196          [-1, 256, 14, 14]               0\n",
      "          Conv2d-197         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-198         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-199         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-200         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-201          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-202          [-1, 256, 14, 14]             512\n",
      "            ReLU-203          [-1, 256, 14, 14]               0\n",
      "          Conv2d-204          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-205          [-1, 256, 14, 14]             512\n",
      "            ReLU-206          [-1, 256, 14, 14]               0\n",
      "          Conv2d-207         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-208         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-209         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-210         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-211          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-212          [-1, 256, 14, 14]             512\n",
      "            ReLU-213          [-1, 256, 14, 14]               0\n",
      "          Conv2d-214          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-215          [-1, 256, 14, 14]             512\n",
      "            ReLU-216          [-1, 256, 14, 14]               0\n",
      "          Conv2d-217         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-218         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-219         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-220         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-221          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-222          [-1, 256, 14, 14]             512\n",
      "            ReLU-223          [-1, 256, 14, 14]               0\n",
      "          Conv2d-224          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-225          [-1, 256, 14, 14]             512\n",
      "            ReLU-226          [-1, 256, 14, 14]               0\n",
      "          Conv2d-227         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-228         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-229         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-230         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-231          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-232          [-1, 256, 14, 14]             512\n",
      "            ReLU-233          [-1, 256, 14, 14]               0\n",
      "          Conv2d-234          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-235          [-1, 256, 14, 14]             512\n",
      "            ReLU-236          [-1, 256, 14, 14]               0\n",
      "          Conv2d-237         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-238         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-239         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-240         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-241          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-242          [-1, 256, 14, 14]             512\n",
      "            ReLU-243          [-1, 256, 14, 14]               0\n",
      "          Conv2d-244          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-245          [-1, 256, 14, 14]             512\n",
      "            ReLU-246          [-1, 256, 14, 14]               0\n",
      "          Conv2d-247         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-248         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-249         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-250         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-251          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-252          [-1, 256, 14, 14]             512\n",
      "            ReLU-253          [-1, 256, 14, 14]               0\n",
      "          Conv2d-254          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-255          [-1, 256, 14, 14]             512\n",
      "            ReLU-256          [-1, 256, 14, 14]               0\n",
      "          Conv2d-257         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-258         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-259         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-260         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-261          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-262          [-1, 256, 14, 14]             512\n",
      "            ReLU-263          [-1, 256, 14, 14]               0\n",
      "          Conv2d-264          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-265          [-1, 256, 14, 14]             512\n",
      "            ReLU-266          [-1, 256, 14, 14]               0\n",
      "          Conv2d-267         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-268         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-269         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-270         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-271          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-272          [-1, 256, 14, 14]             512\n",
      "            ReLU-273          [-1, 256, 14, 14]               0\n",
      "          Conv2d-274          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-275          [-1, 256, 14, 14]             512\n",
      "            ReLU-276          [-1, 256, 14, 14]               0\n",
      "          Conv2d-277         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-278         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-279         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-280         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-281          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-282          [-1, 256, 14, 14]             512\n",
      "            ReLU-283          [-1, 256, 14, 14]               0\n",
      "          Conv2d-284          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-285          [-1, 256, 14, 14]             512\n",
      "            ReLU-286          [-1, 256, 14, 14]               0\n",
      "          Conv2d-287         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-288         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-289         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-290         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-291          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-292          [-1, 256, 14, 14]             512\n",
      "            ReLU-293          [-1, 256, 14, 14]               0\n",
      "          Conv2d-294          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-295          [-1, 256, 14, 14]             512\n",
      "            ReLU-296          [-1, 256, 14, 14]               0\n",
      "          Conv2d-297         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-298         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-299         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-300         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-301          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-302          [-1, 256, 14, 14]             512\n",
      "            ReLU-303          [-1, 256, 14, 14]               0\n",
      "          Conv2d-304          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-305          [-1, 256, 14, 14]             512\n",
      "            ReLU-306          [-1, 256, 14, 14]               0\n",
      "          Conv2d-307         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-308         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-309         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-310         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-311          [-1, 512, 14, 14]         524,288\n",
      "     BatchNorm2d-312          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-313          [-1, 512, 14, 14]               0\n",
      "          Conv2d-314            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-315            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-316            [-1, 512, 7, 7]               0\n",
      "          Conv2d-317           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-318           [-1, 2048, 7, 7]           4,096\n",
      "          Conv2d-319           [-1, 2048, 7, 7]       2,097,152\n",
      "     BatchNorm2d-320           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-321           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-322           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-323            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-324            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-325            [-1, 512, 7, 7]               0\n",
      "          Conv2d-326            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-327            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-328            [-1, 512, 7, 7]               0\n",
      "          Conv2d-329           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-330           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-331           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-332           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-333            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-334            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-335            [-1, 512, 7, 7]               0\n",
      "          Conv2d-336            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-337            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-338            [-1, 512, 7, 7]               0\n",
      "          Conv2d-339           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-340           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-341           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-342           [-1, 2048, 7, 7]               0\n",
      "AdaptiveAvgPool2d-343           [-1, 2048, 1, 1]               0\n",
      "          Linear-344                  [-1, 256]         524,544\n",
      "            ReLU-345                  [-1, 256]               0\n",
      "         Dropout-346                  [-1, 256]               0\n",
      "          Linear-347                    [-1, 2]             514\n",
      "================================================================\n",
      "Total params: 43,025,218\n",
      "Trainable params: 525,058\n",
      "Non-trainable params: 42,500,160\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 429.73\n",
      "Params size (MB): 164.13\n",
      "Estimated Total Size (MB): 594.43\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "resnet =resnet.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(resnet.parameters(), lr=0.001)\n",
    "summary(resnet, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "13ccc9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9  # The decay factor for each epoch\n",
    "scheduler = lr_scheduler.ExponentialLR(opt, gamma=gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ac1aedc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "096300f1e9e146d8824e51d585de4dc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?epochs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min loss 0.74\n",
      "Iteration: 0, Loss: 0.74\n",
      "Min loss 0.73\n",
      "Min loss 0.72\n",
      "Min loss 0.69\n",
      "Min loss 0.69\n",
      "Min loss 0.65\n",
      "Min loss 0.65\n",
      "Min loss 0.58\n",
      "Min loss 0.58\n",
      "Epoch: 0/50, Test acc: 55.26, Train acc: 56.40\n",
      "Iteration: 0, Loss: 0.76\n",
      "Epoch: 1/50, Test acc: 54.50, Train acc: 55.78\n",
      "Iteration: 0, Loss: 0.64\n",
      "Min loss 0.57\n",
      "Min loss 0.56\n",
      "Min loss 0.56\n",
      "Epoch: 2/50, Test acc: 55.04, Train acc: 56.69\n",
      "Iteration: 0, Loss: 0.68\n",
      "Min loss 0.55\n",
      "Epoch: 3/50, Test acc: 54.82, Train acc: 55.56\n",
      "Iteration: 0, Loss: 0.81\n",
      "Epoch: 4/50, Test acc: 55.48, Train acc: 56.46\n",
      "Iteration: 0, Loss: 0.74\n",
      "Epoch: 5/50, Test acc: 55.37, Train acc: 56.32\n",
      "Iteration: 0, Loss: 0.62\n",
      "Min loss 0.54\n",
      "Epoch: 6/50, Test acc: 54.71, Train acc: 55.68\n",
      "Iteration: 0, Loss: 0.67\n",
      "Epoch: 7/50, Test acc: 54.50, Train acc: 55.66\n",
      "Iteration: 0, Loss: 0.69\n",
      "Min loss 0.50\n",
      "Epoch: 8/50, Test acc: 55.70, Train acc: 56.34\n",
      "Iteration: 0, Loss: 0.69\n",
      "Epoch: 9/50, Test acc: 54.28, Train acc: 55.64\n",
      "Iteration: 0, Loss: 0.76\n",
      "Epoch: 10/50, Test acc: 56.14, Train acc: 56.44\n",
      "Iteration: 0, Loss: 0.70\n",
      "Epoch: 11/50, Test acc: 55.15, Train acc: 56.61\n",
      "Iteration: 0, Loss: 0.73\n",
      "Epoch: 12/50, Test acc: 56.36, Train acc: 56.56\n",
      "Iteration: 0, Loss: 0.68\n",
      "Epoch: 13/50, Test acc: 55.04, Train acc: 56.09\n",
      "Iteration: 0, Loss: 0.71\n",
      "Epoch: 14/50, Test acc: 55.92, Train acc: 56.22\n",
      "Iteration: 0, Loss: 0.65\n",
      "Epoch: 15/50, Test acc: 55.70, Train acc: 55.52\n",
      "Iteration: 0, Loss: 0.68\n",
      "Epoch: 16/50, Test acc: 55.26, Train acc: 55.85\n",
      "Iteration: 0, Loss: 0.71\n",
      "Epoch: 17/50, Test acc: 54.61, Train acc: 56.13\n",
      "Iteration: 0, Loss: 0.69\n",
      "Epoch: 18/50, Test acc: 55.04, Train acc: 56.05\n",
      "Iteration: 0, Loss: 0.74\n",
      "Epoch: 19/50, Test acc: 54.50, Train acc: 55.70\n",
      "Iteration: 0, Loss: 0.70\n",
      "Min loss 0.49\n",
      "Epoch: 20/50, Test acc: 54.82, Train acc: 56.59\n",
      "Iteration: 0, Loss: 0.69\n",
      "Epoch: 21/50, Test acc: 55.26, Train acc: 56.46\n",
      "Iteration: 0, Loss: 0.66\n",
      "Epoch: 22/50, Test acc: 56.25, Train acc: 56.40\n",
      "Iteration: 0, Loss: 0.69\n",
      "Epoch: 23/50, Test acc: 55.15, Train acc: 56.40\n",
      "Iteration: 0, Loss: 0.70\n",
      "Epoch: 24/50, Test acc: 55.70, Train acc: 55.93\n",
      "Iteration: 0, Loss: 0.75\n",
      "Epoch: 25/50, Test acc: 55.70, Train acc: 56.11\n",
      "Iteration: 0, Loss: 0.63\n",
      "Epoch: 26/50, Test acc: 56.03, Train acc: 55.91\n",
      "Iteration: 0, Loss: 0.78\n",
      "Epoch: 27/50, Test acc: 55.04, Train acc: 55.97\n",
      "Iteration: 0, Loss: 0.64\n",
      "Epoch: 28/50, Test acc: 55.37, Train acc: 56.11\n",
      "Iteration: 0, Loss: 0.60\n",
      "Epoch: 29/50, Test acc: 56.36, Train acc: 56.05\n",
      "Iteration: 0, Loss: 0.70\n",
      "Epoch: 30/50, Test acc: 55.70, Train acc: 56.07\n",
      "Iteration: 0, Loss: 0.75\n",
      "Min loss 0.39\n",
      "Epoch: 31/50, Test acc: 55.81, Train acc: 56.30\n",
      "Iteration: 0, Loss: 0.63\n",
      "Epoch: 32/50, Test acc: 55.37, Train acc: 55.87\n",
      "Iteration: 0, Loss: 0.64\n",
      "Epoch: 33/50, Test acc: 56.03, Train acc: 55.97\n",
      "Iteration: 0, Loss: 0.71\n",
      "Epoch: 34/50, Test acc: 55.26, Train acc: 56.01\n",
      "Iteration: 0, Loss: 0.72\n",
      "Epoch: 35/50, Test acc: 55.04, Train acc: 56.40\n",
      "Iteration: 0, Loss: 0.63\n",
      "Epoch: 36/50, Test acc: 55.59, Train acc: 56.38\n",
      "Iteration: 0, Loss: 0.70\n",
      "Epoch: 37/50, Test acc: 55.04, Train acc: 56.32\n",
      "Iteration: 0, Loss: 0.61\n",
      "Epoch: 38/50, Test acc: 55.15, Train acc: 56.28\n",
      "Iteration: 0, Loss: 0.69\n",
      "Epoch: 39/50, Test acc: 54.82, Train acc: 56.13\n",
      "Iteration: 0, Loss: 0.64\n",
      "Epoch: 40/50, Test acc: 55.70, Train acc: 55.72\n",
      "Iteration: 0, Loss: 0.78\n",
      "Epoch: 41/50, Test acc: 54.39, Train acc: 55.85\n",
      "Iteration: 0, Loss: 0.67\n",
      "Epoch: 42/50, Test acc: 55.26, Train acc: 56.34\n",
      "Iteration: 0, Loss: 0.66\n",
      "Epoch: 43/50, Test acc: 55.59, Train acc: 56.36\n",
      "Iteration: 0, Loss: 0.61\n",
      "Epoch: 44/50, Test acc: 55.70, Train acc: 56.59\n",
      "Iteration: 0, Loss: 0.69\n",
      "Epoch: 45/50, Test acc: 55.70, Train acc: 55.78\n",
      "Iteration: 0, Loss: 0.65\n",
      "Epoch: 46/50, Test acc: 55.26, Train acc: 55.97\n",
      "Iteration: 0, Loss: 0.68\n",
      "Epoch: 47/50, Test acc: 55.70, Train acc: 56.28\n",
      "Iteration: 0, Loss: 0.63\n",
      "Epoch: 48/50, Test acc: 56.47, Train acc: 56.50\n",
      "Iteration: 0, Loss: 0.64\n",
      "Epoch: 49/50, Test acc: 54.17, Train acc: 56.46\n"
     ]
    }
   ],
   "source": [
    "best_model, loss_epoch_arr, train_accuracy_lis, val_accuracy_lis, val_loss_epoch_arr = train_func(resnet, loss_fn, opt, trainloader, valloader, scheduler , epochs=50\n",
    "                                                                            )      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7f98e956",
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = DataLoader(test_ds, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "527ddd70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51.31578947368421"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation(testloader,resnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be6e6b5",
   "metadata": {},
   "source": [
    "# VGG-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "09532ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BTBM\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\BTBM\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_BN_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_BN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "vgg16 = models.vgg16_bn(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "48f748b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in vgg16.features.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fd2304cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = vgg16.classifier[6].in_features\n",
    "vgg16.classifier[6] = nn.Linear(num_features, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "46d6b58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16 = vgg16.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(vgg16.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b9204e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9  # The decay factor for each epoch\n",
    "scheduler = lr_scheduler.ExponentialLR(opt, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a609c1f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "006d92bc7cac4f0da24cd57464661cd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?epochs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min loss 0.77\n",
      "Iteration: 0, Loss: 0.77\n",
      "Min loss 0.72\n",
      "Min loss 0.69\n",
      "Min loss 0.67\n",
      "Min loss 0.51\n",
      "Min loss 0.50\n",
      "Min loss 0.48\n",
      "Min loss 0.45\n",
      "Min loss 0.40\n",
      "Epoch: 0/50, Test acc: 61.84, Train acc: 62.44\n",
      "Iteration: 0, Loss: 0.92\n",
      "Epoch: 1/50, Test acc: 62.94, Train acc: 62.75\n",
      "Iteration: 0, Loss: 0.65\n",
      "Min loss 0.38\n",
      "Epoch: 2/50, Test acc: 62.83, Train acc: 62.42\n",
      "Iteration: 0, Loss: 0.83\n",
      "Epoch: 3/50, Test acc: 63.05, Train acc: 62.83\n",
      "Iteration: 0, Loss: 0.61\n",
      "Epoch: 4/50, Test acc: 63.05, Train acc: 62.54\n",
      "Iteration: 0, Loss: 0.86\n",
      "Min loss 0.37\n",
      "Epoch: 5/50, Test acc: 63.60, Train acc: 62.73\n",
      "Iteration: 0, Loss: 0.76\n",
      "Epoch: 6/50, Test acc: 63.60, Train acc: 62.91\n",
      "Iteration: 0, Loss: 0.78\n",
      "Epoch: 7/50, Test acc: 63.16, Train acc: 62.75\n",
      "Iteration: 0, Loss: 0.90\n",
      "Epoch: 8/50, Test acc: 63.38, Train acc: 62.54\n",
      "Iteration: 0, Loss: 0.77\n",
      "Epoch: 9/50, Test acc: 62.28, Train acc: 62.61\n",
      "Iteration: 0, Loss: 1.05\n",
      "Epoch: 10/50, Test acc: 63.82, Train acc: 62.75\n",
      "Iteration: 0, Loss: 0.89\n",
      "Epoch: 11/50, Test acc: 61.84, Train acc: 62.52\n",
      "Iteration: 0, Loss: 0.97\n",
      "Epoch: 12/50, Test acc: 63.49, Train acc: 62.73\n",
      "Iteration: 0, Loss: 0.70\n",
      "Epoch: 13/50, Test acc: 62.94, Train acc: 62.77\n",
      "Iteration: 0, Loss: 0.70\n",
      "Epoch: 14/50, Test acc: 63.38, Train acc: 62.73\n",
      "Iteration: 0, Loss: 0.69\n",
      "Epoch: 15/50, Test acc: 63.82, Train acc: 62.67\n",
      "Iteration: 0, Loss: 0.86\n",
      "Epoch: 16/50, Test acc: 63.05, Train acc: 62.58\n",
      "Iteration: 0, Loss: 1.08\n",
      "Epoch: 17/50, Test acc: 62.50, Train acc: 62.83\n",
      "Iteration: 0, Loss: 0.74\n",
      "Epoch: 18/50, Test acc: 62.50, Train acc: 62.52\n",
      "Iteration: 0, Loss: 0.69\n",
      "Epoch: 19/50, Test acc: 63.49, Train acc: 62.56\n",
      "Iteration: 0, Loss: 0.97\n",
      "Epoch: 20/50, Test acc: 63.27, Train acc: 62.95\n",
      "Iteration: 0, Loss: 0.88\n",
      "Epoch: 21/50, Test acc: 62.83, Train acc: 62.83\n",
      "Iteration: 0, Loss: 0.71\n",
      "Epoch: 22/50, Test acc: 61.95, Train acc: 62.42\n",
      "Iteration: 0, Loss: 0.71\n",
      "Epoch: 23/50, Test acc: 63.05, Train acc: 62.67\n",
      "Iteration: 0, Loss: 0.66\n",
      "Epoch: 24/50, Test acc: 62.28, Train acc: 62.63\n",
      "Iteration: 0, Loss: 1.04\n",
      "Epoch: 25/50, Test acc: 63.82, Train acc: 62.61\n",
      "Iteration: 0, Loss: 0.77\n",
      "Min loss 0.36\n",
      "Epoch: 26/50, Test acc: 63.49, Train acc: 62.87\n",
      "Iteration: 0, Loss: 0.82\n",
      "Epoch: 27/50, Test acc: 62.28, Train acc: 62.36\n",
      "Iteration: 0, Loss: 0.71\n",
      "Epoch: 28/50, Test acc: 62.61, Train acc: 62.75\n",
      "Iteration: 0, Loss: 0.94\n",
      "Min loss 0.30\n",
      "Min loss 0.28\n",
      "Epoch: 29/50, Test acc: 62.39, Train acc: 62.40\n",
      "Iteration: 0, Loss: 0.82\n",
      "Epoch: 30/50, Test acc: 63.05, Train acc: 62.50\n",
      "Iteration: 0, Loss: 0.48\n",
      "Epoch: 31/50, Test acc: 63.49, Train acc: 62.61\n",
      "Iteration: 0, Loss: 0.89\n",
      "Epoch: 32/50, Test acc: 62.50, Train acc: 62.58\n",
      "Iteration: 0, Loss: 0.79\n",
      "Epoch: 33/50, Test acc: 63.38, Train acc: 62.65\n",
      "Iteration: 0, Loss: 0.52\n",
      "Epoch: 34/50, Test acc: 61.95, Train acc: 62.30\n",
      "Iteration: 0, Loss: 0.96\n",
      "Epoch: 35/50, Test acc: 63.38, Train acc: 62.56\n",
      "Iteration: 0, Loss: 0.91\n",
      "Epoch: 36/50, Test acc: 63.05, Train acc: 62.32\n",
      "Iteration: 0, Loss: 0.70\n",
      "Epoch: 37/50, Test acc: 63.16, Train acc: 62.40\n",
      "Iteration: 0, Loss: 0.88\n",
      "Epoch: 38/50, Test acc: 61.95, Train acc: 62.63\n",
      "Iteration: 0, Loss: 0.72\n",
      "Epoch: 39/50, Test acc: 63.49, Train acc: 62.46\n",
      "Iteration: 0, Loss: 0.54\n",
      "Epoch: 40/50, Test acc: 63.05, Train acc: 62.54\n",
      "Iteration: 0, Loss: 0.88\n",
      "Epoch: 41/50, Test acc: 63.60, Train acc: 62.56\n",
      "Iteration: 0, Loss: 0.69\n",
      "Epoch: 42/50, Test acc: 62.94, Train acc: 62.58\n",
      "Iteration: 0, Loss: 0.72\n",
      "Epoch: 43/50, Test acc: 63.27, Train acc: 62.48\n",
      "Iteration: 0, Loss: 0.79\n",
      "Epoch: 44/50, Test acc: 63.27, Train acc: 62.54\n",
      "Iteration: 0, Loss: 0.98\n",
      "Epoch: 45/50, Test acc: 62.50, Train acc: 62.56\n",
      "Iteration: 0, Loss: 0.70\n",
      "Epoch: 46/50, Test acc: 63.82, Train acc: 62.85\n",
      "Iteration: 0, Loss: 0.69\n",
      "Epoch: 47/50, Test acc: 63.16, Train acc: 62.95\n",
      "Iteration: 0, Loss: 0.61\n",
      "Epoch: 48/50, Test acc: 62.28, Train acc: 62.44\n",
      "Iteration: 0, Loss: 0.79\n",
      "Epoch: 49/50, Test acc: 61.84, Train acc: 62.44\n"
     ]
    }
   ],
   "source": [
    "best_model, loss_epoch_arr, train_accuracy_lis, val_accuracy_lis, val_loss_epoch_arr = train_func(vgg16, loss_fn, opt, trainloader, valloader, scheduler , epochs=50\n",
    "                                                                            )      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ff9264c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = DataLoader(test_ds, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "612cc87e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59.86842105263158"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation(testloader,vgg16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211d13c9",
   "metadata": {},
   "source": [
    "# VGG-19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8d814e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BTBM\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_BN_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_BN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "vgg19 = models.vgg19_bn(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "65b34257",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in vgg19.features.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9b8e8ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = vgg19.classifier[6].in_features\n",
    "vgg19.classifier[6] = nn.Linear(num_features, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "842262ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg19 = vgg19.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(vgg19.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8ee9bd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9  # The decay factor for each epoch\n",
    "scheduler = lr_scheduler.ExponentialLR(opt, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d57cf00f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f87ad5b72f1445d6bc5ca82797435903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?epochs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min loss 0.73\n",
      "Iteration: 0, Loss: 0.73\n",
      "Min loss 0.65\n",
      "Min loss 0.60\n",
      "Min loss 0.54\n",
      "Min loss 0.50\n",
      "Epoch: 0/50, Test acc: 60.75, Train acc: 61.62\n",
      "Iteration: 0, Loss: 0.75\n",
      "Min loss 0.50\n",
      "Epoch: 1/50, Test acc: 60.42, Train acc: 61.45\n",
      "Iteration: 0, Loss: 0.83\n",
      "Min loss 0.49\n",
      "Epoch: 2/50, Test acc: 60.31, Train acc: 61.27\n",
      "Iteration: 0, Loss: 0.68\n",
      "Min loss 0.46\n",
      "Min loss 0.44\n",
      "Epoch: 3/50, Test acc: 60.31, Train acc: 61.11\n",
      "Iteration: 0, Loss: 0.70\n",
      "Min loss 0.41\n",
      "Epoch: 4/50, Test acc: 60.42, Train acc: 61.48\n",
      "Iteration: 0, Loss: 0.76\n",
      "Epoch: 5/50, Test acc: 60.53, Train acc: 61.54\n",
      "Iteration: 0, Loss: 0.65\n",
      "Epoch: 6/50, Test acc: 60.86, Train acc: 61.68\n",
      "Iteration: 0, Loss: 0.72\n",
      "Epoch: 7/50, Test acc: 60.31, Train acc: 61.48\n",
      "Iteration: 0, Loss: 0.65\n",
      "Min loss 0.40\n",
      "Epoch: 8/50, Test acc: 60.09, Train acc: 60.94\n",
      "Iteration: 0, Loss: 0.81\n",
      "Epoch: 9/50, Test acc: 60.96, Train acc: 61.48\n",
      "Iteration: 0, Loss: 0.75\n",
      "Epoch: 10/50, Test acc: 60.42, Train acc: 60.94\n",
      "Iteration: 0, Loss: 0.83\n",
      "Epoch: 11/50, Test acc: 60.31, Train acc: 61.33\n",
      "Iteration: 0, Loss: 0.64\n",
      "Epoch: 12/50, Test acc: 60.31, Train acc: 61.50\n",
      "Iteration: 0, Loss: 0.62\n",
      "Epoch: 13/50, Test acc: 60.75, Train acc: 61.70\n",
      "Iteration: 0, Loss: 0.82\n",
      "Epoch: 14/50, Test acc: 60.20, Train acc: 61.50\n",
      "Iteration: 0, Loss: 0.80\n",
      "Epoch: 15/50, Test acc: 59.76, Train acc: 60.98\n",
      "Iteration: 0, Loss: 0.77\n",
      "Epoch: 16/50, Test acc: 60.86, Train acc: 61.48\n",
      "Iteration: 0, Loss: 0.76\n",
      "Epoch: 17/50, Test acc: 60.64, Train acc: 61.54\n",
      "Iteration: 0, Loss: 0.88\n",
      "Epoch: 18/50, Test acc: 59.76, Train acc: 61.15\n",
      "Iteration: 0, Loss: 0.77\n",
      "Epoch: 19/50, Test acc: 59.98, Train acc: 60.82\n",
      "Iteration: 0, Loss: 0.88\n",
      "Epoch: 20/50, Test acc: 59.87, Train acc: 61.41\n",
      "Iteration: 0, Loss: 0.84\n",
      "Epoch: 21/50, Test acc: 60.42, Train acc: 61.41\n",
      "Iteration: 0, Loss: 0.93\n",
      "Epoch: 22/50, Test acc: 60.42, Train acc: 61.39\n",
      "Iteration: 0, Loss: 0.86\n",
      "Epoch: 23/50, Test acc: 60.42, Train acc: 61.13\n",
      "Iteration: 0, Loss: 0.89\n",
      "Epoch: 24/50, Test acc: 60.53, Train acc: 61.54\n",
      "Iteration: 0, Loss: 0.81\n",
      "Min loss 0.38\n",
      "Epoch: 25/50, Test acc: 60.09, Train acc: 61.04\n",
      "Iteration: 0, Loss: 0.69\n",
      "Epoch: 26/50, Test acc: 60.09, Train acc: 61.41\n",
      "Iteration: 0, Loss: 0.76\n",
      "Epoch: 27/50, Test acc: 61.29, Train acc: 61.52\n",
      "Iteration: 0, Loss: 0.80\n",
      "Epoch: 28/50, Test acc: 60.64, Train acc: 61.50\n",
      "Iteration: 0, Loss: 0.67\n",
      "Epoch: 29/50, Test acc: 60.64, Train acc: 61.66\n",
      "Iteration: 0, Loss: 0.67\n",
      "Epoch: 30/50, Test acc: 60.96, Train acc: 61.54\n",
      "Iteration: 0, Loss: 0.64\n",
      "Epoch: 31/50, Test acc: 60.53, Train acc: 61.56\n",
      "Iteration: 0, Loss: 0.69\n",
      "Epoch: 32/50, Test acc: 60.31, Train acc: 61.00\n",
      "Iteration: 0, Loss: 0.63\n",
      "Epoch: 33/50, Test acc: 60.31, Train acc: 61.17\n",
      "Iteration: 0, Loss: 0.75\n",
      "Epoch: 34/50, Test acc: 60.64, Train acc: 61.68\n",
      "Iteration: 0, Loss: 0.77\n",
      "Epoch: 35/50, Test acc: 59.98, Train acc: 60.94\n",
      "Iteration: 0, Loss: 0.74\n",
      "Epoch: 36/50, Test acc: 60.64, Train acc: 61.27\n",
      "Iteration: 0, Loss: 0.69\n",
      "Epoch: 37/50, Test acc: 60.96, Train acc: 61.68\n",
      "Iteration: 0, Loss: 1.01\n",
      "Epoch: 38/50, Test acc: 60.09, Train acc: 61.35\n",
      "Iteration: 0, Loss: 0.85\n",
      "Epoch: 39/50, Test acc: 60.09, Train acc: 61.02\n",
      "Iteration: 0, Loss: 0.65\n",
      "Epoch: 40/50, Test acc: 60.31, Train acc: 61.48\n",
      "Iteration: 0, Loss: 0.60\n",
      "Epoch: 41/50, Test acc: 60.20, Train acc: 61.25\n",
      "Iteration: 0, Loss: 0.63\n",
      "Epoch: 42/50, Test acc: 60.42, Train acc: 60.92\n",
      "Iteration: 0, Loss: 0.77\n",
      "Epoch: 43/50, Test acc: 60.31, Train acc: 61.23\n",
      "Iteration: 0, Loss: 0.84\n",
      "Epoch: 44/50, Test acc: 60.42, Train acc: 61.19\n",
      "Iteration: 0, Loss: 0.71\n",
      "Epoch: 45/50, Test acc: 60.31, Train acc: 61.60\n",
      "Iteration: 0, Loss: 0.68\n",
      "Epoch: 46/50, Test acc: 60.42, Train acc: 60.88\n",
      "Iteration: 0, Loss: 0.58\n",
      "Epoch: 47/50, Test acc: 60.42, Train acc: 61.43\n",
      "Iteration: 0, Loss: 0.53\n",
      "Epoch: 48/50, Test acc: 60.09, Train acc: 61.31\n",
      "Iteration: 0, Loss: 0.66\n",
      "Epoch: 49/50, Test acc: 60.31, Train acc: 61.27\n"
     ]
    }
   ],
   "source": [
    "best_model, loss_epoch_arr, train_accuracy_lis, val_accuracy_lis, val_loss_epoch_arr = train_func(vgg19, loss_fn, opt, trainloader, valloader, scheduler , epochs=50\n",
    "                                                                            )      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "770f63c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = DataLoader(test_ds, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2f4cdf03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57.23684210526316"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation(testloader,vgg19)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d616fc3",
   "metadata": {},
   "source": [
    "# Densenet121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a1b999af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BTBM\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "densenet121 = models.densenet121(weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bd0787b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in densenet121.features.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a9b28d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = densenet121.classifier.in_features\n",
    "densenet121.classifier = nn.Linear(num_features, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "75be32f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "densenet121 = densenet121.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(densenet121.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fb8df2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9  # The decay factor for each epoch\n",
    "scheduler = lr_scheduler.ExponentialLR(opt, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a0dbface",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c10826ef14b64a659fa9d37f525ceaf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?epochs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min loss 0.68\n",
      "Iteration: 0, Loss: 0.68\n",
      "Min loss 0.68\n",
      "Min loss 0.66\n",
      "Min loss 0.59\n",
      "Epoch: 0/50, Test acc: 46.82, Train acc: 48.67\n",
      "Min loss 0.58\n",
      "Iteration: 0, Loss: 0.58\n",
      "Epoch: 1/50, Test acc: 46.71, Train acc: 48.59\n",
      "Iteration: 0, Loss: 0.67\n",
      "Epoch: 2/50, Test acc: 47.15, Train acc: 49.31\n",
      "Iteration: 0, Loss: 0.78\n",
      "Epoch: 3/50, Test acc: 47.04, Train acc: 48.92\n",
      "Iteration: 0, Loss: 0.70\n",
      "Epoch: 4/50, Test acc: 46.93, Train acc: 48.45\n",
      "Iteration: 0, Loss: 0.77\n",
      "Min loss 0.58\n",
      "Epoch: 5/50, Test acc: 45.94, Train acc: 48.59\n",
      "Iteration: 0, Loss: 0.61\n",
      "Epoch: 6/50, Test acc: 47.59, Train acc: 48.96\n",
      "Iteration: 0, Loss: 0.69\n",
      "Min loss 0.58\n",
      "Epoch: 7/50, Test acc: 46.38, Train acc: 48.70\n",
      "Iteration: 0, Loss: 0.70\n",
      "Min loss 0.58\n",
      "Epoch: 8/50, Test acc: 47.48, Train acc: 49.04\n",
      "Iteration: 0, Loss: 0.69\n",
      "Epoch: 9/50, Test acc: 46.93, Train acc: 48.84\n",
      "Iteration: 0, Loss: 0.77\n",
      "Epoch: 10/50, Test acc: 46.60, Train acc: 49.11\n",
      "Iteration: 0, Loss: 0.75\n",
      "Min loss 0.58\n",
      "Epoch: 11/50, Test acc: 47.26, Train acc: 49.17\n",
      "Iteration: 0, Loss: 0.69\n",
      "Epoch: 12/50, Test acc: 46.82, Train acc: 49.21\n",
      "Iteration: 0, Loss: 0.71\n",
      "Min loss 0.58\n",
      "Epoch: 13/50, Test acc: 46.60, Train acc: 48.80\n",
      "Iteration: 0, Loss: 0.68\n",
      "Min loss 0.56\n",
      "Epoch: 14/50, Test acc: 47.15, Train acc: 48.57\n",
      "Iteration: 0, Loss: 0.77\n",
      "Epoch: 15/50, Test acc: 47.48, Train acc: 48.92\n",
      "Iteration: 0, Loss: 0.77\n",
      "Min loss 0.51\n",
      "Epoch: 16/50, Test acc: 46.27, Train acc: 49.00\n",
      "Iteration: 0, Loss: 0.66\n",
      "Epoch: 17/50, Test acc: 46.82, Train acc: 48.88\n",
      "Iteration: 0, Loss: 0.74\n",
      "Epoch: 18/50, Test acc: 47.81, Train acc: 49.15\n",
      "Iteration: 0, Loss: 0.67\n",
      "Epoch: 19/50, Test acc: 47.48, Train acc: 48.39\n",
      "Iteration: 0, Loss: 0.79\n",
      "Epoch: 20/50, Test acc: 45.83, Train acc: 48.28\n",
      "Iteration: 0, Loss: 0.65\n",
      "Epoch: 21/50, Test acc: 45.94, Train acc: 48.84\n",
      "Iteration: 0, Loss: 0.77\n",
      "Epoch: 22/50, Test acc: 47.37, Train acc: 48.65\n",
      "Iteration: 0, Loss: 0.71\n",
      "Epoch: 23/50, Test acc: 46.82, Train acc: 48.63\n",
      "Iteration: 0, Loss: 0.65\n",
      "Epoch: 24/50, Test acc: 46.82, Train acc: 48.76\n",
      "Iteration: 0, Loss: 0.70\n",
      "Epoch: 25/50, Test acc: 47.37, Train acc: 49.15\n",
      "Iteration: 0, Loss: 0.72\n",
      "Epoch: 26/50, Test acc: 47.37, Train acc: 49.66\n",
      "Iteration: 0, Loss: 0.67\n",
      "Epoch: 27/50, Test acc: 46.38, Train acc: 48.94\n",
      "Iteration: 0, Loss: 0.77\n",
      "Epoch: 28/50, Test acc: 47.04, Train acc: 49.00\n",
      "Iteration: 0, Loss: 0.70\n",
      "Epoch: 29/50, Test acc: 46.49, Train acc: 49.19\n",
      "Iteration: 0, Loss: 0.77\n",
      "Epoch: 30/50, Test acc: 46.16, Train acc: 48.41\n",
      "Iteration: 0, Loss: 0.64\n",
      "Epoch: 31/50, Test acc: 47.15, Train acc: 48.70\n",
      "Iteration: 0, Loss: 0.75\n",
      "Epoch: 32/50, Test acc: 47.81, Train acc: 49.19\n",
      "Iteration: 0, Loss: 0.68\n",
      "Epoch: 33/50, Test acc: 46.82, Train acc: 49.00\n",
      "Iteration: 0, Loss: 0.69\n",
      "Epoch: 34/50, Test acc: 46.16, Train acc: 48.76\n",
      "Iteration: 0, Loss: 0.69\n",
      "Epoch: 35/50, Test acc: 46.82, Train acc: 48.41\n",
      "Iteration: 0, Loss: 0.70\n",
      "Epoch: 36/50, Test acc: 46.05, Train acc: 48.35\n",
      "Iteration: 0, Loss: 0.72\n",
      "Epoch: 37/50, Test acc: 46.49, Train acc: 49.00\n",
      "Iteration: 0, Loss: 0.62\n",
      "Epoch: 38/50, Test acc: 46.27, Train acc: 48.00\n",
      "Iteration: 0, Loss: 0.72\n",
      "Epoch: 39/50, Test acc: 46.82, Train acc: 48.78\n",
      "Iteration: 0, Loss: 0.69\n",
      "Epoch: 40/50, Test acc: 47.26, Train acc: 48.61\n",
      "Iteration: 0, Loss: 0.66\n",
      "Epoch: 41/50, Test acc: 47.37, Train acc: 48.86\n",
      "Iteration: 0, Loss: 0.71\n",
      "Epoch: 42/50, Test acc: 46.93, Train acc: 48.96\n",
      "Iteration: 0, Loss: 0.68\n",
      "Epoch: 43/50, Test acc: 46.38, Train acc: 49.15\n",
      "Iteration: 0, Loss: 0.69\n",
      "Epoch: 44/50, Test acc: 46.38, Train acc: 48.20\n",
      "Iteration: 0, Loss: 0.63\n",
      "Epoch: 45/50, Test acc: 47.70, Train acc: 48.00\n",
      "Iteration: 0, Loss: 0.64\n",
      "Epoch: 46/50, Test acc: 46.49, Train acc: 48.94\n",
      "Iteration: 0, Loss: 0.64\n",
      "Epoch: 47/50, Test acc: 45.50, Train acc: 48.65\n",
      "Iteration: 0, Loss: 0.75\n",
      "Epoch: 48/50, Test acc: 46.49, Train acc: 49.43\n",
      "Iteration: 0, Loss: 0.69\n",
      "Epoch: 49/50, Test acc: 46.49, Train acc: 48.82\n"
     ]
    }
   ],
   "source": [
    "best_model, loss_epoch_arr, train_accuracy_lis, val_accuracy_lis, val_loss_epoch_arr = train_func(densenet121, loss_fn, opt, trainloader, valloader, scheduler , epochs=50\n",
    "                                                                            )      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "61184d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = DataLoader(test_ds, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ab40a4d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51.31578947368421"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation(testloader,densenet121)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9377bc8e",
   "metadata": {},
   "source": [
    "# DenseNet201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a13b1d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BTBM\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\BTBM\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet201_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet201_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "densenet201 = models.densenet201(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b5af2bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in densenet201.features.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f1a728bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = densenet201.classifier.in_features\n",
    "densenet201.classifier = nn.Linear(num_features, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "972cba92",
   "metadata": {},
   "outputs": [],
   "source": [
    "densenet201 = densenet201.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(densenet201.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "21556599",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9  # The decay factor for each epoch\n",
    "scheduler = lr_scheduler.ExponentialLR(opt, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "48225c2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0ec50239534461894d401b6dcd3c5b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?epochs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min loss 0.69\n",
      "Iteration: 0, Loss: 0.69\n",
      "Min loss 0.64\n",
      "Min loss 0.58\n",
      "Min loss 0.58\n",
      "Min loss 0.55\n",
      "Min loss 0.55\n",
      "Min loss 0.54\n",
      "Min loss 0.48\n",
      "Epoch: 0/50, Test acc: 65.13, Train acc: 67.50\n",
      "Iteration: 0, Loss: 0.65\n",
      "Epoch: 1/50, Test acc: 64.91, Train acc: 67.13\n",
      "Iteration: 0, Loss: 0.72\n",
      "Epoch: 2/50, Test acc: 65.24, Train acc: 67.56\n",
      "Iteration: 0, Loss: 0.63\n",
      "Min loss 0.47\n",
      "Min loss 0.46\n",
      "Epoch: 3/50, Test acc: 65.46, Train acc: 67.43\n",
      "Iteration: 0, Loss: 0.65\n",
      "Epoch: 4/50, Test acc: 64.91, Train acc: 67.50\n",
      "Iteration: 0, Loss: 0.60\n",
      "Epoch: 5/50, Test acc: 65.79, Train acc: 67.68\n",
      "Iteration: 0, Loss: 0.59\n",
      "Epoch: 6/50, Test acc: 65.46, Train acc: 67.87\n",
      "Iteration: 0, Loss: 0.56\n",
      "Epoch: 7/50, Test acc: 65.02, Train acc: 67.17\n",
      "Iteration: 0, Loss: 0.58\n",
      "Epoch: 8/50, Test acc: 66.01, Train acc: 67.84\n",
      "Iteration: 0, Loss: 0.59\n",
      "Epoch: 9/50, Test acc: 64.80, Train acc: 67.41\n",
      "Iteration: 0, Loss: 0.57\n",
      "Epoch: 10/50, Test acc: 65.13, Train acc: 67.21\n",
      "Iteration: 0, Loss: 0.57\n",
      "Epoch: 11/50, Test acc: 65.57, Train acc: 68.21\n",
      "Iteration: 0, Loss: 0.61\n",
      "Epoch: 12/50, Test acc: 65.35, Train acc: 67.23\n",
      "Iteration: 0, Loss: 0.58\n",
      "Epoch: 13/50, Test acc: 65.46, Train acc: 67.95\n",
      "Iteration: 0, Loss: 0.56\n",
      "Epoch: 14/50, Test acc: 65.57, Train acc: 67.84\n",
      "Iteration: 0, Loss: 0.67\n",
      "Epoch: 15/50, Test acc: 65.02, Train acc: 67.17\n",
      "Iteration: 0, Loss: 0.55\n",
      "Epoch: 16/50, Test acc: 66.01, Train acc: 67.74\n",
      "Iteration: 0, Loss: 0.63\n",
      "Min loss 0.44\n",
      "Epoch: 17/50, Test acc: 66.01, Train acc: 67.95\n",
      "Iteration: 0, Loss: 0.60\n",
      "Epoch: 18/50, Test acc: 66.01, Train acc: 68.03\n",
      "Iteration: 0, Loss: 0.74\n",
      "Epoch: 19/50, Test acc: 65.46, Train acc: 67.33\n",
      "Iteration: 0, Loss: 0.65\n",
      "Epoch: 20/50, Test acc: 64.80, Train acc: 67.23\n",
      "Iteration: 0, Loss: 0.62\n",
      "Epoch: 21/50, Test acc: 65.35, Train acc: 67.47\n",
      "Iteration: 0, Loss: 0.62\n",
      "Epoch: 22/50, Test acc: 65.13, Train acc: 67.56\n",
      "Iteration: 0, Loss: 0.66\n",
      "Epoch: 23/50, Test acc: 64.80, Train acc: 67.17\n",
      "Iteration: 0, Loss: 0.57\n",
      "Epoch: 24/50, Test acc: 65.57, Train acc: 67.95\n",
      "Iteration: 0, Loss: 0.51\n",
      "Epoch: 25/50, Test acc: 65.90, Train acc: 68.01\n",
      "Iteration: 0, Loss: 0.66\n",
      "Epoch: 26/50, Test acc: 65.57, Train acc: 67.64\n",
      "Iteration: 0, Loss: 0.72\n",
      "Epoch: 27/50, Test acc: 65.35, Train acc: 67.37\n",
      "Iteration: 0, Loss: 0.63\n",
      "Epoch: 28/50, Test acc: 65.46, Train acc: 67.52\n",
      "Iteration: 0, Loss: 0.56\n",
      "Epoch: 29/50, Test acc: 65.90, Train acc: 67.97\n",
      "Iteration: 0, Loss: 0.61\n",
      "Epoch: 30/50, Test acc: 65.46, Train acc: 67.93\n",
      "Iteration: 0, Loss: 0.71\n",
      "Epoch: 31/50, Test acc: 66.12, Train acc: 67.43\n",
      "Iteration: 0, Loss: 0.62\n",
      "Epoch: 32/50, Test acc: 65.35, Train acc: 67.43\n",
      "Iteration: 0, Loss: 0.60\n",
      "Epoch: 33/50, Test acc: 64.69, Train acc: 67.25\n",
      "Iteration: 0, Loss: 0.68\n",
      "Epoch: 34/50, Test acc: 65.46, Train acc: 67.41\n",
      "Iteration: 0, Loss: 0.71\n",
      "Epoch: 35/50, Test acc: 65.13, Train acc: 67.52\n",
      "Iteration: 0, Loss: 0.68\n",
      "Epoch: 36/50, Test acc: 65.13, Train acc: 67.33\n",
      "Iteration: 0, Loss: 0.61\n",
      "Epoch: 37/50, Test acc: 65.35, Train acc: 67.76\n",
      "Iteration: 0, Loss: 0.72\n",
      "Epoch: 38/50, Test acc: 64.80, Train acc: 67.43\n",
      "Iteration: 0, Loss: 0.66\n",
      "Epoch: 39/50, Test acc: 65.90, Train acc: 67.89\n",
      "Iteration: 0, Loss: 0.66\n",
      "Epoch: 40/50, Test acc: 65.57, Train acc: 67.89\n",
      "Iteration: 0, Loss: 0.61\n",
      "Epoch: 41/50, Test acc: 65.35, Train acc: 67.66\n",
      "Iteration: 0, Loss: 0.66\n",
      "Epoch: 42/50, Test acc: 65.02, Train acc: 67.35\n",
      "Iteration: 0, Loss: 0.60\n",
      "Epoch: 43/50, Test acc: 66.01, Train acc: 67.84\n",
      "Iteration: 0, Loss: 0.61\n",
      "Epoch: 44/50, Test acc: 65.35, Train acc: 67.52\n",
      "Iteration: 0, Loss: 0.57\n",
      "Epoch: 45/50, Test acc: 65.35, Train acc: 67.74\n",
      "Iteration: 0, Loss: 0.52\n",
      "Epoch: 46/50, Test acc: 65.57, Train acc: 67.89\n",
      "Iteration: 0, Loss: 0.62\n",
      "Epoch: 47/50, Test acc: 65.13, Train acc: 67.66\n",
      "Iteration: 0, Loss: 0.73\n",
      "Min loss 0.43\n",
      "Epoch: 48/50, Test acc: 65.24, Train acc: 67.31\n",
      "Iteration: 0, Loss: 0.64\n",
      "Epoch: 49/50, Test acc: 65.68, Train acc: 67.58\n"
     ]
    }
   ],
   "source": [
    "best_model, loss_epoch_arr, train_accuracy_lis, val_accuracy_lis, val_loss_epoch_arr = train_func(densenet201, loss_fn, opt, trainloader, valloader, scheduler , epochs=100\n",
    "                                                                            )      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c9dce4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = DataLoader(test_ds, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bb57a212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.14473684210526"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation(testloader,densenet201)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7737bc1c",
   "metadata": {},
   "source": [
    "# Data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "acba0aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1']\n"
     ]
    }
   ],
   "source": [
    "data_dir='G:\\DCDA-Net\\Data_1'\n",
    "print(os.listdir(data_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5f417145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4f6f3cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Resize(size=(224,224)),\n",
    "                transforms.Normalize((0.3570, 0.7503, 0.5559), (0.3445, 0.0633, 0.2608))\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "721934a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4867, 912, 304)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset=ImageFolder(data_dir,transform=transform)\n",
    "torch.manual_seed(42)\n",
    "train_ds, val_ds, test_ds = random_split(dataset, [0.8,0.15,0.05])\n",
    "len(train_ds),len(val_ds),len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "95c3d542",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader =DataLoader(train_ds, batch_size=16, shuffle=True,num_workers=4)\n",
    "valloader = DataLoader(val_ds, batch_size=32, shuffle=True,num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c3b39023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(dataloader, model):\n",
    "    total, correct = 0, 0\n",
    "    model.eval()\n",
    "    for data in dataloader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, pred = torch.max(outputs.data, 1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (pred == labels).sum().item()\n",
    "    return 100 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1f7341e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func(model, loss_fn, opt, trainloader, valloader, scheduler , epochs=50):\n",
    "    loss_epoch_arr = []\n",
    "    train_accuracy_lis = []\n",
    "    val_accuracy_lis = []\n",
    "    val_loss_epoch_arr = []\n",
    "\n",
    "    max_epochs = epochs\n",
    "    min_loss = 1000\n",
    "    model.train()\n",
    "\n",
    "    for epoch in notebook.tqdm(range(max_epochs), total=max_epochs, unit=\"epochs\"):\n",
    "        model.train()\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "    \n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            opt.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            if min_loss > loss.item():\n",
    "                min_loss = loss.item()\n",
    "                best_model = copy.deepcopy(model.state_dict())\n",
    "                print(\"Min loss %0.2f\" %min_loss)\n",
    "            del inputs, labels, outputs\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            if i % 2000 == 0:\n",
    "                print('Iteration: %d, Loss: %0.2f' % (i, loss.item()))\n",
    "            \n",
    "       \n",
    "        loss_epoch_arr.append(loss.item())\n",
    "        train_accuracy = evaluation(trainloader, model)\n",
    "        val_accuracy = evaluation(valloader, model)\n",
    "        print('Epoch: %d/%d, Test acc: %0.2f, Train acc: %0.2f' % (\n",
    "              epoch, max_epochs, val_accuracy, train_accuracy))\n",
    "        train_accuracy_lis.append(train_accuracy)\n",
    "        val_accuracy_lis.append(val_accuracy)\n",
    "    \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(valloader, 0):\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "\n",
    "        del inputs, labels, outputs\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        val_loss_epoch_arr.append(loss.item())\n",
    "    return best_model, loss_epoch_arr, train_accuracy_lis, val_accuracy_lis, val_loss_epoch_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360dfaec",
   "metadata": {},
   "source": [
    "# Resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "43e9b4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = models.resnet50(weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "90e8b4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in resnet.parameters():\n",
    "    param.requires_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "00b0eb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = resnet.fc.in_features\n",
    "new_classifier = nn.Sequential(\n",
    "    nn.Linear(num_features, 256), nn.ReLU(), nn.Dropout(0.2),\n",
    "    nn.Linear(256, 2))\n",
    "resnet.fc = new_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "88ff8a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]           4,096\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-12          [-1, 256, 56, 56]             512\n",
      "           Conv2d-13          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-14          [-1, 256, 56, 56]             512\n",
      "             ReLU-15          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-16          [-1, 256, 56, 56]               0\n",
      "           Conv2d-17           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-18           [-1, 64, 56, 56]             128\n",
      "             ReLU-19           [-1, 64, 56, 56]               0\n",
      "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
      "             ReLU-22           [-1, 64, 56, 56]               0\n",
      "           Conv2d-23          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-24          [-1, 256, 56, 56]             512\n",
      "             ReLU-25          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-26          [-1, 256, 56, 56]               0\n",
      "           Conv2d-27           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-28           [-1, 64, 56, 56]             128\n",
      "             ReLU-29           [-1, 64, 56, 56]               0\n",
      "           Conv2d-30           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-31           [-1, 64, 56, 56]             128\n",
      "             ReLU-32           [-1, 64, 56, 56]               0\n",
      "           Conv2d-33          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-34          [-1, 256, 56, 56]             512\n",
      "             ReLU-35          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-36          [-1, 256, 56, 56]               0\n",
      "           Conv2d-37          [-1, 128, 56, 56]          32,768\n",
      "      BatchNorm2d-38          [-1, 128, 56, 56]             256\n",
      "             ReLU-39          [-1, 128, 56, 56]               0\n",
      "           Conv2d-40          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-41          [-1, 128, 28, 28]             256\n",
      "             ReLU-42          [-1, 128, 28, 28]               0\n",
      "           Conv2d-43          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-44          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-45          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-46          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-47          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-48          [-1, 512, 28, 28]               0\n",
      "           Conv2d-49          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
      "             ReLU-51          [-1, 128, 28, 28]               0\n",
      "           Conv2d-52          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-53          [-1, 128, 28, 28]             256\n",
      "             ReLU-54          [-1, 128, 28, 28]               0\n",
      "           Conv2d-55          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-56          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-57          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-58          [-1, 512, 28, 28]               0\n",
      "           Conv2d-59          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-60          [-1, 128, 28, 28]             256\n",
      "             ReLU-61          [-1, 128, 28, 28]               0\n",
      "           Conv2d-62          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
      "             ReLU-64          [-1, 128, 28, 28]               0\n",
      "           Conv2d-65          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-66          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-67          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-68          [-1, 512, 28, 28]               0\n",
      "           Conv2d-69          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-70          [-1, 128, 28, 28]             256\n",
      "             ReLU-71          [-1, 128, 28, 28]               0\n",
      "           Conv2d-72          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-73          [-1, 128, 28, 28]             256\n",
      "             ReLU-74          [-1, 128, 28, 28]               0\n",
      "           Conv2d-75          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-76          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-77          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-78          [-1, 512, 28, 28]               0\n",
      "           Conv2d-79          [-1, 256, 28, 28]         131,072\n",
      "      BatchNorm2d-80          [-1, 256, 28, 28]             512\n",
      "             ReLU-81          [-1, 256, 28, 28]               0\n",
      "           Conv2d-82          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-83          [-1, 256, 14, 14]             512\n",
      "             ReLU-84          [-1, 256, 14, 14]               0\n",
      "           Conv2d-85         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-86         [-1, 1024, 14, 14]           2,048\n",
      "           Conv2d-87         [-1, 1024, 14, 14]         524,288\n",
      "      BatchNorm2d-88         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-89         [-1, 1024, 14, 14]               0\n",
      "       Bottleneck-90         [-1, 1024, 14, 14]               0\n",
      "           Conv2d-91          [-1, 256, 14, 14]         262,144\n",
      "      BatchNorm2d-92          [-1, 256, 14, 14]             512\n",
      "             ReLU-93          [-1, 256, 14, 14]               0\n",
      "           Conv2d-94          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-95          [-1, 256, 14, 14]             512\n",
      "             ReLU-96          [-1, 256, 14, 14]               0\n",
      "           Conv2d-97         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-98         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-99         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-100         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-101          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-102          [-1, 256, 14, 14]             512\n",
      "            ReLU-103          [-1, 256, 14, 14]               0\n",
      "          Conv2d-104          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-105          [-1, 256, 14, 14]             512\n",
      "            ReLU-106          [-1, 256, 14, 14]               0\n",
      "          Conv2d-107         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-108         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-109         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-110         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-111          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-112          [-1, 256, 14, 14]             512\n",
      "            ReLU-113          [-1, 256, 14, 14]               0\n",
      "          Conv2d-114          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-115          [-1, 256, 14, 14]             512\n",
      "            ReLU-116          [-1, 256, 14, 14]               0\n",
      "          Conv2d-117         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-118         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-119         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-120         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-121          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-122          [-1, 256, 14, 14]             512\n",
      "            ReLU-123          [-1, 256, 14, 14]               0\n",
      "          Conv2d-124          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-125          [-1, 256, 14, 14]             512\n",
      "            ReLU-126          [-1, 256, 14, 14]               0\n",
      "          Conv2d-127         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-128         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-129         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-130         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-131          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-132          [-1, 256, 14, 14]             512\n",
      "            ReLU-133          [-1, 256, 14, 14]               0\n",
      "          Conv2d-134          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-135          [-1, 256, 14, 14]             512\n",
      "            ReLU-136          [-1, 256, 14, 14]               0\n",
      "          Conv2d-137         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-138         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-139         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-140         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-141          [-1, 512, 14, 14]         524,288\n",
      "     BatchNorm2d-142          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-143          [-1, 512, 14, 14]               0\n",
      "          Conv2d-144            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-145            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-146            [-1, 512, 7, 7]               0\n",
      "          Conv2d-147           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-148           [-1, 2048, 7, 7]           4,096\n",
      "          Conv2d-149           [-1, 2048, 7, 7]       2,097,152\n",
      "     BatchNorm2d-150           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-151           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-152           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-153            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-154            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-155            [-1, 512, 7, 7]               0\n",
      "          Conv2d-156            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-157            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-158            [-1, 512, 7, 7]               0\n",
      "          Conv2d-159           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-160           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-161           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-162           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-163            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-164            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-165            [-1, 512, 7, 7]               0\n",
      "          Conv2d-166            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-167            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-168            [-1, 512, 7, 7]               0\n",
      "          Conv2d-169           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-170           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-171           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-172           [-1, 2048, 7, 7]               0\n",
      "AdaptiveAvgPool2d-173           [-1, 2048, 1, 1]               0\n",
      "          Linear-174                  [-1, 256]         524,544\n",
      "            ReLU-175                  [-1, 256]               0\n",
      "         Dropout-176                  [-1, 256]               0\n",
      "          Linear-177                    [-1, 2]             514\n",
      "================================================================\n",
      "Total params: 24,033,090\n",
      "Trainable params: 525,058\n",
      "Non-trainable params: 23,508,032\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 286.56\n",
      "Params size (MB): 91.68\n",
      "Estimated Total Size (MB): 378.81\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "resnet =resnet.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(resnet.parameters(), lr=0.01)\n",
    "summary(resnet, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4ef330f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.8  # The decay factor for each epoch\n",
    "scheduler = lr_scheduler.ExponentialLR(opt, gamma=gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "556d4f89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "519382a1cb7d4231b5fa82a71f2a2771",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?epochs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min loss 0.69\n",
      "Iteration: 0, Loss: 0.69\n",
      "Min loss 0.44\n",
      "Min loss 0.42\n",
      "Epoch: 0/50, Test acc: 49.78, Train acc: 51.12\n",
      "Iteration: 0, Loss: 0.81\n",
      "Epoch: 1/50, Test acc: 50.00, Train acc: 51.26\n",
      "Iteration: 0, Loss: 1.19\n",
      "Epoch: 2/50, Test acc: 49.56, Train acc: 51.00\n",
      "Iteration: 0, Loss: 1.02\n",
      "Min loss 0.34\n",
      "Epoch: 3/50, Test acc: 49.67, Train acc: 50.96\n",
      "Iteration: 0, Loss: 2.08\n",
      "Epoch: 4/50, Test acc: 49.56, Train acc: 51.30\n",
      "Iteration: 0, Loss: 1.20\n",
      "Min loss 0.33\n",
      "Epoch: 5/50, Test acc: 49.45, Train acc: 51.04\n",
      "Iteration: 0, Loss: 1.34\n",
      "Epoch: 6/50, Test acc: 49.56, Train acc: 50.75\n",
      "Iteration: 0, Loss: 1.39\n",
      "Epoch: 7/50, Test acc: 49.12, Train acc: 51.08\n",
      "Iteration: 0, Loss: 0.70\n",
      "Epoch: 8/50, Test acc: 49.23, Train acc: 51.02\n",
      "Iteration: 0, Loss: 0.73\n",
      "Epoch: 9/50, Test acc: 49.12, Train acc: 50.91\n",
      "Iteration: 0, Loss: 1.10\n",
      "Min loss 0.31\n",
      "Min loss 0.17\n",
      "Epoch: 10/50, Test acc: 48.68, Train acc: 50.69\n",
      "Iteration: 0, Loss: 1.07\n",
      "Epoch: 11/50, Test acc: 49.45, Train acc: 50.69\n",
      "Iteration: 0, Loss: 1.11\n",
      "Epoch: 12/50, Test acc: 48.90, Train acc: 50.61\n",
      "Iteration: 0, Loss: 0.99\n",
      "Epoch: 13/50, Test acc: 49.01, Train acc: 51.26\n",
      "Iteration: 0, Loss: 1.32\n",
      "Epoch: 14/50, Test acc: 49.23, Train acc: 51.14\n",
      "Iteration: 0, Loss: 1.24\n",
      "Epoch: 15/50, Test acc: 49.12, Train acc: 50.93\n",
      "Iteration: 0, Loss: 1.26\n",
      "Epoch: 16/50, Test acc: 49.67, Train acc: 51.04\n",
      "Iteration: 0, Loss: 1.23\n",
      "Epoch: 17/50, Test acc: 49.01, Train acc: 51.10\n",
      "Iteration: 0, Loss: 1.02\n",
      "Epoch: 18/50, Test acc: 49.56, Train acc: 50.83\n",
      "Iteration: 0, Loss: 0.74\n",
      "Epoch: 19/50, Test acc: 49.78, Train acc: 51.35\n",
      "Iteration: 0, Loss: 1.12\n",
      "Epoch: 20/50, Test acc: 49.78, Train acc: 50.81\n",
      "Iteration: 0, Loss: 1.36\n",
      "Epoch: 21/50, Test acc: 49.34, Train acc: 51.18\n",
      "Iteration: 0, Loss: 0.89\n",
      "Epoch: 22/50, Test acc: 49.78, Train acc: 51.04\n",
      "Iteration: 0, Loss: 1.22\n",
      "Epoch: 23/50, Test acc: 49.67, Train acc: 51.02\n",
      "Iteration: 0, Loss: 1.22\n",
      "Epoch: 24/50, Test acc: 49.56, Train acc: 51.12\n",
      "Iteration: 0, Loss: 0.90\n",
      "Epoch: 25/50, Test acc: 49.34, Train acc: 50.81\n",
      "Iteration: 0, Loss: 1.05\n",
      "Epoch: 26/50, Test acc: 49.56, Train acc: 50.98\n",
      "Iteration: 0, Loss: 1.65\n",
      "Epoch: 27/50, Test acc: 49.23, Train acc: 50.67\n",
      "Iteration: 0, Loss: 1.49\n",
      "Epoch: 28/50, Test acc: 49.12, Train acc: 50.71\n",
      "Iteration: 0, Loss: 1.62\n",
      "Epoch: 29/50, Test acc: 49.45, Train acc: 51.33\n",
      "Iteration: 0, Loss: 1.40\n",
      "Epoch: 30/50, Test acc: 50.11, Train acc: 51.33\n",
      "Iteration: 0, Loss: 1.34\n",
      "Epoch: 31/50, Test acc: 49.12, Train acc: 50.87\n",
      "Iteration: 0, Loss: 1.55\n",
      "Epoch: 32/50, Test acc: 49.34, Train acc: 50.81\n",
      "Iteration: 0, Loss: 0.79\n",
      "Epoch: 33/50, Test acc: 49.89, Train acc: 51.20\n",
      "Iteration: 0, Loss: 0.61\n",
      "Epoch: 34/50, Test acc: 49.56, Train acc: 51.35\n",
      "Iteration: 0, Loss: 1.19\n",
      "Epoch: 35/50, Test acc: 49.78, Train acc: 51.06\n",
      "Iteration: 0, Loss: 1.59\n",
      "Epoch: 36/50, Test acc: 49.45, Train acc: 51.26\n",
      "Iteration: 0, Loss: 1.04\n",
      "Epoch: 37/50, Test acc: 49.56, Train acc: 51.02\n",
      "Iteration: 0, Loss: 1.26\n",
      "Epoch: 38/50, Test acc: 49.78, Train acc: 50.81\n",
      "Iteration: 0, Loss: 1.51\n",
      "Epoch: 39/50, Test acc: 49.78, Train acc: 50.85\n",
      "Iteration: 0, Loss: 0.82\n",
      "Epoch: 40/50, Test acc: 49.56, Train acc: 50.89\n",
      "Iteration: 0, Loss: 1.18\n",
      "Epoch: 41/50, Test acc: 49.45, Train acc: 51.10\n",
      "Iteration: 0, Loss: 1.39\n",
      "Epoch: 42/50, Test acc: 49.89, Train acc: 50.93\n",
      "Iteration: 0, Loss: 1.33\n",
      "Epoch: 43/50, Test acc: 49.12, Train acc: 50.79\n",
      "Iteration: 0, Loss: 1.03\n",
      "Epoch: 44/50, Test acc: 50.00, Train acc: 51.16\n",
      "Iteration: 0, Loss: 1.22\n",
      "Epoch: 45/50, Test acc: 50.00, Train acc: 51.37\n",
      "Iteration: 0, Loss: 1.46\n",
      "Epoch: 46/50, Test acc: 48.90, Train acc: 51.39\n",
      "Iteration: 0, Loss: 1.80\n",
      "Epoch: 47/50, Test acc: 49.67, Train acc: 50.87\n",
      "Iteration: 0, Loss: 0.88\n",
      "Epoch: 48/50, Test acc: 48.90, Train acc: 50.77\n",
      "Iteration: 0, Loss: 0.89\n",
      "Epoch: 49/50, Test acc: 49.34, Train acc: 51.37\n"
     ]
    }
   ],
   "source": [
    "best_model, loss_epoch_arr, train_accuracy_lis, val_accuracy_lis, val_loss_epoch_arr = train_func(resnet, loss_fn, opt, trainloader, valloader, scheduler , epochs=50\n",
    "                                                                            )      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9fd1027b",
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = DataLoader(test_ds, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e08416c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49.3421052631579"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation(testloader,resnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f935744",
   "metadata": {},
   "source": [
    "# Resnet-152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "54b2a663",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = models.resnet152(weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "15e0929d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in resnet.parameters():\n",
    "    param.requires_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "335ec501",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = resnet.fc.in_features\n",
    "new_classifier = nn.Sequential(\n",
    "    nn.Linear(num_features, 256), nn.ReLU(), nn.Dropout(0.2),\n",
    "    nn.Linear(256, 2))\n",
    "resnet.fc = new_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e1404cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]           4,096\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-12          [-1, 256, 56, 56]             512\n",
      "           Conv2d-13          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-14          [-1, 256, 56, 56]             512\n",
      "             ReLU-15          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-16          [-1, 256, 56, 56]               0\n",
      "           Conv2d-17           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-18           [-1, 64, 56, 56]             128\n",
      "             ReLU-19           [-1, 64, 56, 56]               0\n",
      "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
      "             ReLU-22           [-1, 64, 56, 56]               0\n",
      "           Conv2d-23          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-24          [-1, 256, 56, 56]             512\n",
      "             ReLU-25          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-26          [-1, 256, 56, 56]               0\n",
      "           Conv2d-27           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-28           [-1, 64, 56, 56]             128\n",
      "             ReLU-29           [-1, 64, 56, 56]               0\n",
      "           Conv2d-30           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-31           [-1, 64, 56, 56]             128\n",
      "             ReLU-32           [-1, 64, 56, 56]               0\n",
      "           Conv2d-33          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-34          [-1, 256, 56, 56]             512\n",
      "             ReLU-35          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-36          [-1, 256, 56, 56]               0\n",
      "           Conv2d-37          [-1, 128, 56, 56]          32,768\n",
      "      BatchNorm2d-38          [-1, 128, 56, 56]             256\n",
      "             ReLU-39          [-1, 128, 56, 56]               0\n",
      "           Conv2d-40          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-41          [-1, 128, 28, 28]             256\n",
      "             ReLU-42          [-1, 128, 28, 28]               0\n",
      "           Conv2d-43          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-44          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-45          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-46          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-47          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-48          [-1, 512, 28, 28]               0\n",
      "           Conv2d-49          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
      "             ReLU-51          [-1, 128, 28, 28]               0\n",
      "           Conv2d-52          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-53          [-1, 128, 28, 28]             256\n",
      "             ReLU-54          [-1, 128, 28, 28]               0\n",
      "           Conv2d-55          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-56          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-57          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-58          [-1, 512, 28, 28]               0\n",
      "           Conv2d-59          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-60          [-1, 128, 28, 28]             256\n",
      "             ReLU-61          [-1, 128, 28, 28]               0\n",
      "           Conv2d-62          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
      "             ReLU-64          [-1, 128, 28, 28]               0\n",
      "           Conv2d-65          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-66          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-67          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-68          [-1, 512, 28, 28]               0\n",
      "           Conv2d-69          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-70          [-1, 128, 28, 28]             256\n",
      "             ReLU-71          [-1, 128, 28, 28]               0\n",
      "           Conv2d-72          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-73          [-1, 128, 28, 28]             256\n",
      "             ReLU-74          [-1, 128, 28, 28]               0\n",
      "           Conv2d-75          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-76          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-77          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-78          [-1, 512, 28, 28]               0\n",
      "           Conv2d-79          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-80          [-1, 128, 28, 28]             256\n",
      "             ReLU-81          [-1, 128, 28, 28]               0\n",
      "           Conv2d-82          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-83          [-1, 128, 28, 28]             256\n",
      "             ReLU-84          [-1, 128, 28, 28]               0\n",
      "           Conv2d-85          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-86          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-87          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-88          [-1, 512, 28, 28]               0\n",
      "           Conv2d-89          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-90          [-1, 128, 28, 28]             256\n",
      "             ReLU-91          [-1, 128, 28, 28]               0\n",
      "           Conv2d-92          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-93          [-1, 128, 28, 28]             256\n",
      "             ReLU-94          [-1, 128, 28, 28]               0\n",
      "           Conv2d-95          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-96          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-97          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-98          [-1, 512, 28, 28]               0\n",
      "           Conv2d-99          [-1, 128, 28, 28]          65,536\n",
      "     BatchNorm2d-100          [-1, 128, 28, 28]             256\n",
      "            ReLU-101          [-1, 128, 28, 28]               0\n",
      "          Conv2d-102          [-1, 128, 28, 28]         147,456\n",
      "     BatchNorm2d-103          [-1, 128, 28, 28]             256\n",
      "            ReLU-104          [-1, 128, 28, 28]               0\n",
      "          Conv2d-105          [-1, 512, 28, 28]          65,536\n",
      "     BatchNorm2d-106          [-1, 512, 28, 28]           1,024\n",
      "            ReLU-107          [-1, 512, 28, 28]               0\n",
      "      Bottleneck-108          [-1, 512, 28, 28]               0\n",
      "          Conv2d-109          [-1, 128, 28, 28]          65,536\n",
      "     BatchNorm2d-110          [-1, 128, 28, 28]             256\n",
      "            ReLU-111          [-1, 128, 28, 28]               0\n",
      "          Conv2d-112          [-1, 128, 28, 28]         147,456\n",
      "     BatchNorm2d-113          [-1, 128, 28, 28]             256\n",
      "            ReLU-114          [-1, 128, 28, 28]               0\n",
      "          Conv2d-115          [-1, 512, 28, 28]          65,536\n",
      "     BatchNorm2d-116          [-1, 512, 28, 28]           1,024\n",
      "            ReLU-117          [-1, 512, 28, 28]               0\n",
      "      Bottleneck-118          [-1, 512, 28, 28]               0\n",
      "          Conv2d-119          [-1, 256, 28, 28]         131,072\n",
      "     BatchNorm2d-120          [-1, 256, 28, 28]             512\n",
      "            ReLU-121          [-1, 256, 28, 28]               0\n",
      "          Conv2d-122          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-123          [-1, 256, 14, 14]             512\n",
      "            ReLU-124          [-1, 256, 14, 14]               0\n",
      "          Conv2d-125         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-126         [-1, 1024, 14, 14]           2,048\n",
      "          Conv2d-127         [-1, 1024, 14, 14]         524,288\n",
      "     BatchNorm2d-128         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-129         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-130         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-131          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-132          [-1, 256, 14, 14]             512\n",
      "            ReLU-133          [-1, 256, 14, 14]               0\n",
      "          Conv2d-134          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-135          [-1, 256, 14, 14]             512\n",
      "            ReLU-136          [-1, 256, 14, 14]               0\n",
      "          Conv2d-137         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-138         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-139         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-140         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-141          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-142          [-1, 256, 14, 14]             512\n",
      "            ReLU-143          [-1, 256, 14, 14]               0\n",
      "          Conv2d-144          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-145          [-1, 256, 14, 14]             512\n",
      "            ReLU-146          [-1, 256, 14, 14]               0\n",
      "          Conv2d-147         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-148         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-149         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-150         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-151          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-152          [-1, 256, 14, 14]             512\n",
      "            ReLU-153          [-1, 256, 14, 14]               0\n",
      "          Conv2d-154          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-155          [-1, 256, 14, 14]             512\n",
      "            ReLU-156          [-1, 256, 14, 14]               0\n",
      "          Conv2d-157         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-158         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-159         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-160         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-161          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-162          [-1, 256, 14, 14]             512\n",
      "            ReLU-163          [-1, 256, 14, 14]               0\n",
      "          Conv2d-164          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-165          [-1, 256, 14, 14]             512\n",
      "            ReLU-166          [-1, 256, 14, 14]               0\n",
      "          Conv2d-167         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-168         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-169         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-170         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-171          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-172          [-1, 256, 14, 14]             512\n",
      "            ReLU-173          [-1, 256, 14, 14]               0\n",
      "          Conv2d-174          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-175          [-1, 256, 14, 14]             512\n",
      "            ReLU-176          [-1, 256, 14, 14]               0\n",
      "          Conv2d-177         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-178         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-179         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-180         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-181          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-182          [-1, 256, 14, 14]             512\n",
      "            ReLU-183          [-1, 256, 14, 14]               0\n",
      "          Conv2d-184          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-185          [-1, 256, 14, 14]             512\n",
      "            ReLU-186          [-1, 256, 14, 14]               0\n",
      "          Conv2d-187         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-188         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-189         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-190         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-191          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-192          [-1, 256, 14, 14]             512\n",
      "            ReLU-193          [-1, 256, 14, 14]               0\n",
      "          Conv2d-194          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-195          [-1, 256, 14, 14]             512\n",
      "            ReLU-196          [-1, 256, 14, 14]               0\n",
      "          Conv2d-197         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-198         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-199         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-200         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-201          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-202          [-1, 256, 14, 14]             512\n",
      "            ReLU-203          [-1, 256, 14, 14]               0\n",
      "          Conv2d-204          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-205          [-1, 256, 14, 14]             512\n",
      "            ReLU-206          [-1, 256, 14, 14]               0\n",
      "          Conv2d-207         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-208         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-209         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-210         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-211          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-212          [-1, 256, 14, 14]             512\n",
      "            ReLU-213          [-1, 256, 14, 14]               0\n",
      "          Conv2d-214          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-215          [-1, 256, 14, 14]             512\n",
      "            ReLU-216          [-1, 256, 14, 14]               0\n",
      "          Conv2d-217         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-218         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-219         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-220         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-221          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-222          [-1, 256, 14, 14]             512\n",
      "            ReLU-223          [-1, 256, 14, 14]               0\n",
      "          Conv2d-224          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-225          [-1, 256, 14, 14]             512\n",
      "            ReLU-226          [-1, 256, 14, 14]               0\n",
      "          Conv2d-227         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-228         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-229         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-230         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-231          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-232          [-1, 256, 14, 14]             512\n",
      "            ReLU-233          [-1, 256, 14, 14]               0\n",
      "          Conv2d-234          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-235          [-1, 256, 14, 14]             512\n",
      "            ReLU-236          [-1, 256, 14, 14]               0\n",
      "          Conv2d-237         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-238         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-239         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-240         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-241          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-242          [-1, 256, 14, 14]             512\n",
      "            ReLU-243          [-1, 256, 14, 14]               0\n",
      "          Conv2d-244          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-245          [-1, 256, 14, 14]             512\n",
      "            ReLU-246          [-1, 256, 14, 14]               0\n",
      "          Conv2d-247         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-248         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-249         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-250         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-251          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-252          [-1, 256, 14, 14]             512\n",
      "            ReLU-253          [-1, 256, 14, 14]               0\n",
      "          Conv2d-254          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-255          [-1, 256, 14, 14]             512\n",
      "            ReLU-256          [-1, 256, 14, 14]               0\n",
      "          Conv2d-257         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-258         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-259         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-260         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-261          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-262          [-1, 256, 14, 14]             512\n",
      "            ReLU-263          [-1, 256, 14, 14]               0\n",
      "          Conv2d-264          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-265          [-1, 256, 14, 14]             512\n",
      "            ReLU-266          [-1, 256, 14, 14]               0\n",
      "          Conv2d-267         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-268         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-269         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-270         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-271          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-272          [-1, 256, 14, 14]             512\n",
      "            ReLU-273          [-1, 256, 14, 14]               0\n",
      "          Conv2d-274          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-275          [-1, 256, 14, 14]             512\n",
      "            ReLU-276          [-1, 256, 14, 14]               0\n",
      "          Conv2d-277         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-278         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-279         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-280         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-281          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-282          [-1, 256, 14, 14]             512\n",
      "            ReLU-283          [-1, 256, 14, 14]               0\n",
      "          Conv2d-284          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-285          [-1, 256, 14, 14]             512\n",
      "            ReLU-286          [-1, 256, 14, 14]               0\n",
      "          Conv2d-287         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-288         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-289         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-290         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-291          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-292          [-1, 256, 14, 14]             512\n",
      "            ReLU-293          [-1, 256, 14, 14]               0\n",
      "          Conv2d-294          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-295          [-1, 256, 14, 14]             512\n",
      "            ReLU-296          [-1, 256, 14, 14]               0\n",
      "          Conv2d-297         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-298         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-299         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-300         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-301          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-302          [-1, 256, 14, 14]             512\n",
      "            ReLU-303          [-1, 256, 14, 14]               0\n",
      "          Conv2d-304          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-305          [-1, 256, 14, 14]             512\n",
      "            ReLU-306          [-1, 256, 14, 14]               0\n",
      "          Conv2d-307         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-308         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-309         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-310         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-311          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-312          [-1, 256, 14, 14]             512\n",
      "            ReLU-313          [-1, 256, 14, 14]               0\n",
      "          Conv2d-314          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-315          [-1, 256, 14, 14]             512\n",
      "            ReLU-316          [-1, 256, 14, 14]               0\n",
      "          Conv2d-317         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-318         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-319         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-320         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-321          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-322          [-1, 256, 14, 14]             512\n",
      "            ReLU-323          [-1, 256, 14, 14]               0\n",
      "          Conv2d-324          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-325          [-1, 256, 14, 14]             512\n",
      "            ReLU-326          [-1, 256, 14, 14]               0\n",
      "          Conv2d-327         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-328         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-329         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-330         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-331          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-332          [-1, 256, 14, 14]             512\n",
      "            ReLU-333          [-1, 256, 14, 14]               0\n",
      "          Conv2d-334          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-335          [-1, 256, 14, 14]             512\n",
      "            ReLU-336          [-1, 256, 14, 14]               0\n",
      "          Conv2d-337         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-338         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-339         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-340         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-341          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-342          [-1, 256, 14, 14]             512\n",
      "            ReLU-343          [-1, 256, 14, 14]               0\n",
      "          Conv2d-344          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-345          [-1, 256, 14, 14]             512\n",
      "            ReLU-346          [-1, 256, 14, 14]               0\n",
      "          Conv2d-347         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-348         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-349         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-350         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-351          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-352          [-1, 256, 14, 14]             512\n",
      "            ReLU-353          [-1, 256, 14, 14]               0\n",
      "          Conv2d-354          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-355          [-1, 256, 14, 14]             512\n",
      "            ReLU-356          [-1, 256, 14, 14]               0\n",
      "          Conv2d-357         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-358         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-359         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-360         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-361          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-362          [-1, 256, 14, 14]             512\n",
      "            ReLU-363          [-1, 256, 14, 14]               0\n",
      "          Conv2d-364          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-365          [-1, 256, 14, 14]             512\n",
      "            ReLU-366          [-1, 256, 14, 14]               0\n",
      "          Conv2d-367         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-368         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-369         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-370         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-371          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-372          [-1, 256, 14, 14]             512\n",
      "            ReLU-373          [-1, 256, 14, 14]               0\n",
      "          Conv2d-374          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-375          [-1, 256, 14, 14]             512\n",
      "            ReLU-376          [-1, 256, 14, 14]               0\n",
      "          Conv2d-377         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-378         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-379         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-380         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-381          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-382          [-1, 256, 14, 14]             512\n",
      "            ReLU-383          [-1, 256, 14, 14]               0\n",
      "          Conv2d-384          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-385          [-1, 256, 14, 14]             512\n",
      "            ReLU-386          [-1, 256, 14, 14]               0\n",
      "          Conv2d-387         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-388         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-389         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-390         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-391          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-392          [-1, 256, 14, 14]             512\n",
      "            ReLU-393          [-1, 256, 14, 14]               0\n",
      "          Conv2d-394          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-395          [-1, 256, 14, 14]             512\n",
      "            ReLU-396          [-1, 256, 14, 14]               0\n",
      "          Conv2d-397         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-398         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-399         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-400         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-401          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-402          [-1, 256, 14, 14]             512\n",
      "            ReLU-403          [-1, 256, 14, 14]               0\n",
      "          Conv2d-404          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-405          [-1, 256, 14, 14]             512\n",
      "            ReLU-406          [-1, 256, 14, 14]               0\n",
      "          Conv2d-407         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-408         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-409         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-410         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-411          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-412          [-1, 256, 14, 14]             512\n",
      "            ReLU-413          [-1, 256, 14, 14]               0\n",
      "          Conv2d-414          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-415          [-1, 256, 14, 14]             512\n",
      "            ReLU-416          [-1, 256, 14, 14]               0\n",
      "          Conv2d-417         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-418         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-419         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-420         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-421          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-422          [-1, 256, 14, 14]             512\n",
      "            ReLU-423          [-1, 256, 14, 14]               0\n",
      "          Conv2d-424          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-425          [-1, 256, 14, 14]             512\n",
      "            ReLU-426          [-1, 256, 14, 14]               0\n",
      "          Conv2d-427         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-428         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-429         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-430         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-431          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-432          [-1, 256, 14, 14]             512\n",
      "            ReLU-433          [-1, 256, 14, 14]               0\n",
      "          Conv2d-434          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-435          [-1, 256, 14, 14]             512\n",
      "            ReLU-436          [-1, 256, 14, 14]               0\n",
      "          Conv2d-437         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-438         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-439         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-440         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-441          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-442          [-1, 256, 14, 14]             512\n",
      "            ReLU-443          [-1, 256, 14, 14]               0\n",
      "          Conv2d-444          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-445          [-1, 256, 14, 14]             512\n",
      "            ReLU-446          [-1, 256, 14, 14]               0\n",
      "          Conv2d-447         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-448         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-449         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-450         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-451          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-452          [-1, 256, 14, 14]             512\n",
      "            ReLU-453          [-1, 256, 14, 14]               0\n",
      "          Conv2d-454          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-455          [-1, 256, 14, 14]             512\n",
      "            ReLU-456          [-1, 256, 14, 14]               0\n",
      "          Conv2d-457         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-458         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-459         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-460         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-461          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-462          [-1, 256, 14, 14]             512\n",
      "            ReLU-463          [-1, 256, 14, 14]               0\n",
      "          Conv2d-464          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-465          [-1, 256, 14, 14]             512\n",
      "            ReLU-466          [-1, 256, 14, 14]               0\n",
      "          Conv2d-467         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-468         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-469         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-470         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-471          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-472          [-1, 256, 14, 14]             512\n",
      "            ReLU-473          [-1, 256, 14, 14]               0\n",
      "          Conv2d-474          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-475          [-1, 256, 14, 14]             512\n",
      "            ReLU-476          [-1, 256, 14, 14]               0\n",
      "          Conv2d-477         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-478         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-479         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-480         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-481          [-1, 512, 14, 14]         524,288\n",
      "     BatchNorm2d-482          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-483          [-1, 512, 14, 14]               0\n",
      "          Conv2d-484            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-485            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-486            [-1, 512, 7, 7]               0\n",
      "          Conv2d-487           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-488           [-1, 2048, 7, 7]           4,096\n",
      "          Conv2d-489           [-1, 2048, 7, 7]       2,097,152\n",
      "     BatchNorm2d-490           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-491           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-492           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-493            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-494            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-495            [-1, 512, 7, 7]               0\n",
      "          Conv2d-496            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-497            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-498            [-1, 512, 7, 7]               0\n",
      "          Conv2d-499           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-500           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-501           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-502           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-503            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-504            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-505            [-1, 512, 7, 7]               0\n",
      "          Conv2d-506            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-507            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-508            [-1, 512, 7, 7]               0\n",
      "          Conv2d-509           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-510           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-511           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-512           [-1, 2048, 7, 7]               0\n",
      "AdaptiveAvgPool2d-513           [-1, 2048, 1, 1]               0\n",
      "          Linear-514                  [-1, 256]         524,544\n",
      "            ReLU-515                  [-1, 256]               0\n",
      "         Dropout-516                  [-1, 256]               0\n",
      "          Linear-517                    [-1, 2]             514\n",
      "================================================================\n",
      "Total params: 58,668,866\n",
      "Trainable params: 525,058\n",
      "Non-trainable params: 58,143,808\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 606.59\n",
      "Params size (MB): 223.80\n",
      "Estimated Total Size (MB): 830.97\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "resnet =resnet.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(resnet.parameters(), lr=0.01)\n",
    "summary(resnet, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4beebac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.8  # The decay factor for each epoch\n",
    "scheduler = lr_scheduler.ExponentialLR(opt, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8dfa0813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b934b54061204eb59eaf636b623c3837",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?epochs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min loss 0.71\n",
      "Iteration: 0, Loss: 0.71\n",
      "Min loss 0.55\n",
      "Min loss 0.48\n",
      "Min loss 0.45\n",
      "Epoch: 0/50, Test acc: 56.14, Train acc: 54.04\n",
      "Iteration: 0, Loss: 0.73\n",
      "Epoch: 1/50, Test acc: 55.15, Train acc: 54.12\n",
      "Iteration: 0, Loss: 0.97\n",
      "Epoch: 2/50, Test acc: 54.61, Train acc: 53.83\n",
      "Iteration: 0, Loss: 0.62\n",
      "Min loss 0.35\n",
      "Epoch: 3/50, Test acc: 54.71, Train acc: 54.26\n",
      "Iteration: 0, Loss: 0.82\n",
      "Min loss 0.29\n",
      "Epoch: 4/50, Test acc: 55.04, Train acc: 54.14\n",
      "Iteration: 0, Loss: 1.36\n",
      "Min loss 0.24\n",
      "Epoch: 5/50, Test acc: 54.82, Train acc: 54.00\n",
      "Iteration: 0, Loss: 1.14\n",
      "Epoch: 6/50, Test acc: 54.39, Train acc: 54.22\n",
      "Iteration: 0, Loss: 0.61\n",
      "Epoch: 7/50, Test acc: 54.82, Train acc: 54.06\n",
      "Iteration: 0, Loss: 0.45\n",
      "Epoch: 8/50, Test acc: 55.04, Train acc: 54.06\n",
      "Iteration: 0, Loss: 1.00\n",
      "Epoch: 9/50, Test acc: 54.93, Train acc: 53.69\n",
      "Iteration: 0, Loss: 1.19\n",
      "Epoch: 10/50, Test acc: 55.48, Train acc: 54.24\n",
      "Iteration: 0, Loss: 1.08\n",
      "Epoch: 11/50, Test acc: 56.14, Train acc: 54.10\n",
      "Iteration: 0, Loss: 1.38\n",
      "Epoch: 12/50, Test acc: 55.48, Train acc: 53.87\n",
      "Iteration: 0, Loss: 0.88\n",
      "Epoch: 13/50, Test acc: 55.70, Train acc: 53.54\n",
      "Iteration: 0, Loss: 1.12\n",
      "Epoch: 14/50, Test acc: 55.04, Train acc: 53.89\n",
      "Iteration: 0, Loss: 1.43\n",
      "Epoch: 15/50, Test acc: 55.15, Train acc: 54.06\n",
      "Iteration: 0, Loss: 1.37\n",
      "Epoch: 16/50, Test acc: 54.93, Train acc: 53.93\n",
      "Iteration: 0, Loss: 1.31\n",
      "Epoch: 17/50, Test acc: 54.93, Train acc: 54.10\n",
      "Iteration: 0, Loss: 1.12\n",
      "Epoch: 18/50, Test acc: 54.28, Train acc: 54.24\n",
      "Iteration: 0, Loss: 1.15\n",
      "Epoch: 19/50, Test acc: 54.82, Train acc: 53.81\n",
      "Iteration: 0, Loss: 0.85\n",
      "Epoch: 20/50, Test acc: 54.61, Train acc: 53.63\n",
      "Iteration: 0, Loss: 1.20\n",
      "Epoch: 21/50, Test acc: 54.93, Train acc: 53.93\n",
      "Iteration: 0, Loss: 0.82\n",
      "Epoch: 22/50, Test acc: 54.93, Train acc: 53.91\n",
      "Iteration: 0, Loss: 0.58\n",
      "Epoch: 23/50, Test acc: 55.15, Train acc: 54.00\n",
      "Iteration: 0, Loss: 0.70\n",
      "Epoch: 24/50, Test acc: 55.15, Train acc: 53.91\n",
      "Iteration: 0, Loss: 1.05\n",
      "Min loss 0.15\n",
      "Epoch: 25/50, Test acc: 54.39, Train acc: 54.20\n",
      "Iteration: 0, Loss: 1.05\n",
      "Epoch: 26/50, Test acc: 55.48, Train acc: 53.63\n",
      "Iteration: 0, Loss: 0.97\n",
      "Epoch: 27/50, Test acc: 54.61, Train acc: 54.24\n",
      "Iteration: 0, Loss: 1.06\n",
      "Min loss 0.12\n",
      "Epoch: 28/50, Test acc: 55.81, Train acc: 54.14\n",
      "Iteration: 0, Loss: 0.95\n",
      "Epoch: 29/50, Test acc: 55.04, Train acc: 54.55\n",
      "Iteration: 0, Loss: 1.05\n",
      "Epoch: 30/50, Test acc: 55.26, Train acc: 53.75\n",
      "Iteration: 0, Loss: 1.42\n",
      "Epoch: 31/50, Test acc: 55.04, Train acc: 54.12\n",
      "Iteration: 0, Loss: 0.69\n",
      "Epoch: 32/50, Test acc: 54.93, Train acc: 53.73\n",
      "Iteration: 0, Loss: 1.20\n",
      "Epoch: 33/50, Test acc: 55.81, Train acc: 53.83\n",
      "Iteration: 0, Loss: 0.68\n",
      "Epoch: 34/50, Test acc: 54.61, Train acc: 54.45\n",
      "Iteration: 0, Loss: 0.64\n",
      "Epoch: 35/50, Test acc: 55.04, Train acc: 53.30\n",
      "Iteration: 0, Loss: 0.84\n",
      "Epoch: 36/50, Test acc: 54.50, Train acc: 53.65\n",
      "Iteration: 0, Loss: 1.30\n",
      "Epoch: 37/50, Test acc: 55.48, Train acc: 54.24\n",
      "Iteration: 0, Loss: 1.10\n",
      "Epoch: 38/50, Test acc: 55.37, Train acc: 54.16\n",
      "Iteration: 0, Loss: 0.93\n",
      "Epoch: 39/50, Test acc: 55.26, Train acc: 54.53\n",
      "Iteration: 0, Loss: 0.88\n",
      "Epoch: 40/50, Test acc: 55.37, Train acc: 54.53\n",
      "Iteration: 0, Loss: 0.84\n",
      "Epoch: 41/50, Test acc: 55.37, Train acc: 54.04\n",
      "Iteration: 0, Loss: 0.99\n",
      "Epoch: 42/50, Test acc: 55.70, Train acc: 53.67\n",
      "Iteration: 0, Loss: 1.32\n",
      "Epoch: 43/50, Test acc: 54.93, Train acc: 54.00\n",
      "Iteration: 0, Loss: 0.82\n",
      "Epoch: 44/50, Test acc: 56.03, Train acc: 54.16\n",
      "Iteration: 0, Loss: 0.78\n",
      "Epoch: 45/50, Test acc: 55.37, Train acc: 54.28\n",
      "Iteration: 0, Loss: 1.45\n",
      "Epoch: 46/50, Test acc: 55.04, Train acc: 54.22\n",
      "Iteration: 0, Loss: 0.82\n",
      "Epoch: 47/50, Test acc: 55.04, Train acc: 54.28\n",
      "Iteration: 0, Loss: 1.12\n",
      "Epoch: 48/50, Test acc: 54.39, Train acc: 53.69\n",
      "Iteration: 0, Loss: 1.49\n",
      "Epoch: 49/50, Test acc: 55.48, Train acc: 54.26\n"
     ]
    }
   ],
   "source": [
    "best_model, loss_epoch_arr, train_accuracy_lis, val_accuracy_lis, val_loss_epoch_arr = train_func(resnet, loss_fn, opt, trainloader, valloader, scheduler , epochs=50\n",
    "                                                                            )      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "938fc411",
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = DataLoader(test_ds, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "49c9307a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55.26315789473684"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation(testloader,resnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a939af52",
   "metadata": {},
   "source": [
    "# Resnet 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0486b7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = models.resnet18(weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "aa54c6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in resnet.parameters():\n",
    "    param.requires_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d71ef3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = resnet.fc.in_features\n",
    "new_classifier = nn.Sequential(\n",
    "    nn.Linear(num_features, 256), nn.ReLU(), nn.Dropout(0.2),\n",
    "    nn.Linear(256, 2))\n",
    "resnet.fc = new_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "79d0d446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-11           [-1, 64, 56, 56]               0\n",
      "           Conv2d-12           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-13           [-1, 64, 56, 56]             128\n",
      "             ReLU-14           [-1, 64, 56, 56]               0\n",
      "           Conv2d-15           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-16           [-1, 64, 56, 56]             128\n",
      "             ReLU-17           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-18           [-1, 64, 56, 56]               0\n",
      "           Conv2d-19          [-1, 128, 28, 28]          73,728\n",
      "      BatchNorm2d-20          [-1, 128, 28, 28]             256\n",
      "             ReLU-21          [-1, 128, 28, 28]               0\n",
      "           Conv2d-22          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-23          [-1, 128, 28, 28]             256\n",
      "           Conv2d-24          [-1, 128, 28, 28]           8,192\n",
      "      BatchNorm2d-25          [-1, 128, 28, 28]             256\n",
      "             ReLU-26          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-27          [-1, 128, 28, 28]               0\n",
      "           Conv2d-28          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-29          [-1, 128, 28, 28]             256\n",
      "             ReLU-30          [-1, 128, 28, 28]               0\n",
      "           Conv2d-31          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-32          [-1, 128, 28, 28]             256\n",
      "             ReLU-33          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-34          [-1, 128, 28, 28]               0\n",
      "           Conv2d-35          [-1, 256, 14, 14]         294,912\n",
      "      BatchNorm2d-36          [-1, 256, 14, 14]             512\n",
      "             ReLU-37          [-1, 256, 14, 14]               0\n",
      "           Conv2d-38          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-39          [-1, 256, 14, 14]             512\n",
      "           Conv2d-40          [-1, 256, 14, 14]          32,768\n",
      "      BatchNorm2d-41          [-1, 256, 14, 14]             512\n",
      "             ReLU-42          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-43          [-1, 256, 14, 14]               0\n",
      "           Conv2d-44          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-45          [-1, 256, 14, 14]             512\n",
      "             ReLU-46          [-1, 256, 14, 14]               0\n",
      "           Conv2d-47          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-48          [-1, 256, 14, 14]             512\n",
      "             ReLU-49          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-50          [-1, 256, 14, 14]               0\n",
      "           Conv2d-51            [-1, 512, 7, 7]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-53            [-1, 512, 7, 7]               0\n",
      "           Conv2d-54            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 7, 7]           1,024\n",
      "           Conv2d-56            [-1, 512, 7, 7]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-58            [-1, 512, 7, 7]               0\n",
      "       BasicBlock-59            [-1, 512, 7, 7]               0\n",
      "           Conv2d-60            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-62            [-1, 512, 7, 7]               0\n",
      "           Conv2d-63            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-65            [-1, 512, 7, 7]               0\n",
      "       BasicBlock-66            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                  [-1, 256]         131,328\n",
      "             ReLU-69                  [-1, 256]               0\n",
      "          Dropout-70                  [-1, 256]               0\n",
      "           Linear-71                    [-1, 2]             514\n",
      "================================================================\n",
      "Total params: 11,308,354\n",
      "Trainable params: 131,842\n",
      "Non-trainable params: 11,176,512\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 62.79\n",
      "Params size (MB): 43.14\n",
      "Estimated Total Size (MB): 106.50\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "resnet =resnet.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(resnet.parameters(), lr=0.01)\n",
    "summary(resnet, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "355779ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.8  # The decay factor for each epoch\n",
    "scheduler = lr_scheduler.ExponentialLR(opt, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9631aa08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b9c1517de7e456c8eb21ee9c52a8468",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?epochs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min loss 0.67\n",
      "Iteration: 0, Loss: 0.67\n",
      "Min loss 0.58\n",
      "Min loss 0.56\n",
      "Min loss 0.54\n",
      "Min loss 0.50\n",
      "Min loss 0.48\n",
      "Min loss 0.41\n",
      "Epoch: 0/50, Test acc: 56.14, Train acc: 56.32\n",
      "Iteration: 0, Loss: 0.87\n",
      "Epoch: 1/50, Test acc: 56.69, Train acc: 56.28\n",
      "Iteration: 0, Loss: 0.86\n",
      "Min loss 0.39\n",
      "Min loss 0.38\n",
      "Epoch: 2/50, Test acc: 56.80, Train acc: 54.76\n",
      "Iteration: 0, Loss: 0.85\n",
      "Epoch: 3/50, Test acc: 56.91, Train acc: 55.19\n",
      "Iteration: 0, Loss: 0.99\n",
      "Epoch: 4/50, Test acc: 56.69, Train acc: 54.94\n",
      "Iteration: 0, Loss: 0.99\n",
      "Epoch: 5/50, Test acc: 56.36, Train acc: 55.48\n",
      "Iteration: 0, Loss: 0.68\n",
      "Epoch: 6/50, Test acc: 56.14, Train acc: 56.01\n",
      "Iteration: 0, Loss: 1.06\n",
      "Epoch: 7/50, Test acc: 57.02, Train acc: 55.97\n",
      "Iteration: 0, Loss: 0.71\n",
      "Epoch: 8/50, Test acc: 56.58, Train acc: 55.52\n",
      "Iteration: 0, Loss: 0.78\n",
      "Epoch: 9/50, Test acc: 56.47, Train acc: 55.29\n",
      "Iteration: 0, Loss: 0.67\n",
      "Epoch: 10/50, Test acc: 56.69, Train acc: 55.87\n",
      "Iteration: 0, Loss: 0.81\n",
      "Epoch: 11/50, Test acc: 56.80, Train acc: 55.11\n",
      "Iteration: 0, Loss: 0.87\n",
      "Epoch: 12/50, Test acc: 56.91, Train acc: 55.19\n",
      "Iteration: 0, Loss: 0.69\n",
      "Epoch: 13/50, Test acc: 56.14, Train acc: 55.93\n",
      "Iteration: 0, Loss: 0.85\n",
      "Epoch: 14/50, Test acc: 56.25, Train acc: 56.05\n",
      "Iteration: 0, Loss: 0.68\n",
      "Epoch: 15/50, Test acc: 56.25, Train acc: 54.84\n",
      "Iteration: 0, Loss: 0.86\n",
      "Epoch: 16/50, Test acc: 56.80, Train acc: 55.60\n",
      "Iteration: 0, Loss: 0.82\n",
      "Epoch: 17/50, Test acc: 57.02, Train acc: 55.52\n",
      "Iteration: 0, Loss: 0.67\n",
      "Epoch: 18/50, Test acc: 56.69, Train acc: 54.98\n",
      "Iteration: 0, Loss: 0.65\n",
      "Epoch: 19/50, Test acc: 56.91, Train acc: 55.04\n",
      "Iteration: 0, Loss: 0.61\n",
      "Epoch: 20/50, Test acc: 57.13, Train acc: 55.62\n",
      "Iteration: 0, Loss: 0.68\n",
      "Epoch: 21/50, Test acc: 56.91, Train acc: 55.74\n",
      "Iteration: 0, Loss: 0.76\n",
      "Epoch: 22/50, Test acc: 56.58, Train acc: 55.25\n",
      "Iteration: 0, Loss: 0.67\n",
      "Epoch: 23/50, Test acc: 56.91, Train acc: 55.33\n",
      "Iteration: 0, Loss: 0.48\n",
      "Epoch: 24/50, Test acc: 57.13, Train acc: 54.98\n",
      "Iteration: 0, Loss: 0.63\n",
      "Epoch: 25/50, Test acc: 56.58, Train acc: 55.25\n",
      "Iteration: 0, Loss: 0.93\n",
      "Epoch: 26/50, Test acc: 56.47, Train acc: 55.54\n",
      "Iteration: 0, Loss: 0.96\n",
      "Epoch: 27/50, Test acc: 56.58, Train acc: 55.97\n",
      "Iteration: 0, Loss: 0.89\n",
      "Epoch: 28/50, Test acc: 56.80, Train acc: 55.70\n",
      "Iteration: 0, Loss: 0.52\n",
      "Epoch: 29/50, Test acc: 56.80, Train acc: 55.11\n",
      "Iteration: 0, Loss: 0.60\n",
      "Epoch: 30/50, Test acc: 57.24, Train acc: 55.50\n",
      "Iteration: 0, Loss: 0.58\n",
      "Epoch: 31/50, Test acc: 56.03, Train acc: 55.27\n",
      "Iteration: 0, Loss: 0.55\n",
      "Epoch: 32/50, Test acc: 56.36, Train acc: 55.50\n",
      "Iteration: 0, Loss: 0.68\n",
      "Epoch: 33/50, Test acc: 56.58, Train acc: 55.04\n",
      "Iteration: 0, Loss: 0.63\n",
      "Min loss 0.35\n",
      "Epoch: 34/50, Test acc: 57.35, Train acc: 55.48\n",
      "Iteration: 0, Loss: 0.83\n",
      "Epoch: 35/50, Test acc: 56.03, Train acc: 55.80\n",
      "Iteration: 0, Loss: 0.67\n",
      "Epoch: 36/50, Test acc: 56.47, Train acc: 54.61\n",
      "Iteration: 0, Loss: 0.95\n",
      "Min loss 0.21\n",
      "Epoch: 37/50, Test acc: 56.25, Train acc: 55.33\n",
      "Iteration: 0, Loss: 1.13\n",
      "Epoch: 38/50, Test acc: 56.69, Train acc: 55.68\n",
      "Iteration: 0, Loss: 0.96\n",
      "Epoch: 39/50, Test acc: 56.25, Train acc: 55.93\n",
      "Iteration: 0, Loss: 0.78\n",
      "Epoch: 40/50, Test acc: 57.24, Train acc: 54.78\n",
      "Iteration: 0, Loss: 0.91\n",
      "Epoch: 41/50, Test acc: 56.03, Train acc: 55.35\n",
      "Iteration: 0, Loss: 0.59\n",
      "Epoch: 42/50, Test acc: 56.25, Train acc: 55.62\n",
      "Iteration: 0, Loss: 0.63\n",
      "Epoch: 43/50, Test acc: 56.80, Train acc: 55.78\n",
      "Iteration: 0, Loss: 0.90\n",
      "Epoch: 44/50, Test acc: 56.91, Train acc: 54.80\n",
      "Iteration: 0, Loss: 0.62\n",
      "Epoch: 45/50, Test acc: 56.47, Train acc: 55.02\n",
      "Iteration: 0, Loss: 0.78\n",
      "Epoch: 46/50, Test acc: 56.69, Train acc: 56.11\n",
      "Iteration: 0, Loss: 0.74\n",
      "Epoch: 47/50, Test acc: 56.69, Train acc: 55.52\n",
      "Iteration: 0, Loss: 0.72\n",
      "Epoch: 48/50, Test acc: 56.69, Train acc: 54.96\n",
      "Iteration: 0, Loss: 0.68\n",
      "Epoch: 49/50, Test acc: 56.80, Train acc: 55.11\n"
     ]
    }
   ],
   "source": [
    "best_model, loss_epoch_arr, train_accuracy_lis, val_accuracy_lis, val_loss_epoch_arr = train_func(resnet, loss_fn, opt, trainloader, valloader, scheduler , epochs=50\n",
    "                                                                            )      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "86877df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = DataLoader(test_ds, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a9bb7f0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57.56578947368421"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation(testloader,resnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62580ab8",
   "metadata": {},
   "source": [
    "# resnet-34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "984ed57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = models.resnet34(weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "95a7a80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in resnet.parameters():\n",
    "    param.requires_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "98ddf257",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = resnet.fc.in_features\n",
    "new_classifier = nn.Sequential(\n",
    "    nn.Linear(num_features, 256), nn.ReLU(), nn.Dropout(0.2),\n",
    "    nn.Linear(256, 2))\n",
    "resnet.fc = new_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c6a7f596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-11           [-1, 64, 56, 56]               0\n",
      "           Conv2d-12           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-13           [-1, 64, 56, 56]             128\n",
      "             ReLU-14           [-1, 64, 56, 56]               0\n",
      "           Conv2d-15           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-16           [-1, 64, 56, 56]             128\n",
      "             ReLU-17           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-18           [-1, 64, 56, 56]               0\n",
      "           Conv2d-19           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-20           [-1, 64, 56, 56]             128\n",
      "             ReLU-21           [-1, 64, 56, 56]               0\n",
      "           Conv2d-22           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-23           [-1, 64, 56, 56]             128\n",
      "             ReLU-24           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-25           [-1, 64, 56, 56]               0\n",
      "           Conv2d-26          [-1, 128, 28, 28]          73,728\n",
      "      BatchNorm2d-27          [-1, 128, 28, 28]             256\n",
      "             ReLU-28          [-1, 128, 28, 28]               0\n",
      "           Conv2d-29          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-30          [-1, 128, 28, 28]             256\n",
      "           Conv2d-31          [-1, 128, 28, 28]           8,192\n",
      "      BatchNorm2d-32          [-1, 128, 28, 28]             256\n",
      "             ReLU-33          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-34          [-1, 128, 28, 28]               0\n",
      "           Conv2d-35          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-36          [-1, 128, 28, 28]             256\n",
      "             ReLU-37          [-1, 128, 28, 28]               0\n",
      "           Conv2d-38          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-39          [-1, 128, 28, 28]             256\n",
      "             ReLU-40          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-41          [-1, 128, 28, 28]               0\n",
      "           Conv2d-42          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-43          [-1, 128, 28, 28]             256\n",
      "             ReLU-44          [-1, 128, 28, 28]               0\n",
      "           Conv2d-45          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-46          [-1, 128, 28, 28]             256\n",
      "             ReLU-47          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-48          [-1, 128, 28, 28]               0\n",
      "           Conv2d-49          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
      "             ReLU-51          [-1, 128, 28, 28]               0\n",
      "           Conv2d-52          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-53          [-1, 128, 28, 28]             256\n",
      "             ReLU-54          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-55          [-1, 128, 28, 28]               0\n",
      "           Conv2d-56          [-1, 256, 14, 14]         294,912\n",
      "      BatchNorm2d-57          [-1, 256, 14, 14]             512\n",
      "             ReLU-58          [-1, 256, 14, 14]               0\n",
      "           Conv2d-59          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-60          [-1, 256, 14, 14]             512\n",
      "           Conv2d-61          [-1, 256, 14, 14]          32,768\n",
      "      BatchNorm2d-62          [-1, 256, 14, 14]             512\n",
      "             ReLU-63          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-64          [-1, 256, 14, 14]               0\n",
      "           Conv2d-65          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-66          [-1, 256, 14, 14]             512\n",
      "             ReLU-67          [-1, 256, 14, 14]               0\n",
      "           Conv2d-68          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-69          [-1, 256, 14, 14]             512\n",
      "             ReLU-70          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-71          [-1, 256, 14, 14]               0\n",
      "           Conv2d-72          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-73          [-1, 256, 14, 14]             512\n",
      "             ReLU-74          [-1, 256, 14, 14]               0\n",
      "           Conv2d-75          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-76          [-1, 256, 14, 14]             512\n",
      "             ReLU-77          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-78          [-1, 256, 14, 14]               0\n",
      "           Conv2d-79          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-80          [-1, 256, 14, 14]             512\n",
      "             ReLU-81          [-1, 256, 14, 14]               0\n",
      "           Conv2d-82          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-83          [-1, 256, 14, 14]             512\n",
      "             ReLU-84          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-85          [-1, 256, 14, 14]               0\n",
      "           Conv2d-86          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-87          [-1, 256, 14, 14]             512\n",
      "             ReLU-88          [-1, 256, 14, 14]               0\n",
      "           Conv2d-89          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-90          [-1, 256, 14, 14]             512\n",
      "             ReLU-91          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-92          [-1, 256, 14, 14]               0\n",
      "           Conv2d-93          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-94          [-1, 256, 14, 14]             512\n",
      "             ReLU-95          [-1, 256, 14, 14]               0\n",
      "           Conv2d-96          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-97          [-1, 256, 14, 14]             512\n",
      "             ReLU-98          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-99          [-1, 256, 14, 14]               0\n",
      "          Conv2d-100            [-1, 512, 7, 7]       1,179,648\n",
      "     BatchNorm2d-101            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-102            [-1, 512, 7, 7]               0\n",
      "          Conv2d-103            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-104            [-1, 512, 7, 7]           1,024\n",
      "          Conv2d-105            [-1, 512, 7, 7]         131,072\n",
      "     BatchNorm2d-106            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-107            [-1, 512, 7, 7]               0\n",
      "      BasicBlock-108            [-1, 512, 7, 7]               0\n",
      "          Conv2d-109            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-110            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-111            [-1, 512, 7, 7]               0\n",
      "          Conv2d-112            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-113            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-114            [-1, 512, 7, 7]               0\n",
      "      BasicBlock-115            [-1, 512, 7, 7]               0\n",
      "          Conv2d-116            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-117            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-118            [-1, 512, 7, 7]               0\n",
      "          Conv2d-119            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-120            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-121            [-1, 512, 7, 7]               0\n",
      "      BasicBlock-122            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-123            [-1, 512, 1, 1]               0\n",
      "          Linear-124                  [-1, 256]         131,328\n",
      "            ReLU-125                  [-1, 256]               0\n",
      "         Dropout-126                  [-1, 256]               0\n",
      "          Linear-127                    [-1, 2]             514\n",
      "================================================================\n",
      "Total params: 21,416,514\n",
      "Trainable params: 131,842\n",
      "Non-trainable params: 21,284,672\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 96.29\n",
      "Params size (MB): 81.70\n",
      "Estimated Total Size (MB): 178.56\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "resnet =resnet.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(resnet.parameters(), lr=0.01)\n",
    "summary(resnet, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c781ba7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.8  # The decay factor for each epoch\n",
    "scheduler = lr_scheduler.ExponentialLR(opt, gamma=gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "85905561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af5215031bcc45949c7933d0600194dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?epochs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min loss 0.65\n",
      "Iteration: 0, Loss: 0.65\n",
      "Min loss 0.62\n",
      "Min loss 0.58\n",
      "Min loss 0.53\n",
      "Min loss 0.45\n",
      "Min loss 0.42\n",
      "Epoch: 0/50, Test acc: 55.48, Train acc: 56.34\n",
      "Iteration: 0, Loss: 1.18\n",
      "Epoch: 1/50, Test acc: 54.61, Train acc: 56.15\n",
      "Iteration: 0, Loss: 0.72\n",
      "Epoch: 2/50, Test acc: 54.61, Train acc: 55.29\n",
      "Iteration: 0, Loss: 0.78\n",
      "Min loss 0.35\n",
      "Epoch: 3/50, Test acc: 54.61, Train acc: 55.74\n",
      "Iteration: 0, Loss: 0.89\n",
      "Epoch: 4/50, Test acc: 54.61, Train acc: 55.64\n",
      "Iteration: 0, Loss: 0.74\n",
      "Epoch: 5/50, Test acc: 55.26, Train acc: 55.68\n",
      "Iteration: 0, Loss: 0.75\n",
      "Epoch: 6/50, Test acc: 55.04, Train acc: 55.33\n",
      "Iteration: 0, Loss: 1.12\n",
      "Epoch: 7/50, Test acc: 54.17, Train acc: 56.05\n",
      "Iteration: 0, Loss: 0.81\n",
      "Epoch: 8/50, Test acc: 55.70, Train acc: 56.26\n",
      "Iteration: 0, Loss: 0.86\n",
      "Epoch: 9/50, Test acc: 54.71, Train acc: 55.50\n",
      "Iteration: 0, Loss: 0.84\n",
      "Epoch: 10/50, Test acc: 54.17, Train acc: 55.15\n",
      "Iteration: 0, Loss: 0.87\n",
      "Epoch: 11/50, Test acc: 55.15, Train acc: 55.93\n",
      "Iteration: 0, Loss: 0.64\n",
      "Epoch: 12/50, Test acc: 55.26, Train acc: 55.35\n",
      "Iteration: 0, Loss: 0.67\n",
      "Epoch: 13/50, Test acc: 54.82, Train acc: 55.54\n",
      "Iteration: 0, Loss: 0.82\n",
      "Epoch: 14/50, Test acc: 54.61, Train acc: 55.93\n",
      "Iteration: 0, Loss: 0.73\n",
      "Epoch: 15/50, Test acc: 54.71, Train acc: 55.43\n",
      "Iteration: 0, Loss: 0.88\n",
      "Epoch: 16/50, Test acc: 54.39, Train acc: 55.60\n",
      "Iteration: 0, Loss: 0.65\n",
      "Epoch: 17/50, Test acc: 53.95, Train acc: 56.19\n",
      "Iteration: 0, Loss: 0.96\n",
      "Min loss 0.33\n",
      "Epoch: 18/50, Test acc: 54.06, Train acc: 55.25\n",
      "Iteration: 0, Loss: 0.76\n",
      "Epoch: 19/50, Test acc: 55.26, Train acc: 54.69\n",
      "Iteration: 0, Loss: 0.84\n",
      "Epoch: 20/50, Test acc: 55.37, Train acc: 56.15\n",
      "Iteration: 0, Loss: 0.73\n",
      "Epoch: 21/50, Test acc: 55.26, Train acc: 56.01\n",
      "Iteration: 0, Loss: 0.80\n",
      "Epoch: 22/50, Test acc: 54.82, Train acc: 55.91\n",
      "Iteration: 0, Loss: 0.80\n",
      "Epoch: 23/50, Test acc: 55.04, Train acc: 55.46\n",
      "Iteration: 0, Loss: 0.54\n",
      "Epoch: 24/50, Test acc: 55.04, Train acc: 55.95\n",
      "Iteration: 0, Loss: 0.77\n",
      "Epoch: 25/50, Test acc: 55.92, Train acc: 56.61\n",
      "Iteration: 0, Loss: 0.74\n",
      "Epoch: 26/50, Test acc: 55.15, Train acc: 55.95\n",
      "Iteration: 0, Loss: 0.60\n",
      "Epoch: 27/50, Test acc: 55.26, Train acc: 56.52\n",
      "Iteration: 0, Loss: 0.87\n",
      "Epoch: 28/50, Test acc: 54.28, Train acc: 55.66\n",
      "Iteration: 0, Loss: 0.60\n",
      "Min loss 0.30\n",
      "Epoch: 29/50, Test acc: 55.04, Train acc: 55.99\n",
      "Iteration: 0, Loss: 0.82\n",
      "Epoch: 30/50, Test acc: 54.93, Train acc: 55.21\n",
      "Iteration: 0, Loss: 0.68\n",
      "Epoch: 31/50, Test acc: 53.95, Train acc: 55.17\n",
      "Iteration: 0, Loss: 0.81\n",
      "Epoch: 32/50, Test acc: 55.37, Train acc: 55.62\n",
      "Iteration: 0, Loss: 0.76\n",
      "Epoch: 33/50, Test acc: 54.82, Train acc: 55.99\n",
      "Iteration: 0, Loss: 0.68\n",
      "Min loss 0.25\n",
      "Epoch: 34/50, Test acc: 54.93, Train acc: 56.42\n",
      "Iteration: 0, Loss: 0.66\n",
      "Epoch: 35/50, Test acc: 54.50, Train acc: 55.39\n",
      "Iteration: 0, Loss: 1.08\n",
      "Epoch: 36/50, Test acc: 55.15, Train acc: 56.13\n",
      "Iteration: 0, Loss: 0.63\n",
      "Epoch: 37/50, Test acc: 55.04, Train acc: 55.97\n",
      "Iteration: 0, Loss: 0.71\n",
      "Epoch: 38/50, Test acc: 55.15, Train acc: 55.06\n",
      "Iteration: 0, Loss: 0.79\n",
      "Epoch: 39/50, Test acc: 53.40, Train acc: 55.00\n",
      "Iteration: 0, Loss: 0.68\n",
      "Epoch: 40/50, Test acc: 55.04, Train acc: 55.50\n",
      "Iteration: 0, Loss: 0.58\n",
      "Epoch: 41/50, Test acc: 54.61, Train acc: 55.35\n",
      "Iteration: 0, Loss: 0.67\n",
      "Epoch: 42/50, Test acc: 54.82, Train acc: 55.87\n",
      "Iteration: 0, Loss: 0.76\n",
      "Epoch: 43/50, Test acc: 53.84, Train acc: 55.66\n",
      "Iteration: 0, Loss: 0.80\n",
      "Epoch: 44/50, Test acc: 54.39, Train acc: 55.11\n",
      "Iteration: 0, Loss: 0.70\n",
      "Epoch: 45/50, Test acc: 54.71, Train acc: 55.35\n",
      "Iteration: 0, Loss: 0.56\n",
      "Epoch: 46/50, Test acc: 54.82, Train acc: 56.19\n",
      "Iteration: 0, Loss: 0.81\n",
      "Epoch: 47/50, Test acc: 54.50, Train acc: 55.62\n",
      "Iteration: 0, Loss: 0.57\n",
      "Epoch: 48/50, Test acc: 55.37, Train acc: 55.97\n",
      "Iteration: 0, Loss: 0.64\n",
      "Epoch: 49/50, Test acc: 54.93, Train acc: 55.56\n"
     ]
    }
   ],
   "source": [
    "best_model, loss_epoch_arr, train_accuracy_lis, val_accuracy_lis, val_loss_epoch_arr = train_func(resnet, loss_fn, opt, trainloader, valloader, scheduler , epochs=50\n",
    "                                                                            )      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "13ff16b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = DataLoader(test_ds, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "2179b4b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55.26315789473684"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation(testloader,resnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b76b9c",
   "metadata": {},
   "source": [
    "# resnet 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d68c291c",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = models.resnet101(weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "fd1132ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in resnet.parameters():\n",
    "    param.requires_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b80aae1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = resnet.fc.in_features\n",
    "new_classifier = nn.Sequential(\n",
    "    nn.Linear(num_features, 256), nn.ReLU(), nn.Dropout(0.2),\n",
    "    nn.Linear(256, 2))\n",
    "resnet.fc = new_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "5b34f0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]           4,096\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-12          [-1, 256, 56, 56]             512\n",
      "           Conv2d-13          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-14          [-1, 256, 56, 56]             512\n",
      "             ReLU-15          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-16          [-1, 256, 56, 56]               0\n",
      "           Conv2d-17           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-18           [-1, 64, 56, 56]             128\n",
      "             ReLU-19           [-1, 64, 56, 56]               0\n",
      "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
      "             ReLU-22           [-1, 64, 56, 56]               0\n",
      "           Conv2d-23          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-24          [-1, 256, 56, 56]             512\n",
      "             ReLU-25          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-26          [-1, 256, 56, 56]               0\n",
      "           Conv2d-27           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-28           [-1, 64, 56, 56]             128\n",
      "             ReLU-29           [-1, 64, 56, 56]               0\n",
      "           Conv2d-30           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-31           [-1, 64, 56, 56]             128\n",
      "             ReLU-32           [-1, 64, 56, 56]               0\n",
      "           Conv2d-33          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-34          [-1, 256, 56, 56]             512\n",
      "             ReLU-35          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-36          [-1, 256, 56, 56]               0\n",
      "           Conv2d-37          [-1, 128, 56, 56]          32,768\n",
      "      BatchNorm2d-38          [-1, 128, 56, 56]             256\n",
      "             ReLU-39          [-1, 128, 56, 56]               0\n",
      "           Conv2d-40          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-41          [-1, 128, 28, 28]             256\n",
      "             ReLU-42          [-1, 128, 28, 28]               0\n",
      "           Conv2d-43          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-44          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-45          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-46          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-47          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-48          [-1, 512, 28, 28]               0\n",
      "           Conv2d-49          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
      "             ReLU-51          [-1, 128, 28, 28]               0\n",
      "           Conv2d-52          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-53          [-1, 128, 28, 28]             256\n",
      "             ReLU-54          [-1, 128, 28, 28]               0\n",
      "           Conv2d-55          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-56          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-57          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-58          [-1, 512, 28, 28]               0\n",
      "           Conv2d-59          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-60          [-1, 128, 28, 28]             256\n",
      "             ReLU-61          [-1, 128, 28, 28]               0\n",
      "           Conv2d-62          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
      "             ReLU-64          [-1, 128, 28, 28]               0\n",
      "           Conv2d-65          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-66          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-67          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-68          [-1, 512, 28, 28]               0\n",
      "           Conv2d-69          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-70          [-1, 128, 28, 28]             256\n",
      "             ReLU-71          [-1, 128, 28, 28]               0\n",
      "           Conv2d-72          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-73          [-1, 128, 28, 28]             256\n",
      "             ReLU-74          [-1, 128, 28, 28]               0\n",
      "           Conv2d-75          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-76          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-77          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-78          [-1, 512, 28, 28]               0\n",
      "           Conv2d-79          [-1, 256, 28, 28]         131,072\n",
      "      BatchNorm2d-80          [-1, 256, 28, 28]             512\n",
      "             ReLU-81          [-1, 256, 28, 28]               0\n",
      "           Conv2d-82          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-83          [-1, 256, 14, 14]             512\n",
      "             ReLU-84          [-1, 256, 14, 14]               0\n",
      "           Conv2d-85         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-86         [-1, 1024, 14, 14]           2,048\n",
      "           Conv2d-87         [-1, 1024, 14, 14]         524,288\n",
      "      BatchNorm2d-88         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-89         [-1, 1024, 14, 14]               0\n",
      "       Bottleneck-90         [-1, 1024, 14, 14]               0\n",
      "           Conv2d-91          [-1, 256, 14, 14]         262,144\n",
      "      BatchNorm2d-92          [-1, 256, 14, 14]             512\n",
      "             ReLU-93          [-1, 256, 14, 14]               0\n",
      "           Conv2d-94          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-95          [-1, 256, 14, 14]             512\n",
      "             ReLU-96          [-1, 256, 14, 14]               0\n",
      "           Conv2d-97         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-98         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-99         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-100         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-101          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-102          [-1, 256, 14, 14]             512\n",
      "            ReLU-103          [-1, 256, 14, 14]               0\n",
      "          Conv2d-104          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-105          [-1, 256, 14, 14]             512\n",
      "            ReLU-106          [-1, 256, 14, 14]               0\n",
      "          Conv2d-107         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-108         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-109         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-110         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-111          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-112          [-1, 256, 14, 14]             512\n",
      "            ReLU-113          [-1, 256, 14, 14]               0\n",
      "          Conv2d-114          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-115          [-1, 256, 14, 14]             512\n",
      "            ReLU-116          [-1, 256, 14, 14]               0\n",
      "          Conv2d-117         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-118         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-119         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-120         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-121          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-122          [-1, 256, 14, 14]             512\n",
      "            ReLU-123          [-1, 256, 14, 14]               0\n",
      "          Conv2d-124          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-125          [-1, 256, 14, 14]             512\n",
      "            ReLU-126          [-1, 256, 14, 14]               0\n",
      "          Conv2d-127         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-128         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-129         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-130         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-131          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-132          [-1, 256, 14, 14]             512\n",
      "            ReLU-133          [-1, 256, 14, 14]               0\n",
      "          Conv2d-134          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-135          [-1, 256, 14, 14]             512\n",
      "            ReLU-136          [-1, 256, 14, 14]               0\n",
      "          Conv2d-137         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-138         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-139         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-140         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-141          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-142          [-1, 256, 14, 14]             512\n",
      "            ReLU-143          [-1, 256, 14, 14]               0\n",
      "          Conv2d-144          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-145          [-1, 256, 14, 14]             512\n",
      "            ReLU-146          [-1, 256, 14, 14]               0\n",
      "          Conv2d-147         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-148         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-149         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-150         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-151          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-152          [-1, 256, 14, 14]             512\n",
      "            ReLU-153          [-1, 256, 14, 14]               0\n",
      "          Conv2d-154          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-155          [-1, 256, 14, 14]             512\n",
      "            ReLU-156          [-1, 256, 14, 14]               0\n",
      "          Conv2d-157         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-158         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-159         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-160         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-161          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-162          [-1, 256, 14, 14]             512\n",
      "            ReLU-163          [-1, 256, 14, 14]               0\n",
      "          Conv2d-164          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-165          [-1, 256, 14, 14]             512\n",
      "            ReLU-166          [-1, 256, 14, 14]               0\n",
      "          Conv2d-167         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-168         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-169         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-170         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-171          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-172          [-1, 256, 14, 14]             512\n",
      "            ReLU-173          [-1, 256, 14, 14]               0\n",
      "          Conv2d-174          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-175          [-1, 256, 14, 14]             512\n",
      "            ReLU-176          [-1, 256, 14, 14]               0\n",
      "          Conv2d-177         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-178         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-179         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-180         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-181          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-182          [-1, 256, 14, 14]             512\n",
      "            ReLU-183          [-1, 256, 14, 14]               0\n",
      "          Conv2d-184          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-185          [-1, 256, 14, 14]             512\n",
      "            ReLU-186          [-1, 256, 14, 14]               0\n",
      "          Conv2d-187         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-188         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-189         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-190         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-191          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-192          [-1, 256, 14, 14]             512\n",
      "            ReLU-193          [-1, 256, 14, 14]               0\n",
      "          Conv2d-194          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-195          [-1, 256, 14, 14]             512\n",
      "            ReLU-196          [-1, 256, 14, 14]               0\n",
      "          Conv2d-197         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-198         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-199         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-200         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-201          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-202          [-1, 256, 14, 14]             512\n",
      "            ReLU-203          [-1, 256, 14, 14]               0\n",
      "          Conv2d-204          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-205          [-1, 256, 14, 14]             512\n",
      "            ReLU-206          [-1, 256, 14, 14]               0\n",
      "          Conv2d-207         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-208         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-209         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-210         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-211          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-212          [-1, 256, 14, 14]             512\n",
      "            ReLU-213          [-1, 256, 14, 14]               0\n",
      "          Conv2d-214          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-215          [-1, 256, 14, 14]             512\n",
      "            ReLU-216          [-1, 256, 14, 14]               0\n",
      "          Conv2d-217         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-218         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-219         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-220         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-221          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-222          [-1, 256, 14, 14]             512\n",
      "            ReLU-223          [-1, 256, 14, 14]               0\n",
      "          Conv2d-224          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-225          [-1, 256, 14, 14]             512\n",
      "            ReLU-226          [-1, 256, 14, 14]               0\n",
      "          Conv2d-227         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-228         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-229         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-230         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-231          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-232          [-1, 256, 14, 14]             512\n",
      "            ReLU-233          [-1, 256, 14, 14]               0\n",
      "          Conv2d-234          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-235          [-1, 256, 14, 14]             512\n",
      "            ReLU-236          [-1, 256, 14, 14]               0\n",
      "          Conv2d-237         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-238         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-239         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-240         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-241          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-242          [-1, 256, 14, 14]             512\n",
      "            ReLU-243          [-1, 256, 14, 14]               0\n",
      "          Conv2d-244          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-245          [-1, 256, 14, 14]             512\n",
      "            ReLU-246          [-1, 256, 14, 14]               0\n",
      "          Conv2d-247         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-248         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-249         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-250         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-251          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-252          [-1, 256, 14, 14]             512\n",
      "            ReLU-253          [-1, 256, 14, 14]               0\n",
      "          Conv2d-254          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-255          [-1, 256, 14, 14]             512\n",
      "            ReLU-256          [-1, 256, 14, 14]               0\n",
      "          Conv2d-257         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-258         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-259         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-260         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-261          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-262          [-1, 256, 14, 14]             512\n",
      "            ReLU-263          [-1, 256, 14, 14]               0\n",
      "          Conv2d-264          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-265          [-1, 256, 14, 14]             512\n",
      "            ReLU-266          [-1, 256, 14, 14]               0\n",
      "          Conv2d-267         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-268         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-269         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-270         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-271          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-272          [-1, 256, 14, 14]             512\n",
      "            ReLU-273          [-1, 256, 14, 14]               0\n",
      "          Conv2d-274          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-275          [-1, 256, 14, 14]             512\n",
      "            ReLU-276          [-1, 256, 14, 14]               0\n",
      "          Conv2d-277         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-278         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-279         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-280         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-281          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-282          [-1, 256, 14, 14]             512\n",
      "            ReLU-283          [-1, 256, 14, 14]               0\n",
      "          Conv2d-284          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-285          [-1, 256, 14, 14]             512\n",
      "            ReLU-286          [-1, 256, 14, 14]               0\n",
      "          Conv2d-287         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-288         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-289         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-290         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-291          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-292          [-1, 256, 14, 14]             512\n",
      "            ReLU-293          [-1, 256, 14, 14]               0\n",
      "          Conv2d-294          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-295          [-1, 256, 14, 14]             512\n",
      "            ReLU-296          [-1, 256, 14, 14]               0\n",
      "          Conv2d-297         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-298         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-299         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-300         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-301          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-302          [-1, 256, 14, 14]             512\n",
      "            ReLU-303          [-1, 256, 14, 14]               0\n",
      "          Conv2d-304          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-305          [-1, 256, 14, 14]             512\n",
      "            ReLU-306          [-1, 256, 14, 14]               0\n",
      "          Conv2d-307         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-308         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-309         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-310         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-311          [-1, 512, 14, 14]         524,288\n",
      "     BatchNorm2d-312          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-313          [-1, 512, 14, 14]               0\n",
      "          Conv2d-314            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-315            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-316            [-1, 512, 7, 7]               0\n",
      "          Conv2d-317           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-318           [-1, 2048, 7, 7]           4,096\n",
      "          Conv2d-319           [-1, 2048, 7, 7]       2,097,152\n",
      "     BatchNorm2d-320           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-321           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-322           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-323            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-324            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-325            [-1, 512, 7, 7]               0\n",
      "          Conv2d-326            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-327            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-328            [-1, 512, 7, 7]               0\n",
      "          Conv2d-329           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-330           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-331           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-332           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-333            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-334            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-335            [-1, 512, 7, 7]               0\n",
      "          Conv2d-336            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-337            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-338            [-1, 512, 7, 7]               0\n",
      "          Conv2d-339           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-340           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-341           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-342           [-1, 2048, 7, 7]               0\n",
      "AdaptiveAvgPool2d-343           [-1, 2048, 1, 1]               0\n",
      "          Linear-344                  [-1, 256]         524,544\n",
      "            ReLU-345                  [-1, 256]               0\n",
      "         Dropout-346                  [-1, 256]               0\n",
      "          Linear-347                    [-1, 2]             514\n",
      "================================================================\n",
      "Total params: 43,025,218\n",
      "Trainable params: 525,058\n",
      "Non-trainable params: 42,500,160\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 429.73\n",
      "Params size (MB): 164.13\n",
      "Estimated Total Size (MB): 594.43\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "resnet =resnet.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(resnet.parameters(), lr=0.01)\n",
    "summary(resnet, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "fb8dec3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.8  # The decay factor for each epoch\n",
    "scheduler = lr_scheduler.ExponentialLR(opt, gamma=gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6f8ed394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db3d357467ad48229dabce7ac2c60ed9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?epochs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min loss 0.72\n",
      "Iteration: 0, Loss: 0.72\n",
      "Min loss 0.52\n",
      "Min loss 0.45\n",
      "Min loss 0.43\n",
      "Min loss 0.40\n",
      "Epoch: 0/50, Test acc: 53.73, Train acc: 54.63\n",
      "Iteration: 0, Loss: 1.70\n",
      "Epoch: 1/50, Test acc: 55.04, Train acc: 55.95\n",
      "Iteration: 0, Loss: 0.47\n",
      "Min loss 0.35\n",
      "Epoch: 2/50, Test acc: 53.84, Train acc: 55.48\n",
      "Iteration: 0, Loss: 1.34\n",
      "Epoch: 3/50, Test acc: 54.61, Train acc: 55.11\n",
      "Iteration: 0, Loss: 2.11\n",
      "Min loss 0.19\n",
      "Epoch: 4/50, Test acc: 53.62, Train acc: 53.89\n",
      "Iteration: 0, Loss: 1.84\n",
      "Epoch: 5/50, Test acc: 54.61, Train acc: 55.64\n",
      "Iteration: 0, Loss: 0.78\n",
      "Epoch: 6/50, Test acc: 55.04, Train acc: 54.94\n",
      "Iteration: 0, Loss: 0.70\n",
      "Epoch: 7/50, Test acc: 54.06, Train acc: 55.17\n",
      "Iteration: 0, Loss: 1.42\n",
      "Epoch: 8/50, Test acc: 54.61, Train acc: 54.90\n",
      "Iteration: 0, Loss: 1.10\n",
      "Epoch: 9/50, Test acc: 53.84, Train acc: 54.72\n",
      "Iteration: 0, Loss: 1.09\n",
      "Epoch: 10/50, Test acc: 54.82, Train acc: 54.30\n",
      "Iteration: 0, Loss: 1.25\n",
      "Epoch: 11/50, Test acc: 55.59, Train acc: 55.50\n",
      "Iteration: 0, Loss: 1.21\n",
      "Epoch: 12/50, Test acc: 54.93, Train acc: 55.21\n",
      "Iteration: 0, Loss: 1.41\n",
      "Epoch: 13/50, Test acc: 54.71, Train acc: 53.67\n",
      "Iteration: 0, Loss: 0.60\n",
      "Epoch: 14/50, Test acc: 54.82, Train acc: 54.10\n",
      "Iteration: 0, Loss: 1.06\n",
      "Epoch: 15/50, Test acc: 54.39, Train acc: 54.51\n",
      "Iteration: 0, Loss: 1.48\n",
      "Min loss 0.14\n",
      "Epoch: 16/50, Test acc: 54.93, Train acc: 55.41\n",
      "Iteration: 0, Loss: 1.57\n",
      "Epoch: 17/50, Test acc: 54.28, Train acc: 54.63\n",
      "Iteration: 0, Loss: 1.16\n",
      "Epoch: 18/50, Test acc: 54.17, Train acc: 53.89\n",
      "Iteration: 0, Loss: 1.34\n",
      "Epoch: 19/50, Test acc: 53.62, Train acc: 54.47\n",
      "Iteration: 0, Loss: 1.37\n",
      "Min loss 0.09\n",
      "Epoch: 20/50, Test acc: 54.50, Train acc: 55.29\n",
      "Iteration: 0, Loss: 1.31\n",
      "Epoch: 21/50, Test acc: 54.28, Train acc: 54.41\n",
      "Iteration: 0, Loss: 1.44\n",
      "Epoch: 22/50, Test acc: 55.81, Train acc: 55.11\n",
      "Iteration: 0, Loss: 1.32\n",
      "Epoch: 23/50, Test acc: 55.59, Train acc: 54.67\n",
      "Iteration: 0, Loss: 1.04\n",
      "Epoch: 24/50, Test acc: 55.59, Train acc: 55.06\n",
      "Iteration: 0, Loss: 0.82\n",
      "Epoch: 25/50, Test acc: 55.37, Train acc: 55.50\n",
      "Iteration: 0, Loss: 0.68\n",
      "Epoch: 26/50, Test acc: 54.50, Train acc: 55.56\n",
      "Iteration: 0, Loss: 2.05\n",
      "Epoch: 27/50, Test acc: 54.28, Train acc: 54.30\n",
      "Iteration: 0, Loss: 0.66\n",
      "Epoch: 28/50, Test acc: 54.06, Train acc: 55.48\n",
      "Iteration: 0, Loss: 1.26\n",
      "Epoch: 29/50, Test acc: 55.15, Train acc: 54.90\n",
      "Iteration: 0, Loss: 0.89\n",
      "Epoch: 30/50, Test acc: 54.61, Train acc: 54.69\n",
      "Iteration: 0, Loss: 1.01\n",
      "Min loss 0.08\n",
      "Epoch: 31/50, Test acc: 54.71, Train acc: 55.39\n",
      "Iteration: 0, Loss: 1.54\n",
      "Epoch: 32/50, Test acc: 53.29, Train acc: 54.06\n",
      "Iteration: 0, Loss: 0.86\n",
      "Epoch: 33/50, Test acc: 54.28, Train acc: 54.65\n",
      "Iteration: 0, Loss: 0.96\n",
      "Epoch: 34/50, Test acc: 54.61, Train acc: 53.75\n",
      "Iteration: 0, Loss: 1.68\n",
      "Epoch: 35/50, Test acc: 54.28, Train acc: 54.92\n",
      "Iteration: 0, Loss: 0.61\n",
      "Epoch: 36/50, Test acc: 55.15, Train acc: 55.54\n",
      "Iteration: 0, Loss: 1.14\n",
      "Epoch: 37/50, Test acc: 55.48, Train acc: 55.41\n",
      "Iteration: 0, Loss: 0.87\n",
      "Epoch: 38/50, Test acc: 54.50, Train acc: 54.28\n",
      "Iteration: 0, Loss: 0.95\n",
      "Epoch: 39/50, Test acc: 53.73, Train acc: 54.74\n",
      "Iteration: 0, Loss: 1.30\n",
      "Epoch: 40/50, Test acc: 54.61, Train acc: 55.00\n",
      "Iteration: 0, Loss: 1.07\n",
      "Epoch: 41/50, Test acc: 56.03, Train acc: 54.92\n",
      "Iteration: 0, Loss: 1.03\n",
      "Epoch: 42/50, Test acc: 56.25, Train acc: 55.60\n",
      "Iteration: 0, Loss: 0.87\n",
      "Epoch: 43/50, Test acc: 54.71, Train acc: 54.30\n",
      "Iteration: 0, Loss: 1.02\n",
      "Epoch: 44/50, Test acc: 54.82, Train acc: 54.53\n",
      "Iteration: 0, Loss: 1.45\n",
      "Epoch: 45/50, Test acc: 54.82, Train acc: 54.74\n",
      "Iteration: 0, Loss: 1.19\n",
      "Epoch: 46/50, Test acc: 55.81, Train acc: 55.13\n",
      "Iteration: 0, Loss: 1.01\n",
      "Epoch: 47/50, Test acc: 55.37, Train acc: 55.62\n",
      "Iteration: 0, Loss: 1.21\n",
      "Epoch: 48/50, Test acc: 54.06, Train acc: 53.83\n",
      "Iteration: 0, Loss: 0.73\n",
      "Epoch: 49/50, Test acc: 55.37, Train acc: 55.99\n"
     ]
    }
   ],
   "source": [
    "best_model, loss_epoch_arr, train_accuracy_lis, val_accuracy_lis, val_loss_epoch_arr = train_func(resnet, loss_fn, opt, trainloader, valloader, scheduler , epochs=50\n",
    "                                                                            )      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b1b9c60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = DataLoader(test_ds, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "6f04c29e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57.89473684210526"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation(testloader,resnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd53466b",
   "metadata": {},
   "source": [
    "# VGG-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "cb6a530d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16 = models.vgg16_bn(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "55a193db",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in vgg16.features.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ef4b14d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = vgg16.classifier[6].in_features\n",
    "vgg16.classifier[6] = nn.Linear(num_features, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "1153fa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16 = vgg16.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(vgg16.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0a410553",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.8  # The decay factor for each epoch\n",
    "scheduler = lr_scheduler.ExponentialLR(opt, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "569049c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f53bcf394d04f1daff9f489e17f0453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?epochs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min loss 0.82\n",
      "Iteration: 0, Loss: 0.82\n",
      "Epoch: 0/50, Test acc: 48.79, Train acc: 50.54\n",
      "Iteration: 0, Loss: 31.94\n",
      "Epoch: 1/50, Test acc: 48.90, Train acc: 50.65\n",
      "Iteration: 0, Loss: 15.54\n",
      "Epoch: 2/50, Test acc: 48.90, Train acc: 50.69\n",
      "Iteration: 0, Loss: 13.60\n",
      "Min loss 0.40\n",
      "Epoch: 3/50, Test acc: 48.90, Train acc: 50.63\n",
      "Iteration: 0, Loss: 27.05\n",
      "Min loss 0.00\n",
      "Epoch: 4/50, Test acc: 48.90, Train acc: 50.61\n",
      "Iteration: 0, Loss: 35.51\n",
      "Epoch: 5/50, Test acc: 48.90, Train acc: 50.57\n",
      "Iteration: 0, Loss: 27.22\n",
      "Epoch: 6/50, Test acc: 48.79, Train acc: 50.54\n",
      "Iteration: 0, Loss: 16.12\n",
      "Epoch: 7/50, Test acc: 48.90, Train acc: 50.61\n",
      "Iteration: 0, Loss: 12.94\n",
      "Epoch: 8/50, Test acc: 48.79, Train acc: 50.57\n",
      "Iteration: 0, Loss: 21.96\n",
      "Epoch: 9/50, Test acc: 49.01, Train acc: 50.71\n",
      "Iteration: 0, Loss: 37.53\n",
      "Epoch: 10/50, Test acc: 49.01, Train acc: 50.65\n",
      "Iteration: 0, Loss: 12.29\n",
      "Epoch: 11/50, Test acc: 48.90, Train acc: 50.61\n",
      "Iteration: 0, Loss: 32.53\n",
      "Epoch: 12/50, Test acc: 48.90, Train acc: 50.63\n",
      "Iteration: 0, Loss: 16.86\n",
      "Epoch: 13/50, Test acc: 48.90, Train acc: 50.69\n",
      "Iteration: 0, Loss: 21.79\n",
      "Epoch: 14/50, Test acc: 48.68, Train acc: 50.59\n",
      "Iteration: 0, Loss: 19.38\n",
      "Epoch: 15/50, Test acc: 48.90, Train acc: 50.67\n",
      "Iteration: 0, Loss: 13.57\n",
      "Min loss 0.00\n",
      "Epoch: 16/50, Test acc: 48.90, Train acc: 50.67\n",
      "Iteration: 0, Loss: 33.94\n",
      "Epoch: 17/50, Test acc: 48.79, Train acc: 50.59\n",
      "Iteration: 0, Loss: 24.76\n",
      "Epoch: 18/50, Test acc: 48.90, Train acc: 50.61\n",
      "Iteration: 0, Loss: 46.59\n",
      "Epoch: 19/50, Test acc: 48.90, Train acc: 50.65\n",
      "Iteration: 0, Loss: 39.67\n",
      "Epoch: 20/50, Test acc: 48.90, Train acc: 50.61\n",
      "Iteration: 0, Loss: 32.70\n",
      "Epoch: 21/50, Test acc: 48.90, Train acc: 50.61\n",
      "Iteration: 0, Loss: 25.43\n",
      "Epoch: 22/50, Test acc: 48.90, Train acc: 50.61\n",
      "Iteration: 0, Loss: 36.79\n",
      "Epoch: 23/50, Test acc: 48.79, Train acc: 50.54\n",
      "Iteration: 0, Loss: 15.37\n",
      "Epoch: 24/50, Test acc: 48.90, Train acc: 50.61\n",
      "Iteration: 0, Loss: 43.93\n",
      "Epoch: 25/50, Test acc: 48.90, Train acc: 50.71\n",
      "Iteration: 0, Loss: 18.20\n",
      "Epoch: 26/50, Test acc: 48.90, Train acc: 50.63\n",
      "Iteration: 0, Loss: 20.20\n",
      "Epoch: 27/50, Test acc: 48.90, Train acc: 50.63\n",
      "Iteration: 0, Loss: 28.26\n",
      "Epoch: 28/50, Test acc: 48.90, Train acc: 50.61\n",
      "Iteration: 0, Loss: 15.77\n",
      "Epoch: 29/50, Test acc: 48.90, Train acc: 50.57\n",
      "Iteration: 0, Loss: 27.82\n",
      "Epoch: 30/50, Test acc: 48.79, Train acc: 50.61\n",
      "Iteration: 0, Loss: 26.17\n",
      "Epoch: 31/50, Test acc: 48.90, Train acc: 50.59\n",
      "Iteration: 0, Loss: 36.30\n",
      "Epoch: 32/50, Test acc: 48.79, Train acc: 50.52\n",
      "Iteration: 0, Loss: 12.12\n",
      "Epoch: 33/50, Test acc: 48.79, Train acc: 50.50\n",
      "Iteration: 0, Loss: 11.66\n",
      "Epoch: 34/50, Test acc: 48.90, Train acc: 50.65\n",
      "Iteration: 0, Loss: 28.35\n",
      "Epoch: 35/50, Test acc: 48.90, Train acc: 50.65\n",
      "Iteration: 0, Loss: 24.66\n",
      "Epoch: 36/50, Test acc: 48.90, Train acc: 50.67\n",
      "Iteration: 0, Loss: 9.54\n",
      "Epoch: 37/50, Test acc: 48.90, Train acc: 50.67\n",
      "Iteration: 0, Loss: 18.05\n",
      "Epoch: 38/50, Test acc: 49.01, Train acc: 50.63\n",
      "Iteration: 0, Loss: 12.57\n",
      "Epoch: 39/50, Test acc: 48.90, Train acc: 50.69\n",
      "Iteration: 0, Loss: 19.12\n",
      "Epoch: 40/50, Test acc: 49.01, Train acc: 50.67\n",
      "Iteration: 0, Loss: 31.48\n",
      "Epoch: 41/50, Test acc: 48.90, Train acc: 50.59\n",
      "Iteration: 0, Loss: 56.68\n",
      "Epoch: 42/50, Test acc: 48.90, Train acc: 50.59\n",
      "Iteration: 0, Loss: 30.30\n",
      "Epoch: 43/50, Test acc: 48.90, Train acc: 50.69\n",
      "Iteration: 0, Loss: 26.55\n",
      "Epoch: 44/50, Test acc: 48.90, Train acc: 50.69\n",
      "Iteration: 0, Loss: 28.26\n",
      "Epoch: 45/50, Test acc: 48.90, Train acc: 50.54\n",
      "Iteration: 0, Loss: 21.29\n",
      "Epoch: 46/50, Test acc: 48.90, Train acc: 50.69\n",
      "Iteration: 0, Loss: 18.77\n",
      "Epoch: 47/50, Test acc: 48.90, Train acc: 50.67\n",
      "Iteration: 0, Loss: 41.33\n",
      "Epoch: 48/50, Test acc: 48.90, Train acc: 50.63\n",
      "Iteration: 0, Loss: 38.89\n",
      "Epoch: 49/50, Test acc: 48.90, Train acc: 50.63\n"
     ]
    }
   ],
   "source": [
    "best_model, loss_epoch_arr, train_accuracy_lis, val_accuracy_lis, val_loss_epoch_arr = train_func(vgg16, loss_fn, opt, trainloader, valloader, scheduler , epochs=50\n",
    "                                                                            )      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "2ff06611",
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = DataLoader(test_ds, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "7c34e2a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45.06578947368421"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation(testloader,vgg16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6c012e",
   "metadata": {},
   "source": [
    "# VGG-19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "b06077eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg19 = models.vgg19_bn(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "55887125",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in vgg19.features.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "2acc8345",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = vgg19.classifier[6].in_features\n",
    "vgg19.classifier[6] = nn.Linear(num_features, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "2892f549",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg19 = vgg19.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(vgg19.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "3f81f207",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.8  # The decay factor for each epoch\n",
    "scheduler = lr_scheduler.ExponentialLR(opt, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "258f7c3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f35d72215f5a407aa78d2f27f331d053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?epochs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min loss 0.76\n",
      "Iteration: 0, Loss: 0.76\n",
      "Epoch: 0/50, Test acc: 48.25, Train acc: 49.72\n",
      "Iteration: 0, Loss: 13.54\n",
      "Epoch: 1/50, Test acc: 48.25, Train acc: 49.76\n",
      "Iteration: 0, Loss: 12.70\n",
      "Epoch: 2/50, Test acc: 48.25, Train acc: 49.76\n",
      "Iteration: 0, Loss: 21.00\n",
      "Epoch: 3/50, Test acc: 48.25, Train acc: 49.76\n",
      "Iteration: 0, Loss: 14.70\n",
      "Epoch: 4/50, Test acc: 48.25, Train acc: 49.78\n",
      "Iteration: 0, Loss: 31.21\n",
      "Epoch: 5/50, Test acc: 48.25, Train acc: 49.78\n",
      "Iteration: 0, Loss: 49.23\n",
      "Epoch: 6/50, Test acc: 48.25, Train acc: 49.78\n",
      "Iteration: 0, Loss: 23.92\n",
      "Epoch: 7/50, Test acc: 48.25, Train acc: 49.78\n",
      "Iteration: 0, Loss: 4.83\n",
      "Epoch: 8/50, Test acc: 48.25, Train acc: 49.78\n",
      "Iteration: 0, Loss: 26.74\n",
      "Epoch: 9/50, Test acc: 48.25, Train acc: 49.76\n",
      "Iteration: 0, Loss: 29.02\n",
      "Epoch: 10/50, Test acc: 48.25, Train acc: 49.76\n",
      "Iteration: 0, Loss: 24.68\n",
      "Min loss 0.00\n",
      "Epoch: 11/50, Test acc: 48.25, Train acc: 49.80\n",
      "Iteration: 0, Loss: 26.46\n",
      "Epoch: 12/50, Test acc: 48.25, Train acc: 49.76\n",
      "Iteration: 0, Loss: 28.21\n",
      "Epoch: 13/50, Test acc: 48.25, Train acc: 49.78\n",
      "Iteration: 0, Loss: 23.88\n",
      "Epoch: 14/50, Test acc: 48.25, Train acc: 49.76\n",
      "Iteration: 0, Loss: 17.71\n",
      "Epoch: 15/50, Test acc: 48.25, Train acc: 49.76\n",
      "Iteration: 0, Loss: 10.59\n",
      "Epoch: 16/50, Test acc: 48.25, Train acc: 49.76\n",
      "Iteration: 0, Loss: 46.58\n",
      "Epoch: 17/50, Test acc: 48.25, Train acc: 49.78\n",
      "Iteration: 0, Loss: 15.54\n",
      "Epoch: 18/50, Test acc: 48.25, Train acc: 49.74\n",
      "Iteration: 0, Loss: 26.33\n",
      "Epoch: 19/50, Test acc: 48.25, Train acc: 49.78\n",
      "Iteration: 0, Loss: 10.36\n",
      "Epoch: 20/50, Test acc: 48.25, Train acc: 49.74\n",
      "Iteration: 0, Loss: 23.83\n",
      "Epoch: 21/50, Test acc: 48.25, Train acc: 49.80\n",
      "Iteration: 0, Loss: 23.73\n",
      "Epoch: 22/50, Test acc: 48.25, Train acc: 49.78\n",
      "Iteration: 0, Loss: 22.97\n",
      "Epoch: 23/50, Test acc: 48.25, Train acc: 49.74\n",
      "Iteration: 0, Loss: 23.59\n",
      "Epoch: 24/50, Test acc: 48.25, Train acc: 49.76\n",
      "Iteration: 0, Loss: 22.13\n",
      "Epoch: 25/50, Test acc: 48.25, Train acc: 49.78\n",
      "Iteration: 0, Loss: 22.43\n",
      "Epoch: 26/50, Test acc: 48.25, Train acc: 49.76\n",
      "Iteration: 0, Loss: 28.39\n",
      "Epoch: 27/50, Test acc: 48.25, Train acc: 49.78\n",
      "Iteration: 0, Loss: 39.97\n",
      "Epoch: 28/50, Test acc: 48.25, Train acc: 49.78\n",
      "Iteration: 0, Loss: 15.91\n",
      "Epoch: 29/50, Test acc: 48.25, Train acc: 49.72\n",
      "Iteration: 0, Loss: 12.86\n",
      "Epoch: 30/50, Test acc: 48.25, Train acc: 49.76\n",
      "Iteration: 0, Loss: 18.77\n",
      "Epoch: 31/50, Test acc: 48.25, Train acc: 49.76\n",
      "Iteration: 0, Loss: 51.09\n",
      "Epoch: 32/50, Test acc: 48.25, Train acc: 49.80\n",
      "Iteration: 0, Loss: 18.96\n",
      "Epoch: 33/50, Test acc: 48.25, Train acc: 49.74\n",
      "Iteration: 0, Loss: 35.13\n",
      "Epoch: 34/50, Test acc: 48.25, Train acc: 49.76\n",
      "Iteration: 0, Loss: 28.38\n",
      "Epoch: 35/50, Test acc: 48.25, Train acc: 49.78\n",
      "Iteration: 0, Loss: 16.97\n",
      "Epoch: 36/50, Test acc: 48.25, Train acc: 49.78\n",
      "Iteration: 0, Loss: 24.93\n",
      "Epoch: 37/50, Test acc: 48.25, Train acc: 49.78\n",
      "Iteration: 0, Loss: 30.18\n",
      "Epoch: 38/50, Test acc: 48.25, Train acc: 49.76\n",
      "Iteration: 0, Loss: 16.35\n",
      "Epoch: 39/50, Test acc: 48.25, Train acc: 49.76\n",
      "Iteration: 0, Loss: 25.66\n",
      "Epoch: 40/50, Test acc: 48.25, Train acc: 49.74\n",
      "Iteration: 0, Loss: 23.76\n",
      "Epoch: 41/50, Test acc: 48.25, Train acc: 49.76\n",
      "Iteration: 0, Loss: 28.19\n",
      "Epoch: 42/50, Test acc: 48.25, Train acc: 49.78\n",
      "Iteration: 0, Loss: 26.57\n",
      "Epoch: 43/50, Test acc: 48.25, Train acc: 49.78\n",
      "Iteration: 0, Loss: 34.83\n",
      "Epoch: 44/50, Test acc: 48.25, Train acc: 49.76\n",
      "Iteration: 0, Loss: 37.18\n",
      "Epoch: 45/50, Test acc: 48.25, Train acc: 49.78\n",
      "Iteration: 0, Loss: 35.01\n",
      "Epoch: 46/50, Test acc: 48.25, Train acc: 49.78\n",
      "Iteration: 0, Loss: 4.93\n",
      "Epoch: 47/50, Test acc: 48.25, Train acc: 49.80\n",
      "Iteration: 0, Loss: 38.60\n",
      "Epoch: 48/50, Test acc: 48.25, Train acc: 49.78\n",
      "Iteration: 0, Loss: 23.47\n",
      "Epoch: 49/50, Test acc: 48.25, Train acc: 49.78\n"
     ]
    }
   ],
   "source": [
    "best_model, loss_epoch_arr, train_accuracy_lis, val_accuracy_lis, val_loss_epoch_arr = train_func(vgg19, loss_fn, opt, trainloader, valloader, scheduler , epochs=50\n",
    "                                                                            )      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "fca29089",
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = DataLoader(test_ds, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "9270fc4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45.06578947368421"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation(testloader,vgg19)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ad3f11",
   "metadata": {},
   "source": [
    "# Densenet121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "41d8246e",
   "metadata": {},
   "outputs": [],
   "source": [
    "densenet121 = models.densenet121(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "e7a3c953",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in densenet121.features.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "91fc373e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = densenet121.classifier.in_features\n",
    "densenet121.classifier = nn.Linear(num_features, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "185b29e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "densenet121 = densenet121.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(densenet121.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "631b6e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.8  # The decay factor for each epoch\n",
    "scheduler = lr_scheduler.ExponentialLR(opt, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "838672e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b983a64a08f046e2bbbcb05f08340cac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?epochs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min loss 0.69\n",
      "Iteration: 0, Loss: 0.69\n",
      "Min loss 0.61\n",
      "Min loss 0.59\n",
      "Min loss 0.55\n",
      "Min loss 0.53\n",
      "Min loss 0.52\n",
      "Min loss 0.49\n",
      "Epoch: 0/50, Test acc: 55.04, Train acc: 52.37\n",
      "Iteration: 0, Loss: 0.80\n",
      "Epoch: 1/50, Test acc: 54.28, Train acc: 52.04\n",
      "Iteration: 0, Loss: 0.72\n",
      "Epoch: 2/50, Test acc: 55.04, Train acc: 52.27\n",
      "Iteration: 0, Loss: 0.71\n",
      "Min loss 0.46\n",
      "Epoch: 3/50, Test acc: 55.04, Train acc: 52.39\n",
      "Iteration: 0, Loss: 0.78\n",
      "Epoch: 4/50, Test acc: 54.06, Train acc: 52.21\n",
      "Iteration: 0, Loss: 0.66\n",
      "Epoch: 5/50, Test acc: 54.50, Train acc: 52.33\n",
      "Iteration: 0, Loss: 0.77\n",
      "Epoch: 6/50, Test acc: 54.93, Train acc: 52.37\n",
      "Iteration: 0, Loss: 0.66\n",
      "Epoch: 7/50, Test acc: 53.95, Train acc: 52.41\n",
      "Iteration: 0, Loss: 0.60\n",
      "Epoch: 8/50, Test acc: 54.50, Train acc: 51.86\n",
      "Iteration: 0, Loss: 0.76\n",
      "Epoch: 9/50, Test acc: 53.84, Train acc: 52.46\n",
      "Iteration: 0, Loss: 0.86\n",
      "Epoch: 10/50, Test acc: 54.39, Train acc: 52.04\n",
      "Iteration: 0, Loss: 0.68\n",
      "Epoch: 11/50, Test acc: 53.84, Train acc: 52.00\n",
      "Iteration: 0, Loss: 0.61\n",
      "Epoch: 12/50, Test acc: 55.26, Train acc: 52.27\n",
      "Iteration: 0, Loss: 0.73\n",
      "Epoch: 13/50, Test acc: 54.06, Train acc: 52.23\n",
      "Iteration: 0, Loss: 0.63\n",
      "Epoch: 14/50, Test acc: 54.06, Train acc: 52.83\n",
      "Iteration: 0, Loss: 0.73\n",
      "Epoch: 15/50, Test acc: 55.37, Train acc: 52.52\n",
      "Iteration: 0, Loss: 0.77\n",
      "Epoch: 16/50, Test acc: 55.26, Train acc: 52.33\n",
      "Iteration: 0, Loss: 0.73\n",
      "Epoch: 17/50, Test acc: 54.28, Train acc: 52.23\n",
      "Iteration: 0, Loss: 0.66\n",
      "Epoch: 18/50, Test acc: 54.71, Train acc: 52.23\n",
      "Iteration: 0, Loss: 0.71\n",
      "Epoch: 19/50, Test acc: 54.50, Train acc: 52.29\n",
      "Iteration: 0, Loss: 0.70\n",
      "Epoch: 20/50, Test acc: 53.95, Train acc: 52.74\n",
      "Iteration: 0, Loss: 0.63\n",
      "Epoch: 21/50, Test acc: 53.95, Train acc: 51.76\n",
      "Iteration: 0, Loss: 0.77\n",
      "Epoch: 22/50, Test acc: 54.93, Train acc: 52.25\n",
      "Iteration: 0, Loss: 0.73\n",
      "Epoch: 23/50, Test acc: 54.61, Train acc: 52.06\n",
      "Iteration: 0, Loss: 0.63\n",
      "Epoch: 24/50, Test acc: 54.39, Train acc: 51.94\n",
      "Iteration: 0, Loss: 0.72\n",
      "Epoch: 25/50, Test acc: 54.82, Train acc: 52.09\n",
      "Iteration: 0, Loss: 0.78\n",
      "Epoch: 26/50, Test acc: 54.17, Train acc: 52.33\n",
      "Iteration: 0, Loss: 0.73\n",
      "Epoch: 27/50, Test acc: 53.84, Train acc: 52.06\n",
      "Iteration: 0, Loss: 0.77\n",
      "Epoch: 28/50, Test acc: 55.15, Train acc: 52.19\n",
      "Iteration: 0, Loss: 0.63\n",
      "Epoch: 29/50, Test acc: 53.40, Train acc: 51.92\n",
      "Iteration: 0, Loss: 0.81\n",
      "Epoch: 30/50, Test acc: 54.17, Train acc: 52.09\n",
      "Iteration: 0, Loss: 0.83\n",
      "Epoch: 31/50, Test acc: 53.73, Train acc: 52.15\n",
      "Iteration: 0, Loss: 0.69\n",
      "Epoch: 32/50, Test acc: 54.93, Train acc: 52.91\n",
      "Iteration: 0, Loss: 0.67\n",
      "Min loss 0.45\n",
      "Epoch: 33/50, Test acc: 55.15, Train acc: 52.17\n",
      "Iteration: 0, Loss: 0.69\n",
      "Epoch: 34/50, Test acc: 53.95, Train acc: 51.80\n",
      "Iteration: 0, Loss: 0.68\n",
      "Epoch: 35/50, Test acc: 55.04, Train acc: 52.43\n",
      "Iteration: 0, Loss: 0.62\n",
      "Epoch: 36/50, Test acc: 54.61, Train acc: 51.88\n",
      "Iteration: 0, Loss: 0.67\n",
      "Epoch: 37/50, Test acc: 54.28, Train acc: 52.21\n",
      "Iteration: 0, Loss: 0.75\n",
      "Epoch: 38/50, Test acc: 55.26, Train acc: 52.06\n",
      "Iteration: 0, Loss: 0.67\n",
      "Epoch: 39/50, Test acc: 54.82, Train acc: 52.23\n",
      "Iteration: 0, Loss: 0.61\n",
      "Epoch: 40/50, Test acc: 54.06, Train acc: 52.27\n",
      "Iteration: 0, Loss: 0.75\n",
      "Epoch: 41/50, Test acc: 54.06, Train acc: 52.11\n",
      "Iteration: 0, Loss: 0.71\n",
      "Epoch: 42/50, Test acc: 55.59, Train acc: 52.41\n",
      "Iteration: 0, Loss: 0.74\n",
      "Epoch: 43/50, Test acc: 53.84, Train acc: 51.96\n",
      "Iteration: 0, Loss: 0.68\n",
      "Epoch: 44/50, Test acc: 54.17, Train acc: 52.27\n",
      "Iteration: 0, Loss: 0.63\n",
      "Epoch: 45/50, Test acc: 54.28, Train acc: 52.15\n",
      "Iteration: 0, Loss: 0.79\n",
      "Epoch: 46/50, Test acc: 55.04, Train acc: 52.39\n",
      "Iteration: 0, Loss: 0.77\n",
      "Epoch: 47/50, Test acc: 54.17, Train acc: 51.94\n",
      "Iteration: 0, Loss: 0.67\n",
      "Epoch: 48/50, Test acc: 52.85, Train acc: 52.23\n",
      "Iteration: 0, Loss: 0.60\n",
      "Epoch: 49/50, Test acc: 54.61, Train acc: 51.78\n"
     ]
    }
   ],
   "source": [
    "best_model, loss_epoch_arr, train_accuracy_lis, val_accuracy_lis, val_loss_epoch_arr = train_func(densenet121, loss_fn, opt, trainloader, valloader, scheduler , epochs=50\n",
    "                                                                            )      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "833c345b",
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = DataLoader(test_ds, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "881c73ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55.5921052631579"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation(testloader,densenet121)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de01384c",
   "metadata": {},
   "source": [
    "# DenseNet201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "6e193b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "densenet201 = models.densenet201(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "a8b79e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in densenet121.features.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "2b865a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = densenet201.classifier.in_features\n",
    "densenet201.classifier = nn.Linear(num_features, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "34c6dc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "densenet201 = densenet201.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(densenet201.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "a675c60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.8  # The decay factor for each epoch\n",
    "scheduler = lr_scheduler.ExponentialLR(opt, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "fcdac808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe86dc9770a3470398ccbe4fad8af3b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?epochs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min loss 0.76\n",
      "Iteration: 0, Loss: 0.76\n",
      "Min loss 0.73\n",
      "Min loss 0.57\n",
      "Epoch: 0/50, Test acc: 50.11, Train acc: 49.83\n",
      "Iteration: 0, Loss: 0.75\n",
      "Epoch: 1/50, Test acc: 49.78, Train acc: 49.91\n",
      "Iteration: 0, Loss: 0.68\n",
      "Epoch: 2/50, Test acc: 50.22, Train acc: 49.64\n",
      "Iteration: 0, Loss: 0.69\n",
      "Epoch: 3/50, Test acc: 48.36, Train acc: 49.58\n",
      "Iteration: 0, Loss: 0.60\n",
      "Epoch: 4/50, Test acc: 50.00, Train acc: 49.76\n",
      "Iteration: 0, Loss: 0.67\n",
      "Epoch: 5/50, Test acc: 48.46, Train acc: 49.62\n",
      "Iteration: 0, Loss: 0.72\n",
      "Epoch: 6/50, Test acc: 49.34, Train acc: 49.78\n",
      "Iteration: 0, Loss: 0.69\n",
      "Epoch: 7/50, Test acc: 49.34, Train acc: 49.58\n",
      "Iteration: 0, Loss: 0.69\n",
      "Epoch: 8/50, Test acc: 48.79, Train acc: 49.60\n",
      "Iteration: 0, Loss: 0.77\n",
      "Epoch: 9/50, Test acc: 49.56, Train acc: 49.58\n",
      "Iteration: 0, Loss: 0.72\n",
      "Epoch: 10/50, Test acc: 49.89, Train acc: 49.64\n",
      "Iteration: 0, Loss: 0.78\n",
      "Epoch: 11/50, Test acc: 50.00, Train acc: 49.76\n",
      "Iteration: 0, Loss: 0.68\n",
      "Min loss 0.51\n",
      "Epoch: 12/50, Test acc: 48.57, Train acc: 49.43\n",
      "Iteration: 0, Loss: 0.72\n",
      "Epoch: 13/50, Test acc: 49.34, Train acc: 49.74\n",
      "Iteration: 0, Loss: 0.61\n",
      "Epoch: 14/50, Test acc: 48.36, Train acc: 49.37\n",
      "Iteration: 0, Loss: 0.75\n",
      "Epoch: 15/50, Test acc: 48.90, Train acc: 49.68\n",
      "Iteration: 0, Loss: 0.65\n",
      "Epoch: 16/50, Test acc: 50.00, Train acc: 49.66\n",
      "Iteration: 0, Loss: 0.73\n",
      "Epoch: 17/50, Test acc: 48.14, Train acc: 49.27\n",
      "Iteration: 0, Loss: 0.65\n",
      "Epoch: 18/50, Test acc: 49.01, Train acc: 49.76\n",
      "Iteration: 0, Loss: 0.65\n",
      "Epoch: 19/50, Test acc: 48.90, Train acc: 49.62\n",
      "Iteration: 0, Loss: 0.65\n",
      "Epoch: 20/50, Test acc: 49.78, Train acc: 49.99\n",
      "Iteration: 0, Loss: 0.67\n",
      "Epoch: 21/50, Test acc: 49.34, Train acc: 49.62\n",
      "Iteration: 0, Loss: 0.69\n",
      "Epoch: 22/50, Test acc: 48.14, Train acc: 49.62\n",
      "Iteration: 0, Loss: 0.75\n",
      "Epoch: 23/50, Test acc: 49.89, Train acc: 49.70\n",
      "Iteration: 0, Loss: 0.71\n",
      "Epoch: 24/50, Test acc: 48.68, Train acc: 49.52\n",
      "Iteration: 0, Loss: 0.74\n",
      "Epoch: 25/50, Test acc: 50.11, Train acc: 49.87\n",
      "Iteration: 0, Loss: 0.71\n",
      "Epoch: 26/50, Test acc: 48.25, Train acc: 49.48\n",
      "Iteration: 0, Loss: 0.69\n",
      "Epoch: 27/50, Test acc: 50.11, Train acc: 49.78\n",
      "Iteration: 0, Loss: 0.77\n",
      "Epoch: 28/50, Test acc: 47.81, Train acc: 48.92\n",
      "Iteration: 0, Loss: 0.75\n",
      "Epoch: 29/50, Test acc: 47.81, Train acc: 49.02\n",
      "Iteration: 0, Loss: 0.72\n",
      "Epoch: 30/50, Test acc: 50.00, Train acc: 49.85\n",
      "Iteration: 0, Loss: 0.69\n",
      "Epoch: 31/50, Test acc: 48.90, Train acc: 49.70\n",
      "Iteration: 0, Loss: 0.71\n",
      "Epoch: 32/50, Test acc: 49.67, Train acc: 49.85\n",
      "Iteration: 0, Loss: 0.67\n",
      "Epoch: 33/50, Test acc: 48.03, Train acc: 49.41\n",
      "Iteration: 0, Loss: 0.75\n",
      "Epoch: 34/50, Test acc: 49.34, Train acc: 49.76\n",
      "Iteration: 0, Loss: 0.70\n",
      "Epoch: 35/50, Test acc: 50.11, Train acc: 49.68\n",
      "Iteration: 0, Loss: 0.69\n",
      "Epoch: 36/50, Test acc: 49.34, Train acc: 49.85\n",
      "Iteration: 0, Loss: 0.76\n",
      "Epoch: 37/50, Test acc: 49.56, Train acc: 49.70\n",
      "Iteration: 0, Loss: 0.64\n",
      "Epoch: 38/50, Test acc: 48.90, Train acc: 49.52\n",
      "Iteration: 0, Loss: 0.68\n",
      "Epoch: 39/50, Test acc: 50.00, Train acc: 49.58\n",
      "Iteration: 0, Loss: 0.82\n",
      "Epoch: 40/50, Test acc: 48.90, Train acc: 49.58\n",
      "Iteration: 0, Loss: 0.75\n",
      "Epoch: 41/50, Test acc: 48.03, Train acc: 49.56\n",
      "Iteration: 0, Loss: 0.70\n",
      "Epoch: 42/50, Test acc: 49.34, Train acc: 49.62\n",
      "Iteration: 0, Loss: 0.69\n",
      "Epoch: 43/50, Test acc: 50.11, Train acc: 49.72\n",
      "Iteration: 0, Loss: 0.70\n",
      "Epoch: 44/50, Test acc: 48.79, Train acc: 49.68\n",
      "Iteration: 0, Loss: 0.75\n",
      "Epoch: 45/50, Test acc: 48.03, Train acc: 49.68\n",
      "Iteration: 0, Loss: 0.71\n",
      "Epoch: 46/50, Test acc: 49.34, Train acc: 49.56\n",
      "Iteration: 0, Loss: 0.69\n",
      "Epoch: 47/50, Test acc: 48.79, Train acc: 49.62\n",
      "Iteration: 0, Loss: 0.74\n",
      "Epoch: 48/50, Test acc: 49.67, Train acc: 49.70\n",
      "Iteration: 0, Loss: 0.65\n",
      "Epoch: 49/50, Test acc: 49.89, Train acc: 49.97\n"
     ]
    }
   ],
   "source": [
    "best_model, loss_epoch_arr, train_accuracy_lis, val_accuracy_lis, val_loss_epoch_arr = train_func(densenet201, loss_fn, opt, trainloader, valloader, scheduler , epochs=50\n",
    "                                                                            )      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "17815dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = DataLoader(test_ds, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "8a19fefd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46.05263157894737"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation(testloader,densenet201)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef58c81c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2786fb11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
